[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Prep / EDA",
    "section": "",
    "text": "Where the data source, processing, and visualization (EDA) is presented.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-collection",
    "href": "data.html#data-collection",
    "title": "Data Prep / EDA",
    "section": "Data Collection",
    "text": "Data Collection\nAmazon product information was scraped from the website using the API service ScraperAPI; this is because, as Amazon is a hugely popular website, they have many anti-scraping measures in place such as rate-limiting, IP blocking, dymamic loading, and such. Using the external API service, these limitations were able to be avoided. The search queries chosen to search for items were based on top 100 Amazon searches, found on this site and this site. An example of using the API, along with its core endpoint, is below.\n\nimport requests\n\npayload = {\n   'api_key': 'API_KEY',\n   'query': 'iphone 15 charger',\n   's': 'price-asc-rank'\n}\n\nresponse = requests.get('https://api.scraperapi.com/structured/amazon/search',\n                        params=payload).json()\n\nThe jupyter notebook code for the web scraping can be found here.\nAdditionally, more data was used to supplement the existing data. Since the scraped data was only about 26K rows, a Kaggle dataset was used that contains more than one million rows, had around the same fields as the scraped data, and was also from the USA (many Amazon Kaggle datasets were from the non-US).\nThe raw data from both sources can be seen below in Table 1; the scraped raw data CSV can also be viewed here.\n\n\n\n\nTable 1: The raw data from both datasets.\n\n\n\n\n\n\n\n\n\n(a) The raw data scraped from Amazon using ScraperAPI\n\n\n\n\n\n\ntype\nposition\nasin\nname\nimage\nhas_prime\nis_best_seller\nis_amazon_choice\nis_limited_deal\nstars\ntotal_reviews\nurl\navailability_quantity\nspec\nprice_string\nprice_symbol\nprice\noriginal_price\nsection_name\n\n\n\n\n0\nsearch_product\n32\nB0CV5Z91QR\nBetter Home Products Megan Wooden 6 Drawer Dou...\nhttps://m.media-amazon.com/images/I/81fL4qHZ4i...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/Better-Home-Products-Wo...\nNaN\n{}\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nsearch_product\n40\nB0CTZQ73SH\nFixtureDisplays® Bathtub and Shower Tension St...\nhttps://m.media-amazon.com/images/I/51F0eGaPI+...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/FixtureDisplays%C2%AE-B...\nNaN\n{}\n$22.60\n$\n22.60\nNaN\nNaN\n\n\n2\nsearch_product\n15\nB0CR633W5W\n4-Port USB 3.0 Hub Ultra-Thin Expand PC Connec...\nhttps://m.media-amazon.com/images/I/51sJxlimwP...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/4-Port-Ultra-Thin-Expan...\nNaN\n{}\n$3.99\n$\n3.99\nNaN\nNaN\n\n\n3\nsearch_product\n15\nB0CTQSPM8G\nMK000960GWSSD 960GB SATA 6G MU SFF SC DS SSD-1...\nhttps://m.media-amazon.com/images/I/11rdwQUxu0...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/Genuine-Original-MK0009...\nNaN\n{}\n$375.00\n$\n375.00\nNaN\nNaN\n\n\n4\nsearch_product\n44\nB0CJ9ZF7LQ\nSOL DE JANEIROCheirosa '62 Hair & Body Fragran...\nhttps://m.media-amazon.com/images/I/31mCVvwGqC...\nFalse\nFalse\nFalse\nFalse\n4.6\n14.0\nhttps://www.amazon.com/SOL-JANEIRO-Cheirosa-Fr...\nNaN\n{}\n$62.00\n$\n62.00\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The raw data gotten from Kaggle\n\n\n\n\n\n\nasin\ntitle\nimgUrl\nproductURL\nstars\nreviews\nprice\nlistPrice\ncategory_id\nisBestSeller\nboughtInLastMonth\n\n\n\n\n0\nB01CR9B6UY\nDC Cargo E-Track Ratcheting Straps Cargo Tie-D...\nhttps://m.media-amazon.com/images/I/81KFTHU7xx...\nhttps://www.amazon.com/dp/B01CR9B6UY\n4.7\n593\n36.99\n0.00\n24\nFalse\n100\n\n\n1\nB0BWFRDBBW\nLittle Girls Glitter Tulle Dress, Sparkle Polk...\nhttps://m.media-amazon.com/images/I/711QwZxcW8...\nhttps://www.amazon.com/dp/B0BWFRDBBW\n3.9\n7\n46.99\n0.00\n91\nFalse\n0\n\n\n2\nB00XV8CQGO\nClub Little Girl Summer Animal Collection Stic...\nhttps://m.media-amazon.com/images/I/41wQCnt7+E...\nhttps://www.amazon.com/dp/B00XV8CQGO\n0.0\n0\n9.99\n0.00\n95\nFalse\n50\n\n\n3\nB08MBJX3K1\nToddler Boys Girls Sneakers Size 5-12 Lightwei...\nhttps://m.media-amazon.com/images/I/81IscOXT6S...\nhttps://www.amazon.com/dp/B08MBJX3K1\n4.5\n0\n31.99\n35.99\n97\nFalse\n0\n\n\n4\nB08YKJDWLF\nCFS Cut to Fit Carbon Pad Pre Filter Roll for ...\nhttps://m.media-amazon.com/images/I/61j4Gpu4jH...\nhttps://www.amazon.com/dp/B08YKJDWLF\n4.6\n0\n12.59\n0.00\n171\nFalse\n0",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "Data Prep / EDA",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe datasets were cleaned seperately, then concatenated, then some final steps were taken to clean it.\nThe steps to clean the web-scaped data were:\n\nAdd date_scraped column\nRemove unecessary columns: type, position, has_prime, is_amazon_choice, is_limited_deal, availability_quantity, spec, price_string, price_symbol, section_name\nExpand and fix original_price\nRename columns to match standard snake case for merging both datasets\nDrop rows with no asin or name or price\nDrop rows with price of 0.0, since that doesn’t make sense\nFill NaN reviews column with 0\n\nThe steps to clean the Kaggle data were:\n\nAdd date_scraped column\nDrop rows with any NaNs\nFix list_price of $0 to be instead equal to price\nChange category_id to actual category by using category table\nDrop rows with price of $0, since that doesn’t make sense\nRename columns to match standard snake case for merging both datasets\n\nAnd then, after they were concatenated, the steps to clean were:\n\nRemove duplicates (by asin + date scraped)\nRename columns\n\nThe final cleaned (and concatenated) dataset can be seen in Table 2 (with the original raw data in Table 1):\n\n\n\n\nTable 2: The final unioned, cleaned, and processed data.\n\n\n\n\n\n\n\n\n\n\nAsin\nName\nImage Url\nIs Best Seller\nStars\nReviews\nUrl\nPrice\nDate Scraped\nList Price\nBought In Month\nCategory\n\n\n\n\n0\nB077H6599Q\nGenerac 7103 Cold Kit for 9kW - 22kW Air Coole...\nhttps://m.media-amazon.com/images/I/71+HddWhsK...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B077H6599Q\n94.99\n2023-11-01\n105.99\n0.0\nRV Parts & Accessories\n\n\n1\nB095HLD52Z\n3 Pack Apple MFi Certified iPhone Charger Cabl...\nhttps://m.media-amazon.com/images/I/61rqFEt6ku...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B095HLD52Z\n20.99\n2023-11-01\n20.99\n0.0\nAccessories & Supplies\n\n\n2\nB08XK4CBP8\n164pcs Blush White Balloons Garland Arch Kit P...\nhttps://m.media-amazon.com/images/I/71oD9RPq84...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B08XK4CBP8\n14.99\n2023-11-01\n16.99\n50.0\nParty Decorations\n\n\n3\nB0C1Z9G9J9\nMudtun Bone Conduction Earbuds for Small Ear C...\nhttps://m.media-amazon.com/images/I/51js5mISHA...\nFalse\n3.5\n74.0\nhttps://www.amazon.com/dp/B0C1Z9G9J9\n19.99\n2023-11-01\n19.99\n0.0\nHeadphones & Earbuds\n\n\n4\nB0C5NB4SRN\nV-SHURA Music Gifts Table Lamp, Guitar Table L...\nhttps://m.media-amazon.com/images/I/710DKiJYTV...\nFalse\n4.8\n0.0\nhttps://www.amazon.com/dp/B0C5NB4SRN\n55.99\n2023-11-01\n55.99\n0.0\nHome Lighting & Ceiling Fans\n\n\n\n\n\n\n\n\n\n\nThe code for the data cleaning can be found here.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-preprocessing-visualization",
    "href": "data.html#data-preprocessing-visualization",
    "title": "Data Prep / EDA",
    "section": "Data Preprocessing / Visualization",
    "text": "Data Preprocessing / Visualization\nVarious types of EDA were performed in order to examine the data; as a note, most visuals are interactive (zoomable, pannable, etc). The code for all visualizations can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website) until all figures load.\n\n\nFirst, a histogram of all categories of all Amazon products is shown in Figure 1. Note scraped data did not have categories, but the Kaggle data did. It can be seen that from the figure, girl’s and boy’s clothing are the most populous categories, with toys & games the next most populous.\n\n\n\n\n                                                \n\n\nFigure 1: A histogram of all categories of all Amazon products. Note scraped data did not have categories, but the Kaggle data did.\n\n\n\n\nFigure 2 shows a histogram of the stars recieved for a product. It can be seen that the number of stars increases generally exponentially until 4.6 stars, with peaks at 0 and 5 stars as well. This data generally agrees with what can be seen on Amazon when browsing personally, and the peaks at 0 and 5 agree with having low number of reviews.\n\n\n\n\n                                                \n\n\nFigure 2: Histogram of the number of stars recieved for all products.\n\n\n\n\nFigure 3 shows violin plots of the numerical factors: volume of product bought in the month, the price of a product, and the number of reviews. A violin plot was chosen because a histogram would be visually uninformative, due to the amount of outliers. Some insights that can be drawn from these violin plots are:\n\nThese factors have a good number of outliers that are extremely large in value compared to the general distribution of values. This does not seem to be from incorrect values, but from sellers attempting to “game the system”.\n“Bought in Month” has outliers that are regularly spaced, indicating either a form of gaming the system by sellers that tend to follow specific formats, or Amazon only reports volume of bought products at low granularity for high-volume products.\nPrice tends to have outliers that continue regularly up the price scale, except for a couple at the $60-70k range. It is unclear whether to drop those rows containing those prices as they seem valid; it is entirely possible that sellers just list the product at a completely unreasonable price as a placeholder, and don’t expect anyone to purchase at that price.\nThe number of reviews for a product have been shown to be easily gameable with fake reviews, and is likely what is happening here.\n\n\n\n\n\n\n\n\n\nViolin plot of the number of a product bought in a month\n\n\n\n\n\n\n\nViolin plot of price of a product\n\n\n\n\n\n\n\n\n\nViolin plot of the number of reviews\n\n\n\n\n\n\nFigure 3: Violin plots (with overlayed box plot and outliers) of 3 of the main numerical columns.\n\n\n\nThe amount of stars, vs the number of reviews for a product can be seen in Figure 4, colored with the Is Best Seller flag. It can be seen that the distribution of Is Best Seller items tends to follow the overall distribution; additionally, the number of reviews tends to increase as the number of stars increases, with the products with the outlier number of reviews (over 200k) all above 4 stars.\n\n\n\n\n\n\n\n\nFigure 4: Stars vs number of reviews recieved by an amazon product, colored by whether the product was a best-seller.\n\n\n\n\n\nIn order to visualize the content of the text factors, namely the Category and Name of a product, a wordcloud format was used; wordclouds visualize the words that tend to appear in the text with the more frequent words appearing in larger text. In Figure 5, it can be seen that in the categories, “Care”, “Products”, “Boy’s Clothing”, “Consoles”, “Accessories”, and “Games” appear most. In the names of procucts, the words “Stainles Steel”, “Compatible”, “Heavy Duty”, “Boy”, “Girl”, and “Birthday” appear the most.\n\n\n\n\n\n\n\n\nWordcloud for categories of products\n\n\n\n\n\n\n\nWordcloud for the names of products\n\n\n\n\n\n\nFigure 5: Wordclouds (where more frequent appearing words are bigger) of the categories of products and the names of products.\n\n\n\nThe number of stars vs price, with price being on a log scale (to hangle outliers) is shown in Figure 6, along with being colored by if it’s a best seller. As was seen in Figure 4, the distribution of Is Best Seller items tends to follow the overall distribution; additionally, it can be seen that there are products with a price up to $10k across all number of stars.\n\n\n\n\n\n\n\n\nFigure 6: The number of stars a product recieved, vs the price in log scale, colored by whether the product was a best seller.\n\n\n\n\n\n\nInvestigation of products in both sets of data\nThe following plots focused on products that had the same Asin (identifying ID), that were in both the data personally scraped in 2024 and the Kaggle data from 2023. Investigating this subset of data could reveal a lot about how product prices, reviews, and other factors changed over time, as the only way to get time difference data was to use products that actually had more than one timepoint.\nFigure 7 shows the price vs list price of items, colored by what date they were scraped on, with a trendline plotted. It can be seen that on the whole, list price and actual price match and that prices seem to have increased since 2023 to 2024; however, below $50 price (when zooming in) the trendlines cross, implying that lower-priced products actually decreased in price.\n\n\n\n\n                                                \n\n\nFigure 7: Price vs list price of items with the same ASIN across dates scraped, with a trendline.\n\n\n\n\nGiven that outliers can be seen in price affecting the plot of the graph, it was decided for analysis to only consider those prices most populous, aka prices less than $800. Given that, the prices of each Asin were plotted, along with being colored by if the price increased from 2023 to 2024, or decreased. It can be seen that in Figure 8 that indeed for lower priced products (less than $50) the prices mostly did not increase, and instead decreased or stayed the same. The histogram shows the frequency of higher-priced procucts generally exponentially decreasing, and the likelihood of the price increasing going up.\n\n\n\n\n                                                \n\n\nFigure 8: Histogram of prices, colored by whether the change in price increased or decreased over time, for those items that were in both sets of data.\n\n\n\n\nFigure 9 shows a more granular form of the previous plot by plotting price vs the actual price difference amount from 2023 to 2024, with the ability to investigate which products are having such a dramatic increase or decrease in price over time. It can be seen that tech and suitcases were the outliers in price increase, and tech also being the outliers in price decrease.\n\n\n\n\n                                                \n\n\nFigure 9: Price vs the difference in price, over the two sets of data, colored by whether the price diff increased or decreased.\n\n\n\n\nFinally, it was investigated as to which product categories exactly had a price increase or decrease, and the ratio of number of increases to decreases. Only categories with more than 20 products were included, to reduce noise and to focus on meaningful categories. In Figure 10, it can be seen that “Sports and Outdoors”, “Makeup”, “Electrical Equipment”, and “Building Toys” all had more price increases than decreases; every other product category had more price decreases than increases, with “Smart Homes” and “Gift Cards” having the most price decreases.\n\n\n\n\n                                                \n\n\nFigure 10: For the included categories, a bar chat showing the ratio of products that had their price increase over time vs decrease.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Home\n\n\nWelcome to the homepage for machine learning on Amazon retail product data, made by Bhavana Jonnalagadda! You can see all pages listed below, or use the navbar to navigate the pages.\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSVM (Support Vector Machine)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA and SVD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Prep / EDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Code",
      "Home"
    ]
  },
  {
    "objectID": "models/pca.html",
    "href": "models/pca.html",
    "title": "PCA and SVD",
    "section": "",
    "text": "Where Principal Component Analysis and Singular Value Decomposition are used, for dimensionality reduction and EDA.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#overview",
    "href": "models/pca.html#overview",
    "title": "PCA and SVD",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\nAn example 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components generated from it, respectively.\n\n\n\n\n\n\n\nScatterplot after PCA reduced from 3-dimensions to 2-dimensions.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and ML. At its core, PCA aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. This is achieved by identifying the directions of maximum variance in the original data, known as principal components. These principal components are the eigenvectors of the covariance matrix of the data, with their corresponding eigenvalues equaling the amount of variance explained by each principal component (PC). By retaining the PCs that capture the majority of the variance, PCA enables dimensionality reduction without significant loss of information.\nReducing the dimensionality of data is crucial for simplifying analysis and visualization, as it alleviates the curse of dimensionality and reduces computational complexity. High-dimensional data presents challenges such as increased computational cost, sparsity, and overfitting, which can get in the way of analysis and interpretation. Dimensionality reduction techniques like PCA help address these challenges by capturing the essential structure of the data in a lower-dimensional space, making it easier to visualize, interpret, and analyze complex datasets. Also, reducing dimensionality can improve the performance of machine learning algorithms by reducing noise, mitigating overfitting, enhancing model generalization, and potentially increasing the speed of training/evaluation of the models with the reduced amount of data.\nIn this specific project, PCA is used to reduce the amount of numerical/boolean factors to just 2 factors, which then enables the examination of those 2 components visually. The relationship between the reduced-dimension factors and the original factors in the dataset is explored, showing the effect and importance of the various factors. And especially since there are more than 1 million rows in the dataset, every amount of data reduction is helpful for subsequent machine learning model training.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#data-preperation",
    "href": "models/pca.html#data-preperation",
    "title": "PCA and SVD",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nData Prep: All models and methods require specific data formats. Explain what kind of data is needed for PCA and show an image of the sample of data you plan to use. LINK to the sample of data as well.\nCode: Use R or Python to code PCA. LINK to the code.\nResults: Discuss, illustrate, describe, and visualize the results. Understand and explain how to visualize the relationship between variables and principal components. Include at least 2 visualizations for your data.\nConclusions: What did you learn that pertains to your topic?\n\nTable 1\n\n\n\nTable 1: A random sample of the data used with PCA.\n\n\n\n\n\n\n\n\n\n(a) Unscaled data\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n4.5\n618\n26\n26\n600\n\n\n0\n0\n0\n8.89\n8.89\n0\n\n\n0\n2.9\n2\n7.99\n7.99\n100\n\n\n0\n4.8\n0\n100\n100\n0\n\n\n0\n4.6\n408\n259\n370\n200\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaled (by min/max) data\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n1\n0\n0.0588\n0.0544\n0\n\n\n0\n0.82\n0\n0.497\n0.542\n0.778\n\n\n0\n0.88\n0.419\n0.117\n0.109\n0.333\n\n\n0\n0.76\n0\n0.0764\n0.0708\n0\n\n\n0\n0.94\n0\n0.0617\n0.0686\n0",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#code",
    "href": "models/pca.html#code",
    "title": "PCA and SVD",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the PCA can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#results",
    "href": "models/pca.html#results",
    "title": "PCA and SVD",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.\n\n\nFigure 1\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) Unscaled data\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) Scaled (and outliers removed) data\n\n\n\n\n\n\n\nFigure 1: A bar chart of the explained variance ratios of each principal component, and the scree plot of cummulative percentage of the ratios, for both the normal and scaled data.\n\n\n\nFigure 2\n\n\n\nFigure 2: The found principal components, colored by a feature in the dataset where the feature has visible stratification in the PCs.\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\nRelationship of Variables and PCs\n\n\n\n\n\nAn example of the projection of the original data points onto the reduced-dimension line of an eigenvector. The angle/direction of the line is computationally chosen to maximize the sum of all the variance, aka the dotted lines.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#conclusions",
    "href": "models/pca.html#conclusions",
    "title": "PCA and SVD",
    "section": "Conclusions",
    "text": "Conclusions\n\n\n\nTODO\nTODO\nTODO\nTODO\nTODO\nTODO\nTODO",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/nn.html",
    "href": "models/nn.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural Networks\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/bayes.html",
    "href": "models/bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "How Naive Bayes classifies categories.\n\n\n\n\nNaive Bayes is a popular and simple probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification tasks, such as spam email detection, sentiment analysis, and medical diagnosis. The “naive” part of its name comes from the assumption that features are independent of each other, which is often not true in real-world data but simplifies the model and makes it computationally efficient. It uses Bayes’ Theorem, which calculates the probability of a hypothesis (H) given some evidence (E). Standard Multinomial Naive Bayes is suitable for text classification tasks where features are discrete and represent word counts or frequencies; it calculates the likelihood probabilities using the frequency of each word in each class of the label. Bernoulli Naive Bayes is suitable when features are binary (e.g., word presence or absence), and it considers whether features are present or absent in each class of the label.\nSmoothing is required in Naive Bayes models to handle situations where a particular feature (word) in the test data was not present in the training data. This can lead to zero probabilities and cause the model to fail during prediction. Laplace smoothing or add-one smoothing is a common technique where a small constant value (often 1) is added to all counts to avoid zero probabilities. In summary, Naive Bayes is a straightforward yet powerful algorithm for classification tasks, making probabilistic predictions based on Bayes’ theorem and assuming independence between features. Multinomial Naive Bayes works with word frequencies, while Bernoulli Naive Bayes handles binary features, and smoothing is essential to prevent zero probabilities.\nFor this particular project, multinomial Naive Bayes will be used with the feature of product Name, to predict the Category that the product falls into.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#overview",
    "href": "models/bayes.html#overview",
    "title": "Naive Bayes",
    "section": "",
    "text": "How Naive Bayes classifies categories.\n\n\n\n\nNaive Bayes is a popular and simple probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification tasks, such as spam email detection, sentiment analysis, and medical diagnosis. The “naive” part of its name comes from the assumption that features are independent of each other, which is often not true in real-world data but simplifies the model and makes it computationally efficient. It uses Bayes’ Theorem, which calculates the probability of a hypothesis (H) given some evidence (E). Standard Multinomial Naive Bayes is suitable for text classification tasks where features are discrete and represent word counts or frequencies; it calculates the likelihood probabilities using the frequency of each word in each class of the label. Bernoulli Naive Bayes is suitable when features are binary (e.g., word presence or absence), and it considers whether features are present or absent in each class of the label.\nSmoothing is required in Naive Bayes models to handle situations where a particular feature (word) in the test data was not present in the training data. This can lead to zero probabilities and cause the model to fail during prediction. Laplace smoothing or add-one smoothing is a common technique where a small constant value (often 1) is added to all counts to avoid zero probabilities. In summary, Naive Bayes is a straightforward yet powerful algorithm for classification tasks, making probabilistic predictions based on Bayes’ theorem and assuming independence between features. Multinomial Naive Bayes works with word frequencies, while Bernoulli Naive Bayes handles binary features, and smoothing is essential to prevent zero probabilities.\nFor this particular project, multinomial Naive Bayes will be used with the feature of product Name, to predict the Category that the product falls into.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#data-prep",
    "href": "models/bayes.html#data-prep",
    "title": "Naive Bayes",
    "section": "Data Prep",
    "text": "Data Prep\n\n\nWith Naive Bayes, the input data of vectorized text works well; here, multinomial Naive Bayes will be used with the feature of product Name, to predict the Category that the product falls into. Table 1 shows the input data used for all models used, and the data can be found at this link.\n\n\n\n\n\nTable 1: The input feature pre-transformation for input into the model, and the label feature “Category”\n\n\n\n\n\n\n\n\nName\nCategory\n\n\n\n\nBB0117O Silver 57/18/145 unisex Eyewear Frame\nMen's Accessories\n\n\n6 Pack Rinse-Clean Spin Mop Replacement Heads Mop Replace Refills Microfiber Compatible with Spin Mop 2 Tank System,(Small Triangle & Blue)\nHousehold Cleaning Supplies\n\n\n10 PCS Baby Brine Shrimp Net Fish Sieve for Fish Tank Aquarium Fine Artemia Net Small for Brine Shrimp Hatchery\nFish & Aquatic Pets\n\n\nPLANTIFIQUE Foot Peel Mask with Avocado 2 Pack Peeling Foot Mask Dermatologically Tested - Repairs Heels & Removes Dead Skin for Baby Soft Feet - Exfoliating Foot Peel Mask for Dry Cracked Feet\nFoot, Hand & Nail Care Products\n\n\nUnisex-Child Jump Serve Slip on Sneaker\nBoys' Shoes\n\n\n\n\n\n\n\n\n\n\nThe data, before transforming into N-gram word vectors (more info below), must be split into training and testing datasets. This can easily be done by randomly choosing each row to go into training or test, with about 25% of the data going into testing. We also stratify the splitting of data on the label categories, for fairness in evaluation. We split before vectorization for fairness in the conversion/tranformation of the data into vectors.\nIt’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nBased on how the input feature is turned into a vector format, several different models will be run; the following formats of the intput transformed into a sparse matrix vector are used, where a sparse matrix is an efficient representation of a large matrix with many zeros:\n\nCount frequency vector, which does text preprocessing, tokenizing and filtering of stopwords in order to turn a text string into an N-gram of words feature vector. Essentially, each occurence of a word adds to a count in a given row, with the number of columns equaling the number of all words across all input (training) text. Stop words are common English words that should not be considered due to their frequency, like “and” or “the”. The stop words used here are [\"and\", \"for\", \"with\", \"to\"], which are the ones most commonly found in Amazon product titles.\nTerm-frequency (TF) vector, where instead of the count of a word appearing, the number of occurrences of each word in a document is divided by the total number of words in the document. This is to adjust for how longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\nBinary frequency vector, where instead of count a binary 0/1 is used to represent if a word appears in the given row Name. This is used for Bernoulli NB.\n\n\n\nThus, the following NB models will be used with the following data:\n\nMultinomial with count frequency vectors\nMultinomial with TF vectors\nBernoulli with binary frequency vectors\n\n\n\n\n\n&lt;343558x183699 sparse matrix of type '&lt;class 'numpy.float64'&gt; with 5305684 stored elements in Compressed Sparse Row format&gt;\n\n\n\nFigure 1: An example of how sparse matrices are used and the training data as a sparse matrix.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#code",
    "href": "models/bayes.html#code",
    "title": "Naive Bayes",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running Naive Bayes, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#results",
    "href": "models/bayes.html#results",
    "title": "Naive Bayes",
    "section": "Results",
    "text": "Results\nEach model was run as described above in “Data Prep”. The results can be seen in Table 2; as we can see, the base Multinomial NB was the best, with Bernoulli NS significantly underperforming.\n\n\n\nTable 2: The table of result accuracy for each model\n\n\n\n\n\n\n\n\n\n\nModel\nData Format\nAccuracy\n\n\n\n\nMultinomialNB\nCounts\n0.682597\n\n\nMultinomialNB\nTF\n0.606549\n\n\nBernoulliNB\nOccurence\n0.229279\n\n\n\n\n\n\nFor the best performing model, performance metrics were found for each category in the labels; in Table 3 we show the top 10 performing categories, along with the words that were found to have the highest log-prior-probabilities; aka, the words that most influenced the choosing of that category by the model.\n\n\n\nTable 3: The table of performance metrics for the top 15 correctly-labeled categories, along with the most influential words for each category.\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nprecision\nrecall\nf1-score\nTop Feature Words\n\n\n\n\nComputers & Tablets\n0.898876\n0.981595\n0.938416\n[‘laptop’, ‘10’, ‘ssd’, ‘intel’, ‘core’, ‘ram’]\n\n\nKnitting & Crochet Supplies\n0.933824\n0.937962\n0.935888\n[‘set’, ‘kit’, ‘needles’, ‘knitting’, ‘crochet’, ‘yarn’]\n\n\nAdditive Manufacturing Products\n0.937198\n0.9312\n0.934189\n[‘pla’, ‘pro’, ‘filament’, ‘ender’, ‘printer’, ‘3d’]\n\n\nSexual Wellness Products\n0.981538\n0.859838\n0.916667\n[‘vibrator’, ‘women’, ‘dildo’, ‘adult’, ‘toys’, ‘sex’]\n\n\nNeedlework Supplies\n0.926941\n0.900888\n0.913728\n[‘needle’, ‘kits’, ‘cross’, ‘stitch’, ‘kit’, ‘embroidery’]\n\n\nPet Bird Supplies\n0.974684\n0.857939\n0.912593\n[‘parakeet’, ‘toys’, ‘toy’, ‘parrot’, ‘cage’, ‘bird’]\n\n\nLight Bulbs\n0.865291\n0.954485\n0.907702\n[‘base’, ‘pack’, ‘bulbs’, ‘led’, ‘bulb’, ‘light’]\n\n\nCraft & Hobby Fabric\n0.919408\n0.891547\n0.905263\n[‘sewing’, ‘cotton’, ‘the’, ‘by’, ‘yard’, ‘fabric’]\n\n\nWall Art\n0.8475\n0.956276\n0.898608\n[‘room’, ‘canvas’, ‘poster’, ‘decor’, ‘art’, ‘wall’]\n\n\nVacuum Cleaners & Floor Care\n0.819462\n0.952381\n0.880936\n[‘compatible’, ‘filter’, ‘pack’, ‘cleaner’, ‘replacement’, ‘vacuum’]\n\n\nFish & Aquatic Pets\n0.922201\n0.837931\n0.878049\n[‘pump’, ‘water’, ‘filter’, ‘tank’, ‘fish’, ‘aquarium’]\n\n\nCutting Tools\n0.808673\n0.950525\n0.87388\n[‘bits’, ‘shank’, ‘set’, ‘inch’, ‘drill’, ‘bit’]\n\n\nIndustrial Materials\n0.85907\n0.888372\n0.873476\n[‘diy’, ‘length’, ‘12’, ‘in’, ‘sheet’, ‘rubber’]\n\n\nRain Umbrellas\n0.982609\n0.784722\n0.872587\n[‘compact’, ‘folding’, ‘travel’, ‘windproof’, ‘rain’, ‘umbrella’]\n\n\nPower Transmission Products\n0.861373\n0.879121\n0.870156\n[‘steel’, ‘ball’, ‘rubber’, ‘bearings’, ‘bearing’, ‘belt’]\n\n\n\n\n\n\nThe confusion matrix of all categories can be seen in Figure 2. As can be seen, this is not necessarily informative; instead, we create a confusion matrix of the subset of categories that had the top 15 F1-scores in Figure 3.\n\n\n\n\n\n\n\n\nFigure 2: The confusion matrix of all categories\n\n\n\n\n\n\n\n\n\n\nFigure 3: The confusion matrix for top 15 correctly-classified categories",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#conclusion",
    "href": "models/bayes.html#conclusion",
    "title": "Naive Bayes",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the above results, some interesting conclusions can be drawn:\n\nEven though term-frequency (TF) vectors are supposed to be a better data format for input into Naive Bayes models, the count frequency vector data format actually consistently outperformed.\nThough the general accuracy was 68% at best, that’s because there were 248 categories in the label. When examining the performance by each category, many had accuracy over 88% and 6 had performance over 91%.\nSome words were extremely predictive for a unique category; however, some words like “boy” and “clothing” led the model to easily mis-classify the category.\n\nOverall, the multinomial Naive Bayes model performed surprisingly well at predicting the category of a given Amazon product given its product name.\n\n\n\nHow Naive Bayes classifies categories.\nFigure 1: An example of how sparse matrices are used and the training data as a sparse matrix.\nFigure 2: The confusion matrix of all categories\nFigure 3: The confusion matrix for top 15 correctly-classified categories",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "An example of navigating Amazon\n\n\n\n\nIn today’s digital landscape dominated by e-commerce giants such as Amazon, the analysis of retail product data assumes paramount importance as a means to unravel the intricacies of consumer behavior, and unveil the complex relationships that exist between various products. The topic of this project poses a fundamental assertion: that mining the vast repository of Amazon’s retail product data holds the key to unlocking invaluable insights that can revolutionize marketing strategies, inform product development, and optimize overall business operations. This assertion underscores the critical role of data analytics in shaping the success trajectory of businesses operating in the contemporary marketplace. By delving into the granular details of consumer interactions with products on Amazon’s platform, stakeholders gain not only a deeper understanding of consumer preferences but also a nuanced perspective on the dynamics of the digital marketplace itself. The implications of this analysis extend far beyond mere academic curiosity; they are deeply intertwined with the strategic imperatives of businesses seeking to thrive amidst fierce competition and rapid technological advancements. The significance of analyzing Amazon retail product data transcends individual businesses, permeating into the realms of consumer behavior, marketing efficacy, policymaking, and academic research.\nAt its core, this endeavor resonates with a diverse array of stakeholders, each with a vested interest in deciphering the underlying patterns shaping modern consumption patterns and economic dynamics. Consumers, for instance, stand to benefit from the insights gleaned through data analysis, which can translate into more personalized shopping experiences, enhanced product recommendations, and greater transparency within the marketplace. Much work has been done, for example, to improve search engine results and product classification for recommendation purposes (Medini et al. 2019; Smith and Linden 2017). Marketers, on the other hand, find in this data a treasure trove of actionable intelligence, enabling them to fine-tune advertising strategies, optimize pricing models, and anticipate shifts in consumer preferences with a heightened degree of accuracy (Chen, Mislove, and Wilson 2016; Chevalier and Goolsbee 2003). Furthermore, researchers across various disciplines are drawn to the rich dataset offered by Amazon, utilizing sophisticated analytical techniques ranging from machine learning algorithms to traditional statistical methods to unravel the complexities of consumer behavior and product relationships (Shaikh, Rathi, and Janrao 2017; Bell and Bala 2015). Despite the considerable strides made in this field, there remains an abundance of untapped potential, beckoning further exploration and innovation to fully harness the transformative power of Amazon’s retail product data.\n\n\n\n\n\nML in retail\n\n\n\n\nIn this project, machine learning methods play a pivotal role in extracting meaningful insights from the vast sea of Amazon’s retail product data. Techniques such as Principal Component Analysis (PCA) enable dimensionality reduction, allowing for the identification of key features influencing consumer behavior. Clustering algorithms group similar products or consumers together, facilitating targeted marketing strategies and personalized recommendations. Neural networks, with their ability to learn intricate patterns, offer valuable predictive capabilities, aiding in demand forecasting and inventory management. Regression analysis uncovers relationships between variables, potentially helping businesses understand the impact of pricing, promotions, and other factors on sales. Support Vector Machines (SVM), Naive Bayes, and decision trees provide additional tools for classification and prediction tasks, enriching the analytical toolkit available to researchers and practitioners alike. Through the application of these advanced methodologies, one can navigate the complexities of the digital marketplace, leveraging data-driven insights to drive innovation and strategic decision-making.\nTo get specific, the way that machine learning will be used is in both supervised and unsupervised learning settings depends on the data present within this dataset. For unsupervised learning methods, much can be done with the text data (aka the title of the product and the categories); one can do embeddings, bag-of-words, or other methods that reduce text into a couple of meaningful tags/individual words. One can also do dimension reduction and clustering in unsupervised contexts, as there are plently of numerical features in the data (number of reviews, number of stars, price, etc). For supervised learning, the feature used for label could be the feature of “Is Best Seller”, which would give a binary label; or, for regression and numerical/continuous labeling, the feature “Price” could be used as it is an important feature to learn about and could potentially be predicted from the other features present. For both supervised and unsupervised learning, it is recognized that some features are essentially useless or not needed; those would be the unique strings for each product row, namely “ASIN”, “Image URL”, and “URL”. These are important info for getting context for the product (and potentially for aquiring new data in the form of image data), but not necessary for the techniques performed on this site as no computer vision methods will be used.\nIn addition to the techniques mentioned, reinforcement learning models can also be employed to optimize strategies in response to changing market dynamics. By simulating different scenarios and learning from interactions, these models can adapt and refine decision-making processes over time, enhancing business agility and resilience. Moreover, natural language processing (NLP) techniques can extract sentiment analysis from customer reviews, providing valuable insights into product perception and sentiment trends that influence purchasing decisions. The integration of these advanced methodologies into the analysis of Amazon’s retail product data signifies a holistic approach to understanding consumer behavior and market trends, empowering businesses to stay ahead in an ever-evolving digital landscape.\n\n\n\nIs there noticable trends in prices over time, over all products?\nAre there price trends over time within a specific product category?\nIs there any meaningful similarities in the images between products with similar descriptions?\nAre there price/rating similarities between the products with similar images?\nDo ratings have any correlation with the price of products?\nDo ratings have any correlation with the product category?\nCan the price of a product be predicted based on all other factors?\nCan the rating of a product be predicted based on all other factors?\nCan the category of a product be predicted based on the description?\nIs the description of a product similar to the generated description based on the product image (or the generated bag of words, tags, etc)?",
    "crumbs": [
      "Code",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#questions-to-answer",
    "href": "intro.html#questions-to-answer",
    "title": "Introduction",
    "section": "",
    "text": "Is there noticable trends in prices over time, over all products?\nAre there price trends over time within a specific product category?\nIs there any meaningful similarities in the images between products with similar descriptions?\nAre there price/rating similarities between the products with similar images?\nDo ratings have any correlation with the price of products?\nDo ratings have any correlation with the product category?\nCan the price of a product be predicted based on all other factors?\nCan the rating of a product be predicted based on all other factors?\nCan the category of a product be predicted based on the description?\nIs the description of a product similar to the generated description based on the product image (or the generated bag of words, tags, etc)?",
    "crumbs": [
      "Code",
      "Introduction"
    ]
  },
  {
    "objectID": "models/clustering.html",
    "href": "models/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in order to discover patterns in the data.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#overview",
    "href": "models/clustering.html#overview",
    "title": "Clustering",
    "section": "Overview",
    "text": "Overview\nHere, describe clustering, including partitional vs, hierarchical. Note the distance metrics used. Discuss how you plan to use clustering to engage in discovery. Have two images. More is fine.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#data-preperation",
    "href": "models/clustering.html#data-preperation",
    "title": "Clustering",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nClustering Methods\nAll models and methods require specific data formats. Clustering requires ONLY unlabeled numeric data. Explain this and show an image of the sample of data you plan to use. LINK to the sample of data as well.\n\n\n\n\nTable 1: The data used for clustering.\n\n\n\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n0.94\n0\n0.0426\n0.0412\n0\n\n\n0\n0\n0\n0.00825\n0.00799\n0\n\n\n0\n0\n0\n0.0903\n0.0875\n0\n\n\n0\n0.8\n0\n0.018\n0.0225\n0.333\n\n\n0\n1\n0\n0.0205\n0.0199\n0",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#code",
    "href": "models/clustering.html#code",
    "title": "Clustering",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the PCA can be found here, or click on the link card to the right.\n\n\n\n\n\n\nUse R or Python to code k-means clustering and to code hierarchical clustering. Use Cosine Similarity as the distance measure for the hclust. LINK to the code.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#results",
    "href": "models/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) The elbow plot of WCSS vs rank.\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) The plot of silhouette scores vs rank.\n\n\n\n\n\n\n\nFigure 1: The plots showcasing the effectiveness of different ranks with Kmeans clustering.\n\n\n\n\n\n\n\n\n\nFigure 2: A dengrogram of the hierarchical clustering performed.\n\n\n\n\n\n\n\n                                                \n\n\nFigure 3: A comparison of the clustering models ran, at rank 2, using the adjusted Rand Score Index.\n\n\n\n\n\n\n\n\n                                                \n\n\nFigure 4: A splom of selected features (and the subset of rows used in the clustering methods), colored by the cluster assigned to the data point.\n\n\n\n\nDiscuss, illustrate, describe, and visualize the results. Have at least one dendrogram and at least one clustering image (that is not a dendrogram). What values of k did you use for k - means (you should use at least 3 different k values to compare). What did the Silhouette method tell you about the “best k”. Include a vis of the silhouette results. Using the hierarchical clustering, how did this compare and coincide with the k-means? What value of k does hclust suggest?",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#conclusions",
    "href": "models/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nWhat did you learn that pertains to your topic?",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/svm.html",
    "href": "models/svm.html",
    "title": "SVM (Support Vector Machine)",
    "section": "",
    "text": "Example SVM\n\n\n\n\nSupport Vector Machines (SVMs) are a type of supervised learning model used for classification and regression tasks. They work by finding the hyperplane that best separates the data points into different classes.\nSVMs are considered linear separators because they aim to find the best hyperplane that separates data points of different classes with the maximum margin. This hyperplane is defined by a set of weights and biases that are optimized during the training process to minimize classification errors. The kernel function in SVMs allows them to handle nonlinear data by implicitly mapping the input data into a higher-dimensional space where a linear separation is possible. The dot product plays a critical role in the kernel function because it measures the similarity between two data points in this higher-dimensional space.\n\n\n\n\n\nExample SVM\n\n\n\n\n\n\n\nPolynomial Kernel: \\(K(x, y) = (x \\cdot y + r)^d\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(r\\) is a constant term.\n\\(d\\) is the degree of the polynomial.\n\nRBF (Radial Basis Function) Kernel: \\(K(x, y) = \\exp(-\\gamma \\| x - y \\|^2)\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(\\gamma\\) is a hyperparameter that determines the “spread” of the kernel.\n\n\n\n\n\nLet’s say we have a 2D point \\((x, y) = (3, 4)\\) and we want to use a polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to “cast” this point into the proper number of dimensions.\n\nOriginal Point: \\((x, y) = (3, 4)\\)\nPolynomial Kernel Transformation: \\[\n\\begin{align*}\nK((3, 4), (x, y)) & = ((3 \\cdot x + 4 \\cdot y) + 1)^2 \\\\\n& = ((3x + 4y) + 1)^2 \\\\\n& = (3x + 4y + 1)^2\n\\end{align*}\n\\]\n\nSo, applying the polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to the point \\((3, 4)\\) transforms it into the new representation \\((3x + 4y + 1)^2\\) in the higher-dimensional space. This transformed representation enables SVMs to find a nonlinear decision boundary.\nIn summary, SVMs are powerful classifiers that use linear separators in higher-dimensional spaces facilitated by kernel functions like the polynomial and RBF kernels, which allow them to handle nonlinear data effectively.\nIn this project, SVMs will be used specifically to categorize and predict whether a product is a best seller.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#overview",
    "href": "models/svm.html#overview",
    "title": "SVM (Support Vector Machine)",
    "section": "",
    "text": "Example SVM\n\n\n\n\nSupport Vector Machines (SVMs) are a type of supervised learning model used for classification and regression tasks. They work by finding the hyperplane that best separates the data points into different classes.\nSVMs are considered linear separators because they aim to find the best hyperplane that separates data points of different classes with the maximum margin. This hyperplane is defined by a set of weights and biases that are optimized during the training process to minimize classification errors. The kernel function in SVMs allows them to handle nonlinear data by implicitly mapping the input data into a higher-dimensional space where a linear separation is possible. The dot product plays a critical role in the kernel function because it measures the similarity between two data points in this higher-dimensional space.\n\n\n\n\n\nExample SVM\n\n\n\n\n\n\n\nPolynomial Kernel: \\(K(x, y) = (x \\cdot y + r)^d\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(r\\) is a constant term.\n\\(d\\) is the degree of the polynomial.\n\nRBF (Radial Basis Function) Kernel: \\(K(x, y) = \\exp(-\\gamma \\| x - y \\|^2)\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(\\gamma\\) is a hyperparameter that determines the “spread” of the kernel.\n\n\n\n\n\nLet’s say we have a 2D point \\((x, y) = (3, 4)\\) and we want to use a polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to “cast” this point into the proper number of dimensions.\n\nOriginal Point: \\((x, y) = (3, 4)\\)\nPolynomial Kernel Transformation: \\[\n\\begin{align*}\nK((3, 4), (x, y)) & = ((3 \\cdot x + 4 \\cdot y) + 1)^2 \\\\\n& = ((3x + 4y) + 1)^2 \\\\\n& = (3x + 4y + 1)^2\n\\end{align*}\n\\]\n\nSo, applying the polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to the point \\((3, 4)\\) transforms it into the new representation \\((3x + 4y + 1)^2\\) in the higher-dimensional space. This transformed representation enables SVMs to find a nonlinear decision boundary.\nIn summary, SVMs are powerful classifiers that use linear separators in higher-dimensional spaces facilitated by kernel functions like the polynomial and RBF kernels, which allow them to handle nonlinear data effectively.\nIn this project, SVMs will be used specifically to categorize and predict whether a product is a best seller.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/regression.html",
    "href": "models/regression.html",
    "title": "Regression",
    "section": "",
    "text": "Regression\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Regression"
    ]
  },
  {
    "objectID": "models/dtrees.html",
    "href": "models/dtrees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision tree structure\n\n\n\n\nDecision trees are a popular supervised learning algorithm used for both classification and regression tasks in machine learning. They are tree-like structures where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (for classification) or a numerical value (for regression). In classification, they can classify instances by learning decision rules inferred from the features. With regression, they can predict continuous values by averaging the target values of instances falling into the same leaf. When it comes to determining the “goodness” of a split in the decision tree and updating it, several methods are employed. Gini Impurity measures the impurity of a node, where a node is pure (GINI = 0) if all instances below a node belong to the same class. Entropy measures the randomness or uncertainty of a node; it is maximum (1) when all classes are equally distributed and decreases as the node becomes more pure. Information Gain is used to determine the best split in a decision tree. It calculates the difference in entropy (or GINI impurity) before and after the split. The split with the highest information gain is chosen.\n\n\n\n\n\nThe decision tree structure for the given example\n\n\n\n\nFor example, consider a dataset with weather attributes (including the feature “Outlook”) and a target class ‘Play’ (Yes/No). Suppose we want to split based on the ‘Outlook’ attribute. We calculate the entropy for the current node (before split), then we calculate the entropy for each possible branch (‘Sunny’, ‘Overcast’, ‘Rainy’). We then use Information Gain to determine the best split.\nIt’s generally possible to create an infinite number of decision trees because:\n\nThere can be different orderings of features for splitting.\nFor continuous features, there can be infinitely many possible split points.\nTree depth and complexity can vary widely, leading to numerous possible structures.\n\nTo manage this complexity and avoid overfitting, techniques like pruning, limiting tree depth, and using ensemble methods (like Random Forests) are employed. These strategies help create more generalizable and robust decision trees.\nIn this particular project, classification decision trees will be used to predict whether a product is a best seller or not; regression decision trees will be used to predict the price of a product. All numerical, boolean, and categorical features of the dataset will be used in the prediction (but not the unique string features like “Name” and “ASIN”).\n\n\n\nThe data features used were:\n\n\"Stars\", \"Reviews\", \"Price\", \"Date Scraped\", \"Bought In Month\" for predicting \"Is Best Seller\"\n\"Stars\", \"Reviews\", \"Date Scraped\", \"Bought In Month\", \"Is Best Seller\" for predicting \"Price\".\n\nThe data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models:\n\nOnly select rows that contain a non-NaN value for Category and Bought in Month\nSelect rows such that 1% of them are Is Best Seller == True. This is because the dataset contains more than &gt;1M rows, with only 0.4% of them Is Best Seller == True; this makes the prediction less biased and invariant for the model.\nMake datetime binary, as there are only 2 values used for it anyways.\n\nAs DTs are invariant to the scale of the data, no normalization/min-max scaling was needed.\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nTable 1 shows the final input data used for all models used, and the data can be found at this link. The test data is exactly the same format as the pictured samples of training data, except with 75% less rows.\n\n\n\nTable 1: The input training and test data for the classifying and regression decision trees.\n\n\n\n\n\nThe training features for the classification DT\n\n\nStars\nReviews\nPrice\nDate Scraped\nBought In Month\n\n\n\n\n4.3\n0\n45.99\n0\n0\n\n\n4.2\n0\n22.99\n0\n100\n\n\n4.9\n0\n345\n0\n0\n\n\n5\n0\n28.49\n1\n0\n\n\n4.6\n146\n23.55\n1\n0\n\n\n4.7\n0\n71.13\n0\n0\n\n\n\n\n\n\nThe label for the classification DT\n\n\nIs Best Seller\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\nThe training features for the regression DT\n\n\n\n\n\n\n\n\n\nStars\nReviews\nDate Scraped\nBought In Month\nIs Best Seller\n\n\n\n\n3.5\n0\n0\n0\nFalse\n\n\n4.7\n3874\n0\n50\nFalse\n\n\n0\n0\n0\n50\nFalse\n\n\n4.6\n0\n0\n50\nFalse\n\n\n5\n0\n0\n700\nFalse\n\n\n4.4\n37\n0\n0\nFalse\n\n\n\n\n\n\nThe label for the regression DT\n\n\nPrice\n\n\n\n\n24.99\n\n\n16.99\n\n\n33.99\n\n\n31.99\n\n\n9.99\n\n\n20.38\n\n\n\n\n\n\n\n\n\n\n\nAs described above, a classification DT and a regression DT were run on their respective subsets of the dataset, using the standard defaults given by sklearn. In addition, a third DT was run: using random forests for classification (using the same dataset as for the standard classification DT). Random forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. Random decision forests correct for decision trees’ habit of overfitting to their training set.\n\n\n\n\n\nThe jupyter notebook code for running decision trees, data prep, and everything else on this page can be found here, or click on the link card to the right.\n\n\n\n\n\n\n\n\n\nThe models were run, and the standard metrics for evaluating classification (including accuracy) and regression (including R^2 score) are shown in Table 2. As we can see, the random forest classfier (described from here on as “best classifier DT”) was the better performing model slightly; we can also see that the regression DT did not perform well.\n\n\n\nTable 2: Standard metrics showing the performance of the 3 models.\n\n\n\n\n\nThe performance of the regression DT.\n\n\nMetric\nValue\n\n\n\n\nMean Squared Error (MSE)\n3996.33\n\n\nMean Absolute Error (MAE)\n32.5754\n\n\nR^2\n-0.030492\n\n\n\n\n\n\nThe performance of the 2 classification DT models.\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n\nStandard Classifier DT\nFalse\n0.990381\n0.995355\n0.992862\n166850\n\n\n\n\nTrue\n0.0673887\n0.033553\n0.0448\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.985829\n\n\n\nmacro avg\n0.528885\n0.514454\n0.518831\n168519\n\n\n\n\nweighted avg\n0.98124\n0.985829\n0.983472\n168519\n\n\n\nRandom Forest Classifier\nFalse\n0.990312\n0.997393\n0.99384\n166850\n\n\n\n\nTrue\n0.0861345\n0.0245656\n0.0382284\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.987758\n\n\n\nmacro avg\n0.538223\n0.510979\n0.516034\n168519\n\n\n\n\nweighted avg\n0.981357\n0.987758\n0.984376\n168519\n\n\n\n\n\n\n\n\n\nWe also show the structure of the decision trees at the first 3 levels (all trees had more than 50 final levels which was unable to be visualized). In Figure 1, we can see that Bought in Month feature was very important to all 3 trees; we can also see that Stars and Price has a very low Gini index for both classifier DTs with Stars also appearing early in the tree, indicating the features’ importances to the predictions.\n\n\n\n\n\n\n\n\nRandom Forest Classifier\n\n\n\n\n\n\n\nStandard Classifier\n\n\n\n\n\n\n\nRegression DT\n\n\n\n\n\n\nFigure 1: A visualization of the first 3 levels of the 3 decision tree models run.\n\n\n\n\n\nWe show the basic plot of true vs predicted label (aka Price) for the regression decision tree in Figure 2; we can see that unlike the straight line of scatter points we would see in a perfect classification, we have an approximate log-negative trend. The model tended to overprice items with actual price &lt; $100, and underprice the products with high prices.\nWe also show the predicted price vs the error residuals in Figure 3; this plot gives similar insight to the previous, where we can see highest error on low predicted prices and less, but still increasing, error on higher-predicted prices.\n\n\n\n\n\n\n\n\nFigure 2: True vs predicted label for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Predicted price vs the residuals for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFor the best performing classifier DT, we show the confusion matrix of the labels in Figure 4. As we can see, The model did great on correctly classifying not best-sellers; however, that was because there were very few true best-sellers (this agrees with the F1 scores per-category in the above table). The model struggled with correctly classifying true best sellers and had many more FP and FN than TP.\n\n\n\n\n\n\n\nFigure 4: The confusion matrix of true vs predicted labels of the random forest classifier.\n\n\n\n\n\n\n\n\n\nFor the classification task, some conclusions and observations can be drawn from the results:\n\nThe accuracy is deceptively high for both classifiers, as the second True label only was present in 1% of the data.\nThe classifiers were biased towards predicting that a product was NOT a best-seller, as that gave the highest accuracy ultimately.\nAlthough the model had more FP than TP, overall it had many more FN than FP, indicating once again that the model tended to classify products as not best-sellers.\n\nFor the regression task, similarly we can draw observations from the results:\n\nThough the MAE was low, the low R^2 score indicates that the model overall tended to not predict the price well.\nBased on the residuals plots, the model did best (most residuals near zero) when the predicted price was around $150.\nThe model tended to underprice and overprice when further from the $150 price value.\n\nOverall, it seemed the models struggled to perform well but did adequately, especially the regression model which was mildly suprising. It can be concluded that the best-seller status of a product, along with price, is somewhat hard to predict from just the numerical values present in the dataset.\n\n\n\nDecision tree structure\nThe decision tree structure for the given example\nRandom Forest Classifier\nStandard Classifier\nRegression DT\nFigure 2: True vs predicted label for the regression DT.\nFigure 3: Predicted price vs the residuals for the regression DT.\nFigure 4: The confusion matrix of true vs predicted labels of the random forest classifier.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#overview",
    "href": "models/dtrees.html#overview",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision tree structure\n\n\n\n\nDecision trees are a popular supervised learning algorithm used for both classification and regression tasks in machine learning. They are tree-like structures where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (for classification) or a numerical value (for regression). In classification, they can classify instances by learning decision rules inferred from the features. With regression, they can predict continuous values by averaging the target values of instances falling into the same leaf. When it comes to determining the “goodness” of a split in the decision tree and updating it, several methods are employed. Gini Impurity measures the impurity of a node, where a node is pure (GINI = 0) if all instances below a node belong to the same class. Entropy measures the randomness or uncertainty of a node; it is maximum (1) when all classes are equally distributed and decreases as the node becomes more pure. Information Gain is used to determine the best split in a decision tree. It calculates the difference in entropy (or GINI impurity) before and after the split. The split with the highest information gain is chosen.\n\n\n\n\n\nThe decision tree structure for the given example\n\n\n\n\nFor example, consider a dataset with weather attributes (including the feature “Outlook”) and a target class ‘Play’ (Yes/No). Suppose we want to split based on the ‘Outlook’ attribute. We calculate the entropy for the current node (before split), then we calculate the entropy for each possible branch (‘Sunny’, ‘Overcast’, ‘Rainy’). We then use Information Gain to determine the best split.\nIt’s generally possible to create an infinite number of decision trees because:\n\nThere can be different orderings of features for splitting.\nFor continuous features, there can be infinitely many possible split points.\nTree depth and complexity can vary widely, leading to numerous possible structures.\n\nTo manage this complexity and avoid overfitting, techniques like pruning, limiting tree depth, and using ensemble methods (like Random Forests) are employed. These strategies help create more generalizable and robust decision trees.\nIn this particular project, classification decision trees will be used to predict whether a product is a best seller or not; regression decision trees will be used to predict the price of a product. All numerical, boolean, and categorical features of the dataset will be used in the prediction (but not the unique string features like “Name” and “ASIN”).",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#data-prep",
    "href": "models/dtrees.html#data-prep",
    "title": "Decision Trees",
    "section": "",
    "text": "The data features used were:\n\n\"Stars\", \"Reviews\", \"Price\", \"Date Scraped\", \"Bought In Month\" for predicting \"Is Best Seller\"\n\"Stars\", \"Reviews\", \"Date Scraped\", \"Bought In Month\", \"Is Best Seller\" for predicting \"Price\".\n\nThe data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models:\n\nOnly select rows that contain a non-NaN value for Category and Bought in Month\nSelect rows such that 1% of them are Is Best Seller == True. This is because the dataset contains more than &gt;1M rows, with only 0.4% of them Is Best Seller == True; this makes the prediction less biased and invariant for the model.\nMake datetime binary, as there are only 2 values used for it anyways.\n\nAs DTs are invariant to the scale of the data, no normalization/min-max scaling was needed.\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nTable 1 shows the final input data used for all models used, and the data can be found at this link. The test data is exactly the same format as the pictured samples of training data, except with 75% less rows.\n\n\n\nTable 1: The input training and test data for the classifying and regression decision trees.\n\n\n\n\n\nThe training features for the classification DT\n\n\nStars\nReviews\nPrice\nDate Scraped\nBought In Month\n\n\n\n\n4.3\n0\n45.99\n0\n0\n\n\n4.2\n0\n22.99\n0\n100\n\n\n4.9\n0\n345\n0\n0\n\n\n5\n0\n28.49\n1\n0\n\n\n4.6\n146\n23.55\n1\n0\n\n\n4.7\n0\n71.13\n0\n0\n\n\n\n\n\n\nThe label for the classification DT\n\n\nIs Best Seller\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\nThe training features for the regression DT\n\n\n\n\n\n\n\n\n\nStars\nReviews\nDate Scraped\nBought In Month\nIs Best Seller\n\n\n\n\n3.5\n0\n0\n0\nFalse\n\n\n4.7\n3874\n0\n50\nFalse\n\n\n0\n0\n0\n50\nFalse\n\n\n4.6\n0\n0\n50\nFalse\n\n\n5\n0\n0\n700\nFalse\n\n\n4.4\n37\n0\n0\nFalse\n\n\n\n\n\n\nThe label for the regression DT\n\n\nPrice\n\n\n\n\n24.99\n\n\n16.99\n\n\n33.99\n\n\n31.99\n\n\n9.99\n\n\n20.38",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#models-used",
    "href": "models/dtrees.html#models-used",
    "title": "Decision Trees",
    "section": "",
    "text": "As described above, a classification DT and a regression DT were run on their respective subsets of the dataset, using the standard defaults given by sklearn. In addition, a third DT was run: using random forests for classification (using the same dataset as for the standard classification DT). Random forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. Random decision forests correct for decision trees’ habit of overfitting to their training set.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#code",
    "href": "models/dtrees.html#code",
    "title": "Decision Trees",
    "section": "",
    "text": "The jupyter notebook code for running decision trees, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#results",
    "href": "models/dtrees.html#results",
    "title": "Decision Trees",
    "section": "",
    "text": "The models were run, and the standard metrics for evaluating classification (including accuracy) and regression (including R^2 score) are shown in Table 2. As we can see, the random forest classfier (described from here on as “best classifier DT”) was the better performing model slightly; we can also see that the regression DT did not perform well.\n\n\n\nTable 2: Standard metrics showing the performance of the 3 models.\n\n\n\n\n\nThe performance of the regression DT.\n\n\nMetric\nValue\n\n\n\n\nMean Squared Error (MSE)\n3996.33\n\n\nMean Absolute Error (MAE)\n32.5754\n\n\nR^2\n-0.030492\n\n\n\n\n\n\nThe performance of the 2 classification DT models.\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n\nStandard Classifier DT\nFalse\n0.990381\n0.995355\n0.992862\n166850\n\n\n\n\nTrue\n0.0673887\n0.033553\n0.0448\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.985829\n\n\n\nmacro avg\n0.528885\n0.514454\n0.518831\n168519\n\n\n\n\nweighted avg\n0.98124\n0.985829\n0.983472\n168519\n\n\n\nRandom Forest Classifier\nFalse\n0.990312\n0.997393\n0.99384\n166850\n\n\n\n\nTrue\n0.0861345\n0.0245656\n0.0382284\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.987758\n\n\n\nmacro avg\n0.538223\n0.510979\n0.516034\n168519\n\n\n\n\nweighted avg\n0.981357\n0.987758\n0.984376\n168519\n\n\n\n\n\n\n\n\n\nWe also show the structure of the decision trees at the first 3 levels (all trees had more than 50 final levels which was unable to be visualized). In Figure 1, we can see that Bought in Month feature was very important to all 3 trees; we can also see that Stars and Price has a very low Gini index for both classifier DTs with Stars also appearing early in the tree, indicating the features’ importances to the predictions.\n\n\n\n\n\n\n\n\nRandom Forest Classifier\n\n\n\n\n\n\n\nStandard Classifier\n\n\n\n\n\n\n\nRegression DT\n\n\n\n\n\n\nFigure 1: A visualization of the first 3 levels of the 3 decision tree models run.\n\n\n\n\n\nWe show the basic plot of true vs predicted label (aka Price) for the regression decision tree in Figure 2; we can see that unlike the straight line of scatter points we would see in a perfect classification, we have an approximate log-negative trend. The model tended to overprice items with actual price &lt; $100, and underprice the products with high prices.\nWe also show the predicted price vs the error residuals in Figure 3; this plot gives similar insight to the previous, where we can see highest error on low predicted prices and less, but still increasing, error on higher-predicted prices.\n\n\n\n\n\n\n\n\nFigure 2: True vs predicted label for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Predicted price vs the residuals for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFor the best performing classifier DT, we show the confusion matrix of the labels in Figure 4. As we can see, The model did great on correctly classifying not best-sellers; however, that was because there were very few true best-sellers (this agrees with the F1 scores per-category in the above table). The model struggled with correctly classifying true best sellers and had many more FP and FN than TP.\n\n\n\n\n\n\n\nFigure 4: The confusion matrix of true vs predicted labels of the random forest classifier.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#conclusion",
    "href": "models/dtrees.html#conclusion",
    "title": "Decision Trees",
    "section": "",
    "text": "For the classification task, some conclusions and observations can be drawn from the results:\n\nThe accuracy is deceptively high for both classifiers, as the second True label only was present in 1% of the data.\nThe classifiers were biased towards predicting that a product was NOT a best-seller, as that gave the highest accuracy ultimately.\nAlthough the model had more FP than TP, overall it had many more FN than FP, indicating once again that the model tended to classify products as not best-sellers.\n\nFor the regression task, similarly we can draw observations from the results:\n\nThough the MAE was low, the low R^2 score indicates that the model overall tended to not predict the price well.\nBased on the residuals plots, the model did best (most residuals near zero) when the predicted price was around $150.\nThe model tended to underprice and overprice when further from the $150 price value.\n\nOverall, it seemed the models struggled to perform well but did adequately, especially the regression model which was mildly suprising. It can be concluded that the best-seller status of a product, along with price, is somewhat hard to predict from just the numerical values present in the dataset.\n\n\n\nDecision tree structure\nThe decision tree structure for the given example\nRandom Forest Classifier\nStandard Classifier\nRegression DT\nFigure 2: True vs predicted label for the regression DT.\nFigure 3: Predicted price vs the residuals for the regression DT.\nFigure 4: The confusion matrix of true vs predicted labels of the random forest classifier.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Conclusions\nPlaceholder\n\n\nReferences\n\n\nBell, Sean, and Kavita Bala. 2015. “Learning Visual Similarity for Product Design with Convolutional Neural Networks.” ACM Trans. Graph. 34 (4). https://doi.org/10.1145/2766959.\n\n\nChen, Le, Alan Mislove, and Christo Wilson. 2016. “An Empirical Analysis of Algorithmic Pricing on Amazon Marketplace.” In Proceedings of the 25th International Conference on World Wide Web, 1339–49. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2883089.\n\n\nChevalier, Judith, and Austan Goolsbee. 2003. “Measuring Prices and Price Competition Online: Amazon.com and BarnesandNoble.com.” Quantitative Marketing and Economics 1 (2): 203–22. https://doi.org/10.1023/A:1024634613982.\n\n\nMedini, Tharun, Qixuan Huang, Yiqiu Wang, Vijai Mohan, and Anshumali Shrivastava. 2019. “Extreme Classification in Log Memory Using Count-Min Sketch: A Case Study of Amazon Search with 50M Products.” CoRR abs/1910.13830. http://arxiv.org/abs/1910.13830.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nShaikh, Shakila, Sheetal Rathi, and Prachi Janrao. 2017. “Recommendation System in e-Commerce Websites: A Graph Based Approached.” In 2017 IEEE 7th International Advance Computing Conference (IACC), 931–34. https://doi.org/10.1109/IACC.2017.0189.\n\n\nSmith, Brent, and Greg Linden. 2017. “Two Decades of Recommender Systems at Amazon.com.” IEEE Internet Computing 21 (3): 12–18. https://doi.org/10.1109/MIC.2017.72.\n\n\nVos, Nelis J. de. 2015--2024. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.",
    "crumbs": [
      "Code",
      "Conclusions"
    ]
  }
]