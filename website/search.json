[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Prep / EDA",
    "section": "",
    "text": "Where the data source, processing, and visualization (EDA) is presented.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-collection",
    "href": "data.html#data-collection",
    "title": "Data Prep / EDA",
    "section": "Data Collection",
    "text": "Data Collection\nAmazon product information was scraped from the website using the API service ScraperAPI; this is because, as Amazon is a hugely popular website, they have many anti-scraping measures in place such as rate-limiting, IP blocking, dymamic loading, and such. Using the external API service, these limitations were able to be avoided. The search queries chosen to search for items were based on top 100 Amazon searches, found on this site and this site. An example of using the API, along with its core endpoint, is below.\n\nimport requests\n\npayload = {\n   'api_key': 'API_KEY',\n   'query': 'iphone 15 charger',\n   's': 'price-asc-rank'\n}\n\nresponse = requests.get('https://api.scraperapi.com/structured/amazon/search',\n                        params=payload).json()\n\nThe jupyter notebook code for the web scraping can be found here.\nAdditionally, more data was used to supplement the existing data. Since the scraped data was only about 26K rows, a Kaggle dataset was used that contains more than one million rows, had around the same fields as the scraped data, and was also from the USA (many Amazon Kaggle datasets were from the non-US).\nThe raw data from both sources can be seen below in Table 1; the scraped raw data CSV can also be viewed here.\n\n\n\n\nTable 1: The raw data from both datasets.\n\n\n\n\n\n\n\n\n\n(a) The raw data scraped from Amazon using ScraperAPI\n\n\n\n\n\n\ntype\nposition\nasin\nname\nimage\nhas_prime\nis_best_seller\nis_amazon_choice\nis_limited_deal\nstars\ntotal_reviews\nurl\navailability_quantity\nspec\nprice_string\nprice_symbol\nprice\noriginal_price\nsection_name\n\n\n\n\n0\nsearch_product\n32\nB0CV5Z91QR\nBetter Home Products Megan Wooden 6 Drawer Dou...\nhttps://m.media-amazon.com/images/I/81fL4qHZ4i...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/Better-Home-Products-Wo...\nNaN\n{}\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nsearch_product\n40\nB0CTZQ73SH\nFixtureDisplays® Bathtub and Shower Tension St...\nhttps://m.media-amazon.com/images/I/51F0eGaPI+...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/FixtureDisplays%C2%AE-B...\nNaN\n{}\n$22.60\n$\n22.60\nNaN\nNaN\n\n\n2\nsearch_product\n15\nB0CR633W5W\n4-Port USB 3.0 Hub Ultra-Thin Expand PC Connec...\nhttps://m.media-amazon.com/images/I/51sJxlimwP...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/4-Port-Ultra-Thin-Expan...\nNaN\n{}\n$3.99\n$\n3.99\nNaN\nNaN\n\n\n3\nsearch_product\n15\nB0CTQSPM8G\nMK000960GWSSD 960GB SATA 6G MU SFF SC DS SSD-1...\nhttps://m.media-amazon.com/images/I/11rdwQUxu0...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/Genuine-Original-MK0009...\nNaN\n{}\n$375.00\n$\n375.00\nNaN\nNaN\n\n\n4\nsearch_product\n44\nB0CJ9ZF7LQ\nSOL DE JANEIROCheirosa '62 Hair & Body Fragran...\nhttps://m.media-amazon.com/images/I/31mCVvwGqC...\nFalse\nFalse\nFalse\nFalse\n4.6\n14.0\nhttps://www.amazon.com/SOL-JANEIRO-Cheirosa-Fr...\nNaN\n{}\n$62.00\n$\n62.00\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The raw data gotten from Kaggle\n\n\n\n\n\n\nasin\ntitle\nimgUrl\nproductURL\nstars\nreviews\nprice\nlistPrice\ncategory_id\nisBestSeller\nboughtInLastMonth\n\n\n\n\n0\nB01CR9B6UY\nDC Cargo E-Track Ratcheting Straps Cargo Tie-D...\nhttps://m.media-amazon.com/images/I/81KFTHU7xx...\nhttps://www.amazon.com/dp/B01CR9B6UY\n4.7\n593\n36.99\n0.00\n24\nFalse\n100\n\n\n1\nB0BWFRDBBW\nLittle Girls Glitter Tulle Dress, Sparkle Polk...\nhttps://m.media-amazon.com/images/I/711QwZxcW8...\nhttps://www.amazon.com/dp/B0BWFRDBBW\n3.9\n7\n46.99\n0.00\n91\nFalse\n0\n\n\n2\nB00XV8CQGO\nClub Little Girl Summer Animal Collection Stic...\nhttps://m.media-amazon.com/images/I/41wQCnt7+E...\nhttps://www.amazon.com/dp/B00XV8CQGO\n0.0\n0\n9.99\n0.00\n95\nFalse\n50\n\n\n3\nB08MBJX3K1\nToddler Boys Girls Sneakers Size 5-12 Lightwei...\nhttps://m.media-amazon.com/images/I/81IscOXT6S...\nhttps://www.amazon.com/dp/B08MBJX3K1\n4.5\n0\n31.99\n35.99\n97\nFalse\n0\n\n\n4\nB08YKJDWLF\nCFS Cut to Fit Carbon Pad Pre Filter Roll for ...\nhttps://m.media-amazon.com/images/I/61j4Gpu4jH...\nhttps://www.amazon.com/dp/B08YKJDWLF\n4.6\n0\n12.59\n0.00\n171\nFalse\n0",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "Data Prep / EDA",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe datasets were cleaned seperately, then concatenated, then some final steps were taken to clean it.\nThe steps to clean the web-scaped data were:\n\nAdd date_scraped column\nRemove unecessary columns: type, position, has_prime, is_amazon_choice, is_limited_deal, availability_quantity, spec, price_string, price_symbol, section_name\nExpand and fix original_price\nRename columns to match standard snake case for merging both datasets\nDrop rows with no asin or name or price\nDrop rows with price of 0.0, since that doesn’t make sense\nFill NaN reviews column with 0\n\nThe steps to clean the Kaggle data were:\n\nAdd date_scraped column\nDrop rows with any NaNs\nFix list_price of $0 to be instead equal to price\nChange category_id to actual category by using category table\nDrop rows with price of $0, since that doesn’t make sense\nRename columns to match standard snake case for merging both datasets\n\nAnd then, after they were concatenated, the steps to clean were:\n\nRemove duplicates (by asin + date scraped)\nRename columns\n\nThe final cleaned (and concatenated) dataset can be seen in Table 2 (with the original raw data in Table 1):\n\n\n\n\nTable 2: The final unioned, cleaned, and processed data.\n\n\n\n\n\n\n\n\n\n\nAsin\nName\nImage Url\nIs Best Seller\nStars\nReviews\nUrl\nPrice\nDate Scraped\nList Price\nBought In Month\nCategory\n\n\n\n\n0\nB077H6599Q\nGenerac 7103 Cold Kit for 9kW - 22kW Air Coole...\nhttps://m.media-amazon.com/images/I/71+HddWhsK...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B077H6599Q\n94.99\n2023-11-01\n105.99\n0.0\nRV Parts & Accessories\n\n\n1\nB095HLD52Z\n3 Pack Apple MFi Certified iPhone Charger Cabl...\nhttps://m.media-amazon.com/images/I/61rqFEt6ku...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B095HLD52Z\n20.99\n2023-11-01\n20.99\n0.0\nAccessories & Supplies\n\n\n2\nB08XK4CBP8\n164pcs Blush White Balloons Garland Arch Kit P...\nhttps://m.media-amazon.com/images/I/71oD9RPq84...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B08XK4CBP8\n14.99\n2023-11-01\n16.99\n50.0\nParty Decorations\n\n\n3\nB0C1Z9G9J9\nMudtun Bone Conduction Earbuds for Small Ear C...\nhttps://m.media-amazon.com/images/I/51js5mISHA...\nFalse\n3.5\n74.0\nhttps://www.amazon.com/dp/B0C1Z9G9J9\n19.99\n2023-11-01\n19.99\n0.0\nHeadphones & Earbuds\n\n\n4\nB0C5NB4SRN\nV-SHURA Music Gifts Table Lamp, Guitar Table L...\nhttps://m.media-amazon.com/images/I/710DKiJYTV...\nFalse\n4.8\n0.0\nhttps://www.amazon.com/dp/B0C5NB4SRN\n55.99\n2023-11-01\n55.99\n0.0\nHome Lighting & Ceiling Fans\n\n\n\n\n\n\n\n\n\n\nThe code for the data cleaning can be found here.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-preprocessing-visualization",
    "href": "data.html#data-preprocessing-visualization",
    "title": "Data Prep / EDA",
    "section": "Data Preprocessing / Visualization",
    "text": "Data Preprocessing / Visualization\nVarious types of EDA were performed in order to examine the data; as a note, most visuals are interactive (zoomable, pannable, etc). The code for all visualizations can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website) until all figures load.\n\n\nFirst, a histogram of all categories of all Amazon products is shown in Figure 1. Note scraped data did not have categories, but the Kaggle data did. It can be seen that from the figure, girl’s and boy’s clothing are the most populous categories, with toys & games the next most populous.\n\n\n\n\n                                                \n\n\nFigure 1: A histogram of all categories of all Amazon products. Note scraped data did not have categories, but the Kaggle data did.\n\n\n\n\nFigure 2 shows a histogram of the stars recieved for a product. It can be seen that the number of stars increases generally exponentially until 4.6 stars, with peaks at 0 and 5 stars as well. This data generally agrees with what can be seen on Amazon when browsing personally, and the peaks at 0 and 5 agree with having low number of reviews.\n\n\n\n\n                                                \n\n\nFigure 2: Histogram of the number of stars recieved for all products.\n\n\n\n\nFigure 3 shows violin plots of the numerical factors: volume of product bought in the month, the price of a product, and the number of reviews. A violin plot was chosen because a histogram would be visually uninformative, due to the amount of outliers. Some insights that can be drawn from these violin plots are:\n\nThese factors have a good number of outliers that are extremely large in value compared to the general distribution of values. This does not seem to be from incorrect values, but from sellers attempting to “game the system”.\n“Bought in Month” has outliers that are regularly spaced, indicating either a form of gaming the system by sellers that tend to follow specific formats, or Amazon only reports volume of bought products at low granularity for high-volume products.\nPrice tends to have outliers that continue regularly up the price scale, except for a couple at the $60-70k range. It is unclear whether to drop those rows containing those prices as they seem valid; it is entirely possible that sellers just list the product at a completely unreasonable price as a placeholder, and don’t expect anyone to purchase at that price.\nThe number of reviews for a product have been shown to be easily gameable with fake reviews, and is likely what is happening here.\n\n\n\n\n\n\n\n\n\nViolin plot of the number of a product bought in a month\n\n\n\n\n\n\n\nViolin plot of price of a product\n\n\n\n\n\n\n\n\n\nViolin plot of the number of reviews\n\n\n\n\n\n\nFigure 3: Violin plots (with overlayed box plot and outliers) of 3 of the main numerical columns.\n\n\n\nThe amount of stars, vs the number of reviews for a product can be seen in Figure 4, colored with the Is Best Seller flag. It can be seen that the distribution of Is Best Seller items tends to follow the overall distribution; additionally, the number of reviews tends to increase as the number of stars increases, with the products with the outlier number of reviews (over 200k) all above 4 stars.\n\n\n\n\n\n\n\n\nFigure 4: Stars vs number of reviews recieved by an amazon product, colored by whether the product was a best-seller.\n\n\n\n\n\nIn order to visualize the content of the text factors, namely the Category and Name of a product, a wordcloud format was used; wordclouds visualize the words that tend to appear in the text with the more frequent words appearing in larger text. In Figure 5, it can be seen that in the categories, “Care”, “Products”, “Boy’s Clothing”, “Consoles”, “Accessories”, and “Games” appear most. In the names of procucts, the words “Stainles Steel”, “Compatible”, “Heavy Duty”, “Boy”, “Girl”, and “Birthday” appear the most.\n\n\n\n\n\n\n\n\nWordcloud for categories of products\n\n\n\n\n\n\n\nWordcloud for the names of products\n\n\n\n\n\n\nFigure 5: Wordclouds (where more frequent appearing words are bigger) of the categories of products and the names of products.\n\n\n\nThe number of stars vs price, with price being on a log scale (to hangle outliers) is shown in Figure 6, along with being colored by if it’s a best seller. As was seen in Figure 4, the distribution of Is Best Seller items tends to follow the overall distribution; additionally, it can be seen that there are products with a price up to $10k across all number of stars.\n\n\n\n\n\n\n\n\nFigure 6: The number of stars a product recieved, vs the price in log scale, colored by whether the product was a best seller.\n\n\n\n\n\n\nInvestigation of products in both sets of data\nThe following plots focused on products that had the same Asin (identifying ID), that were in both the data personally scraped in 2024 and the Kaggle data from 2023. Investigating this subset of data could reveal a lot about how product prices, reviews, and other factors changed over time, as the only way to get time difference data was to use products that actually had more than one timepoint.\nFigure 7 shows the price vs list price of items, colored by what date they were scraped on, with a trendline plotted. It can be seen that on the whole, list price and actual price match and that prices seem to have increased since 2023 to 2024; however, below $50 price (when zooming in) the trendlines cross, implying that lower-priced products actually decreased in price.\n\n\n\n\n                                                \n\n\nFigure 7: Price vs list price of items with the same ASIN across dates scraped, with a trendline.\n\n\n\n\nGiven that outliers can be seen in price affecting the plot of the graph, it was decided for analysis to only consider those prices most populous, aka prices less than $800. Given that, the prices of each Asin were plotted, along with being colored by if the price increased from 2023 to 2024, or decreased. It can be seen that in Figure 8 that indeed for lower priced products (less than $50) the prices mostly did not increase, and instead decreased or stayed the same. The histogram shows the frequency of higher-priced procucts generally exponentially decreasing, and the likelihood of the price increasing going up.\n\n\n\n\n                                                \n\n\nFigure 8: Histogram of prices, colored by whether the change in price increased or decreased over time, for those items that were in both sets of data.\n\n\n\n\nFigure 9 shows a more granular form of the previous plot by plotting price vs the actual price difference amount from 2023 to 2024, with the ability to investigate which products are having such a dramatic increase or decrease in price over time. It can be seen that tech and suitcases were the outliers in price increase, and tech also being the outliers in price decrease.\n\n\n\n\n                                                \n\n\nFigure 9: Price vs the difference in price, over the two sets of data, colored by whether the price diff increased or decreased.\n\n\n\n\nFinally, it was investigated as to which product categories exactly had a price increase or decrease, and the ratio of number of increases to decreases. Only categories with more than 20 products were included, to reduce noise and to focus on meaningful categories. In Figure 10, it can be seen that “Sports and Outdoors”, “Makeup”, “Electrical Equipment”, and “Building Toys” all had more price increases than decreases; every other product category had more price decreases than increases, with “Smart Homes” and “Gift Cards” having the most price decreases.\n\n\n\n\n                                                \n\n\nFigure 10: For the included categories, a bar chat showing the ratio of products that had their price increase over time vs decrease.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Home\n\n\nWelcome to the homepage for machine learning on Amazon retail product data, made by Bhavana Jonnalagadda! You can see all pages listed below, or use the navbar to navigate the pages.\n\n\n\n\n\n\n\n\n\n\n\n\nPCA and SVD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Prep / EDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSVM (Support Vector Machine)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Code",
      "Home"
    ]
  },
  {
    "objectID": "models/pca.html",
    "href": "models/pca.html",
    "title": "PCA and SVD",
    "section": "",
    "text": "Where Principal Component Analysis and Singular Value Decomposition are used, for dimensionality reduction and EDA.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#overview",
    "href": "models/pca.html#overview",
    "title": "PCA and SVD",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\nAn example 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components generated from it, respectively.\n\n\n\n\n\n\n\nScatterplot after PCA reduced from 3-dimensions to 2-dimensions.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and ML. At its core, PCA aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. This is achieved by identifying the directions of maximum variance in the original data, known as principal components. These principal components are the eigenvectors of the covariance matrix of the data, with their corresponding eigenvalues equaling the amount of variance explained by each principal component (PC). By retaining the PCs that capture the majority of the variance, PCA enables dimensionality reduction without significant loss of information.\nReducing the dimensionality of data is crucial for simplifying analysis and visualization, as it alleviates the curse of dimensionality and reduces computational complexity. High-dimensional data presents challenges such as increased computational cost, sparsity, and overfitting, which can get in the way of analysis and interpretation. Dimensionality reduction techniques like PCA help address these challenges by capturing the essential structure of the data in a lower-dimensional space, making it easier to visualize, interpret, and analyze complex datasets. Also, reducing dimensionality can improve the performance of machine learning algorithms by reducing noise, mitigating overfitting, enhancing model generalization, and potentially increasing the speed of training/evaluation of the models with the reduced amount of data.\nIn this specific project, PCA is used to reduce the amount of numerical/boolean factors to just 2 factors, which then enables the examination of those 2 components visually. The relationship between the reduced-dimension factors and the original factors in the dataset is explored, showing the effect and importance of the various factors. And especially since there are more than 1 million rows in the dataset, every amount of data reduction is helpful for subsequent machine learning model training.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#data-preperation",
    "href": "models/pca.html#data-preperation",
    "title": "PCA and SVD",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nData Prep: All models and methods require specific data formats. Explain what kind of data is needed for PCA and show an image of the sample of data you plan to use. LINK to the sample of data as well.\nCode: Use R or Python to code PCA. LINK to the code.\nResults: Discuss, illustrate, describe, and visualize the results. Understand and explain how to visualize the relationship between variables and principal components. Include at least 2 visualizations for your data.\nConclusions: What did you learn that pertains to your topic?\n\nTable 1\n\n\n\nTable 1: A random sample of the data used with PCA.\n\n\n\n\n\n\n\n\n\n(a) Unscaled data\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n4.5\n618\n26\n26\n600\n\n\n0\n0\n0\n8.89\n8.89\n0\n\n\n0\n2.9\n2\n7.99\n7.99\n100\n\n\n0\n4.8\n0\n100\n100\n0\n\n\n0\n4.6\n408\n259\n370\n200\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaled (by min/max) data\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n1\n0\n0.0588\n0.0544\n0\n\n\n0\n0.82\n0\n0.497\n0.542\n0.778\n\n\n0\n0.88\n0.419\n0.117\n0.109\n0.333\n\n\n0\n0.76\n0\n0.0764\n0.0708\n0\n\n\n0\n0.94\n0\n0.0617\n0.0686\n0",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#code",
    "href": "models/pca.html#code",
    "title": "PCA and SVD",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the PCA can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#results",
    "href": "models/pca.html#results",
    "title": "PCA and SVD",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.\n\n\nFigure 1\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) Unscaled data\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) Scaled (and outliers removed) data\n\n\n\n\n\n\n\nFigure 1: A bar chart of the explained variance ratios of each principal component, and the scree plot of cummulative percentage of the ratios, for both the normal and scaled data.\n\n\n\nFigure 2\n\n\n\nFigure 2: The found principal components, colored by a feature in the dataset where the feature has visible stratification in the PCs.\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\nRelationship of Variables and PCs\n\n\n\n\n\nAn example of the projection of the original data points onto the reduced-dimension line of an eigenvector. The angle/direction of the line is computationally chosen to maximize the sum of all the variance, aka the dotted lines.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#conclusions",
    "href": "models/pca.html#conclusions",
    "title": "PCA and SVD",
    "section": "Conclusions",
    "text": "Conclusions\n\n\n\nTODO\nTODO\nTODO\nTODO\nTODO\nTODO\nTODO",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/nn.html",
    "href": "models/nn.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural Networks\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/bayes.html",
    "href": "models/bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "An example of navigating Amazon\n\n\n\n\nIn today’s digital landscape dominated by e-commerce giants such as Amazon, the analysis of retail product data assumes paramount importance as a means to unravel the intricacies of consumer behavior, and unveil the complex relationships that exist between various products. The topic of this project poses a fundamental assertion: that mining the vast repository of Amazon’s retail product data holds the key to unlocking invaluable insights that can revolutionize marketing strategies, inform product development, and optimize overall business operations. This assertion underscores the critical role of data analytics in shaping the success trajectory of businesses operating in the contemporary marketplace. By delving into the granular details of consumer interactions with products on Amazon’s platform, stakeholders gain not only a deeper understanding of consumer preferences but also a nuanced perspective on the dynamics of the digital marketplace itself. The implications of this analysis extend far beyond mere academic curiosity; they are deeply intertwined with the strategic imperatives of businesses seeking to thrive amidst fierce competition and rapid technological advancements. The significance of analyzing Amazon retail product data transcends individual businesses, permeating into the realms of consumer behavior, marketing efficacy, policymaking, and academic research.\nAt its core, this endeavor resonates with a diverse array of stakeholders, each with a vested interest in deciphering the underlying patterns shaping modern consumption patterns and economic dynamics. Consumers, for instance, stand to benefit from the insights gleaned through data analysis, which can translate into more personalized shopping experiences, enhanced product recommendations, and greater transparency within the marketplace. Much work has been done, for example, to improve search engine results and product classification for recommendation purposes (Medini et al. 2019; Smith and Linden 2017). Marketers, on the other hand, find in this data a treasure trove of actionable intelligence, enabling them to fine-tune advertising strategies, optimize pricing models, and anticipate shifts in consumer preferences with a heightened degree of accuracy (Chen, Mislove, and Wilson 2016; Chevalier and Goolsbee 2003). Furthermore, researchers across various disciplines are drawn to the rich dataset offered by Amazon, utilizing sophisticated analytical techniques ranging from machine learning algorithms to traditional statistical methods to unravel the complexities of consumer behavior and product relationships (Shaikh, Rathi, and Janrao 2017; Bell and Bala 2015). Despite the considerable strides made in this field, there remains an abundance of untapped potential, beckoning further exploration and innovation to fully harness the transformative power of Amazon’s retail product data.\n\n\n\n\n\nML in retail\n\n\n\n\nIn this project, machine learning methods play a pivotal role in extracting meaningful insights from the vast sea of Amazon’s retail product data. Techniques such as Principal Component Analysis (PCA) enable dimensionality reduction, allowing for the identification of key features influencing consumer behavior. Clustering algorithms group similar products or consumers together, facilitating targeted marketing strategies and personalized recommendations. Neural networks, with their ability to learn intricate patterns, offer valuable predictive capabilities, aiding in demand forecasting and inventory management. Regression analysis uncovers relationships between variables, potentially helping businesses understand the impact of pricing, promotions, and other factors on sales. Support Vector Machines (SVM), Naive Bayes, and decision trees provide additional tools for classification and prediction tasks, enriching the analytical toolkit available to researchers and practitioners alike. Through the application of these advanced methodologies, one can navigate the complexities of the digital marketplace, leveraging data-driven insights to drive innovation and strategic decision-making.\n\n\n\nIs there noticable trends in prices over time, over all products?\nAre there price trends over time within a specific product category?\nIs there any meaningful similarities in the images between products with similar descriptions?\nAre there price/rating similarities between the products with similar images?\nDo ratings have any correlation with the price of products?\nDo ratings have any correlation with the product category?\nCan the price of a product be predicted based on all other factors?\nCan the rating of a product be predicted based on all other factors?\nCan the category of a product be predicted based on the description?\nIs the description of a product similar to the generated description based on the product image (or the generated bag of words, tags, etc)?",
    "crumbs": [
      "Code",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#questions-to-answer",
    "href": "intro.html#questions-to-answer",
    "title": "Introduction",
    "section": "",
    "text": "Is there noticable trends in prices over time, over all products?\nAre there price trends over time within a specific product category?\nIs there any meaningful similarities in the images between products with similar descriptions?\nAre there price/rating similarities between the products with similar images?\nDo ratings have any correlation with the price of products?\nDo ratings have any correlation with the product category?\nCan the price of a product be predicted based on all other factors?\nCan the rating of a product be predicted based on all other factors?\nCan the category of a product be predicted based on the description?\nIs the description of a product similar to the generated description based on the product image (or the generated bag of words, tags, etc)?",
    "crumbs": [
      "Code",
      "Introduction"
    ]
  },
  {
    "objectID": "models/clustering.html",
    "href": "models/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in order to discover patterns in the data.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#overview",
    "href": "models/clustering.html#overview",
    "title": "Clustering",
    "section": "Overview",
    "text": "Overview\nHere, describe clustering, including partitional vs, hierarchical. Note the distance metrics used. Discuss how you plan to use clustering to engage in discovery. Have two images. More is fine.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#data-preperation",
    "href": "models/clustering.html#data-preperation",
    "title": "Clustering",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nClustering Methods\nAll models and methods require specific data formats. Clustering requires ONLY unlabeled numeric data. Explain this and show an image of the sample of data you plan to use. LINK to the sample of data as well.\n\n\n\n\nTable 1: The data used for clustering.\n\n\n\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n0.94\n0\n0.0426\n0.0412\n0\n\n\n0\n0\n0\n0.00825\n0.00799\n0\n\n\n0\n0\n0\n0.0903\n0.0875\n0\n\n\n0\n0.8\n0\n0.018\n0.0225\n0.333\n\n\n0\n1\n0\n0.0205\n0.0199\n0",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#code",
    "href": "models/clustering.html#code",
    "title": "Clustering",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the PCA can be found here, or click on the link card to the right.\n\n\n\n\n\n\nUse R or Python to code k-means clustering and to code hierarchical clustering. Use Cosine Similarity as the distance measure for the hclust. LINK to the code.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#results",
    "href": "models/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) The elbow plot of WCSS vs rank.\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) The plot of silhouette scores vs rank.\n\n\n\n\n\n\n\nFigure 1: The plots showcasing the effectiveness of different ranks with Kmeans clustering.\n\n\n\n\n\n\n\n\n\nFigure 2: A dengrogram of the hierarchical clustering performed.\n\n\n\n\n\n\n\n                                                \n\n\nFigure 3: A comparison of the clustering models ran, at rank 2, using the adjusted Rand Score Index.\n\n\n\n\n\n\n\n\n                                                \n\n\nFigure 4: A splom of selected features (and the subset of rows used in the clustering methods), colored by the cluster assigned to the data point.\n\n\n\n\nDiscuss, illustrate, describe, and visualize the results. Have at least one dendrogram and at least one clustering image (that is not a dendrogram). What values of k did you use for k - means (you should use at least 3 different k values to compare). What did the Silhouette method tell you about the “best k”. Include a vis of the silhouette results. Using the hierarchical clustering, how did this compare and coincide with the k-means? What value of k does hclust suggest?",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#conclusions",
    "href": "models/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nWhat did you learn that pertains to your topic?",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/svm.html",
    "href": "models/svm.html",
    "title": "SVM (Support Vector Machine)",
    "section": "",
    "text": "SVM (Support Vector Machine)\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/regression.html",
    "href": "models/regression.html",
    "title": "Regression",
    "section": "",
    "text": "Regression\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Regression"
    ]
  },
  {
    "objectID": "models/dtrees.html",
    "href": "models/dtrees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision Trees\nPlaceholder",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Conclusions\nPlaceholder\n\n\nReferences\n\n\nBell, Sean, and Kavita Bala. 2015. “Learning Visual Similarity for Product Design with Convolutional Neural Networks.” ACM Trans. Graph. 34 (4). https://doi.org/10.1145/2766959.\n\n\nChen, Le, Alan Mislove, and Christo Wilson. 2016. “An Empirical Analysis of Algorithmic Pricing on Amazon Marketplace.” In Proceedings of the 25th International Conference on World Wide Web, 1339–49. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2883089.\n\n\nChevalier, Judith, and Austan Goolsbee. 2003. “Measuring Prices and Price Competition Online: Amazon.com and BarnesandNoble.com.” Quantitative Marketing and Economics 1 (2): 203–22. https://doi.org/10.1023/A:1024634613982.\n\n\nMedini, Tharun, Qixuan Huang, Yiqiu Wang, Vijai Mohan, and Anshumali Shrivastava. 2019. “Extreme Classification in Log Memory Using Count-Min Sketch: A Case Study of Amazon Search with 50M Products.” CoRR abs/1910.13830. http://arxiv.org/abs/1910.13830.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nShaikh, Shakila, Sheetal Rathi, and Prachi Janrao. 2017. “Recommendation System in e-Commerce Websites: A Graph Based Approached.” In 2017 IEEE 7th International Advance Computing Conference (IACC), 931–34. https://doi.org/10.1109/IACC.2017.0189.\n\n\nSmith, Brent, and Greg Linden. 2017. “Two Decades of Recommender Systems at Amazon.com.” IEEE Internet Computing 21 (3): 12–18. https://doi.org/10.1109/MIC.2017.72.\n\n\nVos, Nelis J. de. 2015--2024. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.",
    "crumbs": [
      "Code",
      "Conclusions"
    ]
  }
]