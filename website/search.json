[
  {
    "objectID": "models/svm.html",
    "href": "models/svm.html",
    "title": "SVM (Support Vector Machine)",
    "section": "",
    "text": "Example SVM\n\n\n\n\nSupport Vector Machines (SVMs) are a type of supervised learning model used for classification and regression tasks. They work by finding the hyperplane that best separates the data points into different classes.\nSVMs are considered linear separators because they aim to find the best hyperplane that separates data points of different classes with the maximum margin. This hyperplane is defined by a set of weights and biases that are optimized during the training process to minimize classification errors. The kernel function in SVMs allows them to handle nonlinear data by implicitly mapping the input data into a higher-dimensional space where a linear separation is possible. The dot product plays a critical role in the kernel function because it measures the similarity between two data points in this higher-dimensional space, from which the similarity is used to compute the support vectors and margins.\n\n\n\n\n\nExample SVM\n\n\n\n\n\n\n\nPolynomial Kernel: \\(K(x, y) = (x \\cdot y + r)^d\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(r\\) is a constant term.\n\\(d\\) is the degree of the polynomial.\n\nRBF (Radial Basis Function) Kernel: \\(K(x, y) = \\exp(-\\gamma \\| x - y \\|^2)\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(\\gamma\\) is a hyperparameter that determines the “spread” of the kernel.\nnonlinearly maps samples into a higher dimensional space (Hsu et al. 2003)\n\n\n\n\n\nLet’s say we have a 2D point \\((x, y) = (3, 4)\\) and we want to use a polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to “cast” this point into the proper number of dimensions.\n\nOriginal Point: \\((x, y) = (3, 4)\\)\nPolynomial Kernel Transformation: \\[\n\\begin{align*}\nK((3, 4), (x, y)) & = ((3 \\cdot x + 4 \\cdot y) + 1)^2 \\\\\n& = ((3x + 4y) + 1)^2 \\\\\n& = (3x + 4y + 1)^2\n\\end{align*}\n\\]\n\nSo, applying the polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to the point \\((3, 4)\\) transforms it into the new representation \\((3x + 4y + 1)^2\\) in the higher-dimensional space. This transformed representation enables SVMs to find a nonlinear decision boundary.\nIn summary, SVMs are powerful classifiers that use linear separators in higher-dimensional spaces facilitated by kernel functions like the polynomial and RBF kernels, which allow them to handle nonlinear data effectively.\nIn this project, SVMs will be used specifically to categorize and predict whether a product, given that we have data from 2 time points of its price, will have its price increase over time.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#overview",
    "href": "models/svm.html#overview",
    "title": "SVM (Support Vector Machine)",
    "section": "",
    "text": "Example SVM\n\n\n\n\nSupport Vector Machines (SVMs) are a type of supervised learning model used for classification and regression tasks. They work by finding the hyperplane that best separates the data points into different classes.\nSVMs are considered linear separators because they aim to find the best hyperplane that separates data points of different classes with the maximum margin. This hyperplane is defined by a set of weights and biases that are optimized during the training process to minimize classification errors. The kernel function in SVMs allows them to handle nonlinear data by implicitly mapping the input data into a higher-dimensional space where a linear separation is possible. The dot product plays a critical role in the kernel function because it measures the similarity between two data points in this higher-dimensional space, from which the similarity is used to compute the support vectors and margins.\n\n\n\n\n\nExample SVM\n\n\n\n\n\n\n\nPolynomial Kernel: \\(K(x, y) = (x \\cdot y + r)^d\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(r\\) is a constant term.\n\\(d\\) is the degree of the polynomial.\n\nRBF (Radial Basis Function) Kernel: \\(K(x, y) = \\exp(-\\gamma \\| x - y \\|^2)\\)\n\n\\(x\\) and \\(y\\) are input data points.\n\\(\\gamma\\) is a hyperparameter that determines the “spread” of the kernel.\nnonlinearly maps samples into a higher dimensional space (Hsu et al. 2003)\n\n\n\n\n\nLet’s say we have a 2D point \\((x, y) = (3, 4)\\) and we want to use a polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to “cast” this point into the proper number of dimensions.\n\nOriginal Point: \\((x, y) = (3, 4)\\)\nPolynomial Kernel Transformation: \\[\n\\begin{align*}\nK((3, 4), (x, y)) & = ((3 \\cdot x + 4 \\cdot y) + 1)^2 \\\\\n& = ((3x + 4y) + 1)^2 \\\\\n& = (3x + 4y + 1)^2\n\\end{align*}\n\\]\n\nSo, applying the polynomial kernel with \\(r = 1\\) and \\(d = 2\\) to the point \\((3, 4)\\) transforms it into the new representation \\((3x + 4y + 1)^2\\) in the higher-dimensional space. This transformed representation enables SVMs to find a nonlinear decision boundary.\nIn summary, SVMs are powerful classifiers that use linear separators in higher-dimensional spaces facilitated by kernel functions like the polynomial and RBF kernels, which allow them to handle nonlinear data effectively.\nIn this project, SVMs will be used specifically to categorize and predict whether a product, given that we have data from 2 time points of its price, will have its price increase over time.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#model-selection",
    "href": "models/svm.html#model-selection",
    "title": "SVM (Support Vector Machine)",
    "section": "Model Selection",
    "text": "Model Selection\nFor this project, we will use three different types of common kernels with the SVMs: the RBF kernel, which is commonly used; the linear kernel, which is the base type; and the polynomial kernel, which enables nonlinear tranforms. The linear kernel is the most basic form, but could still have efficacy, as “there are some situations where the RBF kernel is not suitable. In particular, when the number of features is very large, one may just use the linear kernel” (Hsu et al. 2003).\n\nHyperparameter Selection using Cross-Validation (CV)\nIn addition to the kernel selection, there are other hyperparameters (options of a model that must be set before training/fitting it) that need to be selected. For all kernels, the cost, or penalty parameter of the error term, C must be selected. For the linear kernel, no other parameter needs to be selected. For the RBF and polynomial kernels, the gamma \\(\\gamma\\) parameter needs to be set; and for the polynomial kernel, the degree d and coefficient r as described above also need to be picked. To do this selection of hyperparameters, a grid search across all possible combinations of parameters will be used, along with cross-validation with that grid search used as picking which values to search.\nCross-validation is a technique used in machine learning model selection to evaluate the performance and generalization ability of a model. It involves partitioning the available data into subsets: one subset is used for training the model, and the remaining subsets are used for validation. This process is repeated multiple times, each time with a different partitioning scheme. The K-fold cross-validation specifically divides the data into K equally sized folds, using K-1 folds for training and the remaining fold for validation, iterating through all combinations to ensure robust evaluation and minimize overfitting. The average performance across all iterations provides a reliable estimate of the model’s performance on unseen data, helping in selecting the best model for deployment.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#data-prep",
    "href": "models/svm.html#data-prep",
    "title": "SVM (Support Vector Machine)",
    "section": "Data Prep",
    "text": "Data Prep\nSVMs can only work on labeled numeric data; this is because they are generating lines to seperate points in various dimensions, which thus must be in a continuous ordinal space (aka numeric); thus, any categorical variables used must be converted to integers (if ordinal) or one-hot encoded (if not). In addition, SVMs need the numeric data to be scaled (Hsu et al. 2003) and there to be a label that is being predicted.\nVarious transformations were done to the initial Amazon product data in order to get the data to fit these conditions, as well as to create the label being used:\n\nMake DF of products that have a calculable price difference over time\nUse initial price and data, in order to predict later price change\nOnly use data with Category categories that have more than 5 products occuring, to improve prediction\nScale numerical data to [0, 1], make bool Is Best Seller an int\nOne-hot encode categorical var Category\nSplit into train/test, stratifying on the prediction label\n\nA sample of the final form of the data used can be seen in Table 1. A link to the data used can be found here.\n\n\n\nTable 1: Sample of dataset used after transformation.\n\n\n\n\n\nA sample of the label/classification/predictive feature used in the training/test datasets.\n\n\nPrice Increased\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\nThe features used in both the training and test datasets.\n\n\n\n\n\n\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\nCategory\n\n\n\n\n0\n0.826087\n0.168665\n0.0486961\n0.0453502\n0\nIndustrial & Scientific\n\n\n0\n0.478261\n0.306469\n0.0437793\n0.0893706\n0\nHeating, Cooling & Air Quality\n\n\n0\n0.73913\n0.0756083\n0.0919231\n0.110697\n0\nVacuum Cleaners & Floor Care\n\n\n0\n0.782609\n0.703739\n0.0164206\n0.0152924\n0\nComputers\n\n\n0\n0.956522\n0.0452226\n0.105394\n0.0981521\n0\nWomen’s Handbags\n\n\n0\n0.173913\n0.0608902\n0.00975268\n0.00908258\n0\nOral Care Products\n\n\n\n\n\n\n\n\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage. We also stratify the splitting of data on the label categories, for fairness in evaluation. Both sets of data look exactly like the included table visualization, except have disjoint values and different lengths of observations/samples.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#code",
    "href": "models/svm.html#code",
    "title": "SVM (Support Vector Machine)",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the SVMs, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#results",
    "href": "models/svm.html#results",
    "title": "SVM (Support Vector Machine)",
    "section": "Results",
    "text": "Results\n\n\nThe grid-search cross-validation was run on set of hyperparameters, and the top results for each kernel and for each cost searched are shown in Table 2. All the hyperparameters run/tested against are given below:\n\nrbf_params = {\"kernel\": [\"rbf\"], \"C\": [0.5, 1, 10, 100], \"gamma\": [\"auto\", \"scale\", 0.01, 0.001, 0.0001]}\npoly_params = {\"kernel\": [\"poly\"], \"C\": [0.5, 1, 10, 100], \"degree\": [2, 3, 4], \"coef0\": [0.5, 1, 5]}\nlin_params = {\"kernel\": [\"linear\"], \"C\": [0.5, 1, 10, 100]}\n\nAs one can seee, the best kernel tested was the RBF kernel, and had the same accuracy across all costs tested with gamma equaling about 0.01/0.001.\n\n\n\n\nTable 2: The top accuracy results for each kernel, for each cost C, with the top hyperparameters listed. A gamma \\(\\gamma\\) = auto means a value of about 0.01.\n\n\n\n\n\nKernel\nC\nGamma\nDegree\nR\nAccuracy Score\n\n\n\n\nrbf\n100\n0.001\n\n\n0.74661\n\n\nrbf\n10\nauto\n\n\n0.74661\n\n\nrbf\n1\nauto\n\n\n0.74661\n\n\nrbf\n0.5\nauto\n\n\n0.74661\n\n\npoly\n100\n\n2\n1\n0.710169\n\n\npoly\n10\n\n2\n0.5\n0.723729\n\n\npoly\n1\n\n2\n0.5\n0.728814\n\n\npoly\n0.5\n\n2\n0.5\n0.74322\n\n\nlinear\n100\n\n\n\n0.729661\n\n\nlinear\n10\n\n\n\n0.729661\n\n\nlinear\n1\n\n\n\n0.729661\n\n\nlinear\n0.5\n\n\n\n0.744068\n\n\n\n\n\n\n\n\nThe confusion matrix for the best model tested for each kernel is shown below in Figure 1. Unfortunately, it appears that the top 74.66% accuracy came from the model classifying all input as False, even though about 1/4 or 25% were True. This imbalance held even when the performance metric that was being optimized against, in the cross-validation grid search, was balanced accuracy (where an imlabance in output label classes is taken into account).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Confusion matrices of the classification output (on the test dataset) from the best performing model, for each kernel tested.\n\n\n\n\n\n\n\n\n\n\n\n\nDecision boundaries by the best performing overall model (RBF kernel)\n\n\n\n\n\n\n\nDecision boundaries by the best performing Polynomial kernel model (balanced accuracy score, achieved 53%)\n\n\n\n\n\nFigure 2: The decision boundaries for 2 models, visualized across all combination of non-category features.\n\n\n\n\n\n\nThe decision boundaries by the best performing model, visualized across all combination of features (except the one-hot encoded category) is shown in Figure 2. The areas that are filled white are because the model was not evaluated against those values for technical python reasons. As one can see, the decision boundaries agree with the output of the confusion matrix, namely that the best model simply classifies everything as the dominant label in order to achieve the best performance.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/svm.html#conclusions",
    "href": "models/svm.html#conclusions",
    "title": "SVM (Support Vector Machine)",
    "section": "Conclusions",
    "text": "Conclusions\nFrom the above results, several interesting conclusions can be drawn:\n\nThe SVMs struggled to classify the data, and achieved near 75% accuracy simply by classifying everything as the dominant label (which was present in about 75% of the test and train data).\nThe amount of training data to allow the model to separate the classes was likely not enough; an alternative is that the features used are strictly not seperable by class, leading to the model being unable to do so and therefore classifying everything as the dominant label.\nChanges in order of magnitude of the cost of kernels had barely any impact.\n\nOverall, the label Price Increased seems to be a difficult one to predict and the SVMs struggled to classify as such.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "SVM (Support Vector Machine)"
    ]
  },
  {
    "objectID": "models/pca.html",
    "href": "models/pca.html",
    "title": "PCA and SVD",
    "section": "",
    "text": "An example 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components generated from it, respectively.\n\n\n\n\n\nScatterplot after PCA reduced from 3-dimensions to 2-dimensions.\n\n\n\n\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and ML. At its core, PCA aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. This is achieved by identifying the directions of maximum variance in the original data, known as principal components. These principal components are the eigenvectors of the covariance matrix of the data, with their corresponding eigenvalues equaling the amount of variance explained by each principal component (PC). By retaining the PCs that capture the majority of the variance, PCA enables dimensionality reduction without significant loss of information.\nReducing the dimensionality of data is crucial for simplifying analysis and visualization, as it alleviates the curse of dimensionality and reduces computational complexity. High-dimensional data presents challenges such as increased computational cost, sparsity, and overfitting, which can get in the way of analysis and interpretation. Dimensionality reduction techniques like PCA help address these challenges by capturing the essential structure of the data in a lower-dimensional space, making it easier to visualize, interpret, and analyze complex datasets. Also, reducing dimensionality can improve the performance of machine learning algorithms by reducing noise, mitigating overfitting, enhancing model generalization, and potentially increasing the speed of training/evaluation of the models with the reduced amount of data.\nIn this specific project, PCA is used to reduce the amount of numerical/boolean factors to just 2 factors, which then enables the examination of those 2 components visually. The relationship between the reduced-dimension factors and the original factors in the dataset is explored, showing the effect and importance of the various factors. And especially since there are more than 1 million rows in the dataset, every amount of data reduction is helpful for subsequent machine learning model training.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#overview",
    "href": "models/pca.html#overview",
    "title": "PCA and SVD",
    "section": "",
    "text": "An example 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components generated from it, respectively.\n\n\n\n\n\nScatterplot after PCA reduced from 3-dimensions to 2-dimensions.\n\n\n\n\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and ML. At its core, PCA aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. This is achieved by identifying the directions of maximum variance in the original data, known as principal components. These principal components are the eigenvectors of the covariance matrix of the data, with their corresponding eigenvalues equaling the amount of variance explained by each principal component (PC). By retaining the PCs that capture the majority of the variance, PCA enables dimensionality reduction without significant loss of information.\nReducing the dimensionality of data is crucial for simplifying analysis and visualization, as it alleviates the curse of dimensionality and reduces computational complexity. High-dimensional data presents challenges such as increased computational cost, sparsity, and overfitting, which can get in the way of analysis and interpretation. Dimensionality reduction techniques like PCA help address these challenges by capturing the essential structure of the data in a lower-dimensional space, making it easier to visualize, interpret, and analyze complex datasets. Also, reducing dimensionality can improve the performance of machine learning algorithms by reducing noise, mitigating overfitting, enhancing model generalization, and potentially increasing the speed of training/evaluation of the models with the reduced amount of data.\nIn this specific project, PCA is used to reduce the amount of numerical/boolean factors to just 2 factors, which then enables the examination of those 2 components visually. The relationship between the reduced-dimension factors and the original factors in the dataset is explored, showing the effect and importance of the various factors. And especially since there are more than 1 million rows in the dataset, every amount of data reduction is helpful for subsequent machine learning model training.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#data-preperation",
    "href": "models/pca.html#data-preperation",
    "title": "PCA and SVD",
    "section": "Data Preperation",
    "text": "Data Preperation\nThe data, since already cleaned, did not need much transformation to work with PCA. The following steps were taken to prepare the data:\n\nSelected only numerical and boolean columns\nOutliers (outside the 99% quantile) were removed\nThe data was scaled to min/max of 0, 1\n\nA sample of the final form of the data used can be seen in Table 1. A link to the data used can be found here.\nTable 1\n\n\n\nTable 1: A random sample of the data used with PCA.\n\n\n\n\n\n\n\n\n\n(a) Unscaled data\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n4.6\n0\n13\n13\n600\n\n\n0\n4.5\n0\n13\n13\n50\n\n\n0\n4.5\n0\n27\n27\n600\n\n\n0\n3.9\n0\n396\n396\n0\n\n\n0\n4.5\n66\n46\n46\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaled (by min/max) data\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n0.86\n0\n0.129\n0.12\n1\n\n\n0\n0.82\n0.0235\n0.0558\n0.0517\n0\n\n\n0\n0.9\n0.0388\n0.134\n0.124\n0\n\n\n0\n0.9\n0\n0.0294\n0.0272\n0.0556\n\n\n0\n0.9\n0\n0.147\n0.136\n0",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#code",
    "href": "models/pca.html#code",
    "title": "PCA and SVD",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the PCA can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#results",
    "href": "models/pca.html#results",
    "title": "PCA and SVD",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.\n\n\nIn Figure 1, we show the results of running PCA on both the unscaled data, and the data that was scaled (and outliers removed) as a scree plot of the explained variance for each PC found. As one can see, The unscaled data found that 2 PCs were enough, while the scaled data benefits from keeping 3/4 of the PCs. Based on teh number of input columns (6), 3 was chosen.\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) Unscaled data\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) Scaled (and outliers removed) data\n\n\n\n\n\n\n\nFigure 1: A bar chart of the explained variance ratios of each principal component, and the scree plot of cummulative percentage of the ratios, for both the normal and scaled data.\n\n\n\nIn Figure 2, one can see the visualization of the PCs, colored by specific features chosen that show clear stratification in the PC points. The 3-PC data (from using scaled data) is shown on the left, and the 2-PC data (unscaled data) on the right.\n\n\n\nFigure 2: The found principal components, colored by a feature in the dataset where the feature has visible stratification in the PCs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship of Variables and PCs\n\n\n\n\n\nAn example of the projection of the original data points onto the reduced-dimension line of an eigenvector. The angle/direction of the line is computationally chosen to maximize the sum of all the variance, aka the dotted lines.\n\n\n\n\n\nPrincipal Component Analysis (PCA) seeks to capture the maximum variance in a dataset by transforming the original variables into a new set of uncorrelated variables called principal components (PCs). The relationship between the variables and the principal components lies in the weights or loadings assigned to each variable in the construction of these components. These loadings indicate the contribution of each variable to each principal component. Variables with higher loadings on a particular principal component are more strongly correlated with that component and thus play a more significant role in defining its characteristics. Conversely, variables with low or near-zero loadings contribute less to that component’s representation. Therefore, PCA helps to uncover underlying patterns in data by highlighting the relationships and importance of variables in creating the principal components that capture the data’s variance efficiently.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/pca.html#conclusions",
    "href": "models/pca.html#conclusions",
    "title": "PCA and SVD",
    "section": "Conclusions",
    "text": "Conclusions\nFrom the above results, several interesting conclusions can be drawn:\n\nBought in month was an “important” enough feature that PCA stratified on it in both the 3 PC and 2 PC cases.\nNo other feature was as cleanly stratified-on in both cases.\nThe features that were stratified on in teh 2 PC case (where data was not scaled) were the features that didn’t need scaling anyways (Reviews, Date Scraped); this implies the scaling made the features more able to be seperable on variance, which aligns to the reasoning for scaling (and dropping outliers) int the first place.\nThe high explained variance indicates a good ability to capture the variance in the data.\n\nOverall, PCA did a reasonable job with dimensionality reduction, though futher work is needed as only 6 features were used in the first place.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "PCA and SVD"
    ]
  },
  {
    "objectID": "models/dtrees.html",
    "href": "models/dtrees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision tree structure\n\n\n\n\nDecision trees are a popular supervised learning algorithm used for both classification and regression tasks in machine learning. They are tree-like structures where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (for classification) or a numerical value (for regression). In classification, they can classify instances by learning decision rules inferred from the features. With regression, they can predict continuous values by averaging the target values of instances falling into the same leaf. When it comes to determining the “goodness” of a split in the decision tree and updating it, several methods are employed. Gini Impurity measures the impurity of a node, where a node is pure (GINI = 0) if all instances below a node belong to the same class. Entropy measures the randomness or uncertainty of a node; it is maximum (1) when all classes are equally distributed and decreases as the node becomes more pure. Information Gain is used to determine the best split in a decision tree. It calculates the difference in entropy (or GINI impurity) before and after the split. The split with the highest information gain is chosen.\n\n\n\n\n\nThe decision tree structure for the given example\n\n\n\n\nFor example, consider a dataset with weather attributes (including the feature “Outlook”) and a target class ‘Play’ (Yes/No). Suppose we want to split based on the ‘Outlook’ attribute. We calculate the entropy for the current node (before split), then we calculate the entropy for each possible branch (‘Sunny’, ‘Overcast’, ‘Rainy’). We then use Information Gain to determine the best split.\nIt’s generally possible to create an infinite number of decision trees because:\n\nThere can be different orderings of features for splitting.\nFor continuous features, there can be infinitely many possible split points.\nTree depth and complexity can vary widely, leading to numerous possible structures.\n\nTo manage this complexity and avoid overfitting, techniques like pruning, limiting tree depth, and using ensemble methods (like Random Forests) are employed. These strategies help create more generalizable and robust decision trees.\nIn this particular project, classification decision trees will be used to predict whether a product is a best seller or not; regression decision trees will be used to predict the price of a product. All numerical, boolean, and categorical features of the dataset will be used in the prediction (but not the unique string features like “Name” and “ASIN”).\n\n\n\nThe data features used were:\n\n\"Stars\", \"Reviews\", \"Price\", \"Date Scraped\", \"Bought In Month\" for predicting \"Is Best Seller\"\n\"Stars\", \"Reviews\", \"Date Scraped\", \"Bought In Month\", \"Is Best Seller\" for predicting \"Price\".\n\nThe data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models:\n\nOnly select rows that contain a non-NaN value for Category and Bought in Month\nSelect rows such that 1% of them are Is Best Seller == True. This is because the dataset contains more than &gt;1M rows, with only 0.4% of them Is Best Seller == True; this makes the prediction less biased and invariant for the model.\nMake datetime binary, as there are only 2 values used for it anyways.\n\nAs DTs are invariant to the scale of the data, no normalization/min-max scaling was needed.\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nTable 1 shows the final input data used for all models used, and the data can be found at this link. The test data is exactly the same format as the pictured samples of training data, except with 75% less rows.\n\n\n\nTable 1: The input training and test data for the classifying and regression decision trees.\n\n\n\n\n\nThe training features for the classification DT\n\n\nStars\nReviews\nPrice\nDate Scraped\nBought In Month\n\n\n\n\n4.3\n0\n45.99\n0\n0\n\n\n4.2\n0\n22.99\n0\n100\n\n\n4.9\n0\n345\n0\n0\n\n\n5\n0\n28.49\n1\n0\n\n\n4.6\n146\n23.55\n1\n0\n\n\n4.7\n0\n71.13\n0\n0\n\n\n\n\n\n\nThe label for the classification DT\n\n\nIs Best Seller\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\nThe training features for the regression DT\n\n\n\n\n\n\n\n\n\nStars\nReviews\nDate Scraped\nBought In Month\nIs Best Seller\n\n\n\n\n3.5\n0\n0\n0\nFalse\n\n\n4.7\n3874\n0\n50\nFalse\n\n\n0\n0\n0\n50\nFalse\n\n\n4.6\n0\n0\n50\nFalse\n\n\n5\n0\n0\n700\nFalse\n\n\n4.4\n37\n0\n0\nFalse\n\n\n\n\n\n\nThe label for the regression DT\n\n\nPrice\n\n\n\n\n24.99\n\n\n16.99\n\n\n33.99\n\n\n31.99\n\n\n9.99\n\n\n20.38\n\n\n\n\n\n\n\n\n\n\n\nAs described above, a classification DT and a regression DT were run on their respective subsets of the dataset, using the standard defaults given by sklearn. In addition, a third DT was run: using random forests for classification (using the same dataset as for the standard classification DT). Random forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. Random decision forests correct for decision trees’ habit of overfitting to their training set.\n\n\n\n\n\nThe jupyter notebook code for running decision trees, data prep, and everything else on this page can be found here, or click on the link card to the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe models were run, and the standard metrics for evaluating classification (including accuracy) and regression (including R^2 score) are shown in Table 2. As we can see, the random forest classfier (described from here on as “best classifier DT”) was the better performing model slightly; we can also see that the regression DT did not perform well.\n\n\n\nTable 2: Standard metrics showing the performance of the 3 models.\n\n\n\n\n\nThe performance of the regression DT.\n\n\nMetric\nValue\n\n\n\n\nMean Squared Error (MSE)\n3996.33\n\n\nMean Absolute Error (MAE)\n32.5754\n\n\nR^2\n-0.030492\n\n\n\n\n\n\nThe performance of the 2 classification DT models.\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n\nStandard Classifier DT\nFalse\n0.990381\n0.995355\n0.992862\n166850\n\n\n\n\nTrue\n0.0673887\n0.033553\n0.0448\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.985829\n\n\n\nmacro avg\n0.528885\n0.514454\n0.518831\n168519\n\n\n\n\nweighted avg\n0.98124\n0.985829\n0.983472\n168519\n\n\n\nRandom Forest Classifier\nFalse\n0.990312\n0.997393\n0.99384\n166850\n\n\n\n\nTrue\n0.0861345\n0.0245656\n0.0382284\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.987758\n\n\n\nmacro avg\n0.538223\n0.510979\n0.516034\n168519\n\n\n\n\nweighted avg\n0.981357\n0.987758\n0.984376\n168519\n\n\n\n\n\n\n\n\n\nWe also show the structure of the decision trees at the first 3 levels (all trees had more than 50 final levels which was unable to be visualized). In Figure 1, we can see that Bought in Month feature was very important to all 3 trees; we can also see that Stars and Price has a very low Gini index for both classifier DTs with Stars also appearing early in the tree, indicating the features’ importances to the predictions.\n\n\n\n\n\n\n\nRandom Forest Classifier\n\n\n\n\n\nStandard Classifier\n\n\n\n\n\nRegression DT\n\n\n\n\n\nFigure 1: A visualization of the first 3 levels of the 3 decision tree models run.\n\n\n\n\n\nWe show the basic plot of true vs predicted label (aka Price) for the regression decision tree in Figure 2; we can see that unlike the straight line of scatter points we would see in a perfect classification, we have an approximate log-negative trend. The model tended to overprice items with actual price &lt; $100, and underprice the products with high prices.\nWe also show the predicted price vs the error residuals in Figure 3; this plot gives similar insight to the previous, where we can see highest error on low predicted prices and less, but still increasing, error on higher-predicted prices.\n\n\n\n\n\n\n\n\nFigure 2: True vs predicted label for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Predicted price vs the residuals for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFor the best performing classifier DT, we show the confusion matrix of the labels in Figure 4. As we can see, The model did great on correctly classifying not best-sellers; however, that was because there were very few true best-sellers (this agrees with the F1 scores per-category in the above table). The model struggled with correctly classifying true best sellers and had many more FP and FN than TP.\n\n\n\n\n\n\n\nFigure 4: The confusion matrix of true vs predicted labels of the random forest classifier.\n\n\n\n\n\n\n\n\n\nFor the classification task, some conclusions and observations can be drawn from the results:\n\nThe accuracy is deceptively high for both classifiers, as the second True label only was present in 1% of the data.\nThe classifiers were biased towards predicting that a product was NOT a best-seller, as that gave the highest accuracy ultimately.\nAlthough the model had more FP than TP, overall it had many more FN than FP, indicating once again that the model tended to classify products as not best-sellers.\n\nFor the regression task, similarly we can draw observations from the results:\n\nThough the MAE was low, the low R^2 score indicates that the model overall tended to not predict the price well.\nBased on the residuals plots, the model did best (most residuals near zero) when the predicted price was around $150.\nThe model tended to underprice and overprice when further from the $150 price value.\n\nOverall, it seemed the models struggled to perform well but did adequately, especially the regression model which was mildly suprising. It can be concluded that the best-seller status of a product, along with price, is somewhat hard to predict from just the numerical values present in the dataset.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#overview",
    "href": "models/dtrees.html#overview",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision tree structure\n\n\n\n\nDecision trees are a popular supervised learning algorithm used for both classification and regression tasks in machine learning. They are tree-like structures where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (for classification) or a numerical value (for regression). In classification, they can classify instances by learning decision rules inferred from the features. With regression, they can predict continuous values by averaging the target values of instances falling into the same leaf. When it comes to determining the “goodness” of a split in the decision tree and updating it, several methods are employed. Gini Impurity measures the impurity of a node, where a node is pure (GINI = 0) if all instances below a node belong to the same class. Entropy measures the randomness or uncertainty of a node; it is maximum (1) when all classes are equally distributed and decreases as the node becomes more pure. Information Gain is used to determine the best split in a decision tree. It calculates the difference in entropy (or GINI impurity) before and after the split. The split with the highest information gain is chosen.\n\n\n\n\n\nThe decision tree structure for the given example\n\n\n\n\nFor example, consider a dataset with weather attributes (including the feature “Outlook”) and a target class ‘Play’ (Yes/No). Suppose we want to split based on the ‘Outlook’ attribute. We calculate the entropy for the current node (before split), then we calculate the entropy for each possible branch (‘Sunny’, ‘Overcast’, ‘Rainy’). We then use Information Gain to determine the best split.\nIt’s generally possible to create an infinite number of decision trees because:\n\nThere can be different orderings of features for splitting.\nFor continuous features, there can be infinitely many possible split points.\nTree depth and complexity can vary widely, leading to numerous possible structures.\n\nTo manage this complexity and avoid overfitting, techniques like pruning, limiting tree depth, and using ensemble methods (like Random Forests) are employed. These strategies help create more generalizable and robust decision trees.\nIn this particular project, classification decision trees will be used to predict whether a product is a best seller or not; regression decision trees will be used to predict the price of a product. All numerical, boolean, and categorical features of the dataset will be used in the prediction (but not the unique string features like “Name” and “ASIN”).",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#data-prep",
    "href": "models/dtrees.html#data-prep",
    "title": "Decision Trees",
    "section": "",
    "text": "The data features used were:\n\n\"Stars\", \"Reviews\", \"Price\", \"Date Scraped\", \"Bought In Month\" for predicting \"Is Best Seller\"\n\"Stars\", \"Reviews\", \"Date Scraped\", \"Bought In Month\", \"Is Best Seller\" for predicting \"Price\".\n\nThe data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models:\n\nOnly select rows that contain a non-NaN value for Category and Bought in Month\nSelect rows such that 1% of them are Is Best Seller == True. This is because the dataset contains more than &gt;1M rows, with only 0.4% of them Is Best Seller == True; this makes the prediction less biased and invariant for the model.\nMake datetime binary, as there are only 2 values used for it anyways.\n\nAs DTs are invariant to the scale of the data, no normalization/min-max scaling was needed.\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nTable 1 shows the final input data used for all models used, and the data can be found at this link. The test data is exactly the same format as the pictured samples of training data, except with 75% less rows.\n\n\n\nTable 1: The input training and test data for the classifying and regression decision trees.\n\n\n\n\n\nThe training features for the classification DT\n\n\nStars\nReviews\nPrice\nDate Scraped\nBought In Month\n\n\n\n\n4.3\n0\n45.99\n0\n0\n\n\n4.2\n0\n22.99\n0\n100\n\n\n4.9\n0\n345\n0\n0\n\n\n5\n0\n28.49\n1\n0\n\n\n4.6\n146\n23.55\n1\n0\n\n\n4.7\n0\n71.13\n0\n0\n\n\n\n\n\n\nThe label for the classification DT\n\n\nIs Best Seller\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\nThe training features for the regression DT\n\n\n\n\n\n\n\n\n\nStars\nReviews\nDate Scraped\nBought In Month\nIs Best Seller\n\n\n\n\n3.5\n0\n0\n0\nFalse\n\n\n4.7\n3874\n0\n50\nFalse\n\n\n0\n0\n0\n50\nFalse\n\n\n4.6\n0\n0\n50\nFalse\n\n\n5\n0\n0\n700\nFalse\n\n\n4.4\n37\n0\n0\nFalse\n\n\n\n\n\n\nThe label for the regression DT\n\n\nPrice\n\n\n\n\n24.99\n\n\n16.99\n\n\n33.99\n\n\n31.99\n\n\n9.99\n\n\n20.38",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#models-used",
    "href": "models/dtrees.html#models-used",
    "title": "Decision Trees",
    "section": "",
    "text": "As described above, a classification DT and a regression DT were run on their respective subsets of the dataset, using the standard defaults given by sklearn. In addition, a third DT was run: using random forests for classification (using the same dataset as for the standard classification DT). Random forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. Random decision forests correct for decision trees’ habit of overfitting to their training set.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#code",
    "href": "models/dtrees.html#code",
    "title": "Decision Trees",
    "section": "",
    "text": "The jupyter notebook code for running decision trees, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#results",
    "href": "models/dtrees.html#results",
    "title": "Decision Trees",
    "section": "",
    "text": "The models were run, and the standard metrics for evaluating classification (including accuracy) and regression (including R^2 score) are shown in Table 2. As we can see, the random forest classfier (described from here on as “best classifier DT”) was the better performing model slightly; we can also see that the regression DT did not perform well.\n\n\n\nTable 2: Standard metrics showing the performance of the 3 models.\n\n\n\n\n\nThe performance of the regression DT.\n\n\nMetric\nValue\n\n\n\n\nMean Squared Error (MSE)\n3996.33\n\n\nMean Absolute Error (MAE)\n32.5754\n\n\nR^2\n-0.030492\n\n\n\n\n\n\nThe performance of the 2 classification DT models.\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n\nStandard Classifier DT\nFalse\n0.990381\n0.995355\n0.992862\n166850\n\n\n\n\nTrue\n0.0673887\n0.033553\n0.0448\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.985829\n\n\n\nmacro avg\n0.528885\n0.514454\n0.518831\n168519\n\n\n\n\nweighted avg\n0.98124\n0.985829\n0.983472\n168519\n\n\n\nRandom Forest Classifier\nFalse\n0.990312\n0.997393\n0.99384\n166850\n\n\n\n\nTrue\n0.0861345\n0.0245656\n0.0382284\n1669\n\n\n\n\naccuracy\n\n\n\n\n0.987758\n\n\n\nmacro avg\n0.538223\n0.510979\n0.516034\n168519\n\n\n\n\nweighted avg\n0.981357\n0.987758\n0.984376\n168519\n\n\n\n\n\n\n\n\n\nWe also show the structure of the decision trees at the first 3 levels (all trees had more than 50 final levels which was unable to be visualized). In Figure 1, we can see that Bought in Month feature was very important to all 3 trees; we can also see that Stars and Price has a very low Gini index for both classifier DTs with Stars also appearing early in the tree, indicating the features’ importances to the predictions.\n\n\n\n\n\n\n\nRandom Forest Classifier\n\n\n\n\n\nStandard Classifier\n\n\n\n\n\nRegression DT\n\n\n\n\n\nFigure 1: A visualization of the first 3 levels of the 3 decision tree models run.\n\n\n\n\n\nWe show the basic plot of true vs predicted label (aka Price) for the regression decision tree in Figure 2; we can see that unlike the straight line of scatter points we would see in a perfect classification, we have an approximate log-negative trend. The model tended to overprice items with actual price &lt; $100, and underprice the products with high prices.\nWe also show the predicted price vs the error residuals in Figure 3; this plot gives similar insight to the previous, where we can see highest error on low predicted prices and less, but still increasing, error on higher-predicted prices.\n\n\n\n\n\n\n\n\nFigure 2: True vs predicted label for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Predicted price vs the residuals for the regression DT.\n\n\n\n\n\n\n\n\n\n\nFor the best performing classifier DT, we show the confusion matrix of the labels in Figure 4. As we can see, The model did great on correctly classifying not best-sellers; however, that was because there were very few true best-sellers (this agrees with the F1 scores per-category in the above table). The model struggled with correctly classifying true best sellers and had many more FP and FN than TP.\n\n\n\n\n\n\n\nFigure 4: The confusion matrix of true vs predicted labels of the random forest classifier.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/dtrees.html#conclusion",
    "href": "models/dtrees.html#conclusion",
    "title": "Decision Trees",
    "section": "",
    "text": "For the classification task, some conclusions and observations can be drawn from the results:\n\nThe accuracy is deceptively high for both classifiers, as the second True label only was present in 1% of the data.\nThe classifiers were biased towards predicting that a product was NOT a best-seller, as that gave the highest accuracy ultimately.\nAlthough the model had more FP than TP, overall it had many more FN than FP, indicating once again that the model tended to classify products as not best-sellers.\n\nFor the regression task, similarly we can draw observations from the results:\n\nThough the MAE was low, the low R^2 score indicates that the model overall tended to not predict the price well.\nBased on the residuals plots, the model did best (most residuals near zero) when the predicted price was around $150.\nThe model tended to underprice and overprice when further from the $150 price value.\n\nOverall, it seemed the models struggled to perform well but did adequately, especially the regression model which was mildly suprising. It can be concluded that the best-seller status of a product, along with price, is somewhat hard to predict from just the numerical values present in the dataset.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "models/bayes.html",
    "href": "models/bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "How Naive Bayes classifies categories.\n\n\n\n\nNaive Bayes is a popular and simple probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification tasks, such as spam email detection, sentiment analysis, and medical diagnosis. The “naive” part of its name comes from the assumption that features are independent of each other, which is often not true in real-world data but simplifies the model and makes it computationally efficient. It uses Bayes’ Theorem, which calculates the probability of a hypothesis (H) given some evidence (E). Standard Multinomial Naive Bayes is suitable for text classification tasks where features are discrete and represent word counts or frequencies; it calculates the likelihood probabilities using the frequency of each word in each class of the label. Bernoulli Naive Bayes is suitable when features are binary (e.g., word presence or absence), and it considers whether features are present or absent in each class of the label.\nSmoothing is required in Naive Bayes models to handle situations where a particular feature (word) in the test data was not present in the training data. This can lead to zero probabilities and cause the model to fail during prediction. Laplace smoothing or add-one smoothing is a common technique where a small constant value (often 1) is added to all counts to avoid zero probabilities. In summary, Naive Bayes is a straightforward yet powerful algorithm for classification tasks, making probabilistic predictions based on Bayes’ theorem and assuming independence between features. Multinomial Naive Bayes works with word frequencies, while Bernoulli Naive Bayes handles binary features, and smoothing is essential to prevent zero probabilities.\nFor this particular project, multinomial Naive Bayes will be used with the feature of product Name, to predict the Category that the product falls into.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#overview",
    "href": "models/bayes.html#overview",
    "title": "Naive Bayes",
    "section": "",
    "text": "How Naive Bayes classifies categories.\n\n\n\n\nNaive Bayes is a popular and simple probabilistic machine learning algorithm based on Bayes’ theorem. It is commonly used for classification tasks, such as spam email detection, sentiment analysis, and medical diagnosis. The “naive” part of its name comes from the assumption that features are independent of each other, which is often not true in real-world data but simplifies the model and makes it computationally efficient. It uses Bayes’ Theorem, which calculates the probability of a hypothesis (H) given some evidence (E). Standard Multinomial Naive Bayes is suitable for text classification tasks where features are discrete and represent word counts or frequencies; it calculates the likelihood probabilities using the frequency of each word in each class of the label. Bernoulli Naive Bayes is suitable when features are binary (e.g., word presence or absence), and it considers whether features are present or absent in each class of the label.\nSmoothing is required in Naive Bayes models to handle situations where a particular feature (word) in the test data was not present in the training data. This can lead to zero probabilities and cause the model to fail during prediction. Laplace smoothing or add-one smoothing is a common technique where a small constant value (often 1) is added to all counts to avoid zero probabilities. In summary, Naive Bayes is a straightforward yet powerful algorithm for classification tasks, making probabilistic predictions based on Bayes’ theorem and assuming independence between features. Multinomial Naive Bayes works with word frequencies, while Bernoulli Naive Bayes handles binary features, and smoothing is essential to prevent zero probabilities.\nFor this particular project, multinomial Naive Bayes will be used with the feature of product Name, to predict the Category that the product falls into.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#data-prep",
    "href": "models/bayes.html#data-prep",
    "title": "Naive Bayes",
    "section": "Data Prep",
    "text": "Data Prep\n\n\nWith Naive Bayes, the input data of vectorized text works well; here, multinomial Naive Bayes will be used with the feature of product Name, to predict the Category that the product falls into. Table 1 shows the input data used for all models used, and the data can be found at this link.\n\n\n\n\n\nTable 1: The input feature pre-transformation for input into the model, and the label feature “Category”\n\n\n\n\n\n\n\n\nName\nCategory\n\n\n\n\nBB0117O Silver 57/18/145 unisex Eyewear Frame\nMen's Accessories\n\n\n6 Pack Rinse-Clean Spin Mop Replacement Heads Mop Replace Refills Microfiber Compatible with Spin Mop 2 Tank System,(Small Triangle & Blue)\nHousehold Cleaning Supplies\n\n\n10 PCS Baby Brine Shrimp Net Fish Sieve for Fish Tank Aquarium Fine Artemia Net Small for Brine Shrimp Hatchery\nFish & Aquatic Pets\n\n\nPLANTIFIQUE Foot Peel Mask with Avocado 2 Pack Peeling Foot Mask Dermatologically Tested - Repairs Heels & Removes Dead Skin for Baby Soft Feet - Exfoliating Foot Peel Mask for Dry Cracked Feet\nFoot, Hand & Nail Care Products\n\n\nUnisex-Child Jump Serve Slip on Sneaker\nBoys' Shoes\n\n\n\n\n\n\n\n\n\n\nThe data, before transforming into N-gram word vectors (more info below), must be split into training and testing datasets. This can easily be done by randomly choosing each row to go into training or test, with about 25% of the data going into testing. We also stratify the splitting of data on the label categories, for fairness in evaluation. We split before vectorization for fairness in the conversion/tranformation of the data into vectors.\nIt’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nBased on how the input feature is turned into a vector format, several different models will be run; the following formats of the intput transformed into a sparse matrix vector are used, where a sparse matrix is an efficient representation of a large matrix with many zeros:\n\nCount frequency vector, which does text preprocessing, tokenizing and filtering of stopwords in order to turn a text string into an N-gram of words feature vector. Essentially, each occurence of a word adds to a count in a given row, with the number of columns equaling the number of all words across all input (training) text. Stop words are common English words that should not be considered due to their frequency, like “and” or “the”. The stop words used here are [\"and\", \"for\", \"with\", \"to\"], which are the ones most commonly found in Amazon product titles.\nTerm-frequency (TF) vector, where instead of the count of a word appearing, the number of occurrences of each word in a document is divided by the total number of words in the document. This is to adjust for how longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\nBinary frequency vector, where instead of count a binary 0/1 is used to represent if a word appears in the given row Name. This is used for Bernoulli NB.\n\n\n\nThus, the following NB models will be used with the following data:\n\nMultinomial with count frequency vectors\nMultinomial with TF vectors\nBernoulli with binary frequency vectors\n\n\n\n\n\n&lt;343558x183699 sparse matrix of type '&lt;class 'numpy.float64'&gt; with 5305684 stored elements in Compressed Sparse Row format&gt;\n\n\n\nFigure 1: An example of how sparse matrices are used and the training data as a sparse matrix.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#code",
    "href": "models/bayes.html#code",
    "title": "Naive Bayes",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running Naive Bayes, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#results",
    "href": "models/bayes.html#results",
    "title": "Naive Bayes",
    "section": "Results",
    "text": "Results\nEach model was run as described above in “Data Prep”. The results can be seen in Table 2; as we can see, the base Multinomial NB was the best, with Bernoulli NS significantly underperforming.\n\n\n\nTable 2: The table of result accuracy for each model\n\n\n\n\n\n\n\n\n\n\nModel\nData Format\nAccuracy\n\n\n\n\nMultinomialNB\nCounts\n0.682597\n\n\nMultinomialNB\nTF\n0.606549\n\n\nBernoulliNB\nOccurence\n0.229279\n\n\n\n\n\n\nFor the best performing model, performance metrics were found for each category in the labels; in Table 3 we show the top 10 performing categories, along with the words that were found to have the highest log-prior-probabilities; aka, the words that most influenced the choosing of that category by the model.\n\n\n\nTable 3: The table of performance metrics for the top 15 correctly-labeled categories, along with the most influential words for each category.\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nprecision\nrecall\nf1-score\nTop Feature Words\n\n\n\n\nComputers & Tablets\n0.898876\n0.981595\n0.938416\n[‘laptop’, ‘10’, ‘ssd’, ‘intel’, ‘core’, ‘ram’]\n\n\nKnitting & Crochet Supplies\n0.933824\n0.937962\n0.935888\n[‘set’, ‘kit’, ‘needles’, ‘knitting’, ‘crochet’, ‘yarn’]\n\n\nAdditive Manufacturing Products\n0.937198\n0.9312\n0.934189\n[‘pla’, ‘pro’, ‘filament’, ‘ender’, ‘printer’, ‘3d’]\n\n\nSexual Wellness Products\n0.981538\n0.859838\n0.916667\n[‘vibrator’, ‘women’, ‘dildo’, ‘adult’, ‘toys’, ‘sex’]\n\n\nNeedlework Supplies\n0.926941\n0.900888\n0.913728\n[‘needle’, ‘kits’, ‘cross’, ‘stitch’, ‘kit’, ‘embroidery’]\n\n\nPet Bird Supplies\n0.974684\n0.857939\n0.912593\n[‘parakeet’, ‘toys’, ‘toy’, ‘parrot’, ‘cage’, ‘bird’]\n\n\nLight Bulbs\n0.865291\n0.954485\n0.907702\n[‘base’, ‘pack’, ‘bulbs’, ‘led’, ‘bulb’, ‘light’]\n\n\nCraft & Hobby Fabric\n0.919408\n0.891547\n0.905263\n[‘sewing’, ‘cotton’, ‘the’, ‘by’, ‘yard’, ‘fabric’]\n\n\nWall Art\n0.8475\n0.956276\n0.898608\n[‘room’, ‘canvas’, ‘poster’, ‘decor’, ‘art’, ‘wall’]\n\n\nVacuum Cleaners & Floor Care\n0.819462\n0.952381\n0.880936\n[‘compatible’, ‘filter’, ‘pack’, ‘cleaner’, ‘replacement’, ‘vacuum’]\n\n\nFish & Aquatic Pets\n0.922201\n0.837931\n0.878049\n[‘pump’, ‘water’, ‘filter’, ‘tank’, ‘fish’, ‘aquarium’]\n\n\nCutting Tools\n0.808673\n0.950525\n0.87388\n[‘bits’, ‘shank’, ‘set’, ‘inch’, ‘drill’, ‘bit’]\n\n\nIndustrial Materials\n0.85907\n0.888372\n0.873476\n[‘diy’, ‘length’, ‘12’, ‘in’, ‘sheet’, ‘rubber’]\n\n\nRain Umbrellas\n0.982609\n0.784722\n0.872587\n[‘compact’, ‘folding’, ‘travel’, ‘windproof’, ‘rain’, ‘umbrella’]\n\n\nPower Transmission Products\n0.861373\n0.879121\n0.870156\n[‘steel’, ‘ball’, ‘rubber’, ‘bearings’, ‘bearing’, ‘belt’]\n\n\n\n\n\n\nThe confusion matrix of all categories can be seen in Figure 2. As can be seen, this is not necessarily informative; instead, we create a confusion matrix of the subset of categories that had the top 15 F1-scores in Figure 3.\n\n\n\n\n\n\n\n\nFigure 2: The confusion matrix of all categories\n\n\n\n\n\n\n\n\n\n\nFigure 3: The confusion matrix for top 15 correctly-classified categories",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "models/bayes.html#conclusion",
    "href": "models/bayes.html#conclusion",
    "title": "Naive Bayes",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the above results, some interesting conclusions can be drawn:\n\nEven though term-frequency (TF) vectors are supposed to be a better data format for input into Naive Bayes models, the count frequency vector data format actually consistently outperformed.\nThough the general accuracy was 68% at best, that’s because there were 248 categories in the label. When examining the performance by each category, many had accuracy over 88% and 6 had performance over 91%.\nSome words were extremely predictive for a unique category; however, some words like “boy” and “clothing” led the model to easily mis-classify the category.\n\nOverall, the multinomial Naive Bayes model performed surprisingly well at predicting the category of a given Amazon product given its product name.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Home\n\n\nWelcome to the homepage for machine learning on Amazon retail product data, made by Bhavana Jonnalagadda! You can see all pages listed below, or use the navbar to navigate the pages.\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA and SVD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSVM (Support Vector Machine)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Prep / EDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Code",
      "Home"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Conclusions\nPlaceholder\n\n\nReferences\n\n\nBell, Sean, and Kavita Bala. 2015. “Learning Visual Similarity for Product Design with Convolutional Neural Networks.” ACM Trans. Graph. 34 (4). https://doi.org/10.1145/2766959.\n\n\nChen, Le, Alan Mislove, and Christo Wilson. 2016. “An Empirical Analysis of Algorithmic Pricing on Amazon Marketplace.” In Proceedings of the 25th International Conference on World Wide Web, 1339–49. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2883089.\n\n\nChevalier, Judith, and Austan Goolsbee. 2003. “Measuring Prices and Price Competition Online: Amazon.com and BarnesandNoble.com.” Quantitative Marketing and Economics 1 (2): 203–22. https://doi.org/10.1023/A:1024634613982.\n\n\nHsu, Chih-Wei, Chih-Chung Chang, Chih-Jen Lin, et al. 2003. “A Practical Guide to Support Vector Classification.” Tapei.\n\n\nMedini, Tharun, Qixuan Huang, Yiqiu Wang, Vijai Mohan, and Anshumali Shrivastava. 2019. “Extreme Classification in Log Memory Using Count-Min Sketch: A Case Study of Amazon Search with 50M Products.” CoRR abs/1910.13830. http://arxiv.org/abs/1910.13830.\n\n\n“Nested versus non-nested cross-validation.” 2024. Scikit-Learn. https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nShaikh, Shakila, Sheetal Rathi, and Prachi Janrao. 2017. “Recommendation System in e-Commerce Websites: A Graph Based Approached.” In 2017 IEEE 7th International Advance Computing Conference (IACC), 931–34. https://doi.org/10.1109/IACC.2017.0189.\n\n\nSmith, Brent, and Greg Linden. 2017. “Two Decades of Recommender Systems at Amazon.com.” IEEE Internet Computing 21 (3): 12–18. https://doi.org/10.1109/MIC.2017.72.\n\n\nVos, Nelis J. de. 2015--2024. “Kmodes Categorical Clustering Library.” https://github.com/nicodv/kmodes.",
    "crumbs": [
      "Code",
      "Conclusions"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Prep / EDA",
    "section": "",
    "text": "Where the data source, processing, and visualization (EDA) is presented.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-collection",
    "href": "data.html#data-collection",
    "title": "Data Prep / EDA",
    "section": "Data Collection",
    "text": "Data Collection\nAmazon product information was scraped from the website using the API service ScraperAPI; this is because, as Amazon is a hugely popular website, they have many anti-scraping measures in place such as rate-limiting, IP blocking, dymamic loading, and such. Using the external API service, these limitations were able to be avoided. The search queries chosen to search for items were based on top 100 Amazon searches, found on this site and this site. An example of using the API, along with its core endpoint, is below.\n\nimport requests\n\npayload = {\n   'api_key': 'API_KEY',\n   'query': 'iphone 15 charger',\n   's': 'price-asc-rank'\n}\n\nresponse = requests.get('https://api.scraperapi.com/structured/amazon/search',\n                        params=payload).json()\n\nThe jupyter notebook code for the web scraping can be found here.\nAdditionally, more data was used to supplement the existing data. Since the scraped data was only about 26K rows, a Kaggle dataset was used that contains more than one million rows, had around the same fields as the scraped data, and was also from the USA (many Amazon Kaggle datasets were from the non-US).\nThe raw data from both sources can be seen below in Table 1; the scraped raw data CSV can also be viewed here.\n\n\n\n\nTable 1: The raw data from both datasets.\n\n\n\n\n\n\n\n\n\n(a) The raw data scraped from Amazon using ScraperAPI\n\n\n\n\n\n\ntype\nposition\nasin\nname\nimage\nhas_prime\nis_best_seller\nis_amazon_choice\nis_limited_deal\nstars\ntotal_reviews\nurl\navailability_quantity\nspec\nprice_string\nprice_symbol\nprice\noriginal_price\nsection_name\n\n\n\n\n0\nsearch_product\n32\nB0CV5Z91QR\nBetter Home Products Megan Wooden 6 Drawer Dou...\nhttps://m.media-amazon.com/images/I/81fL4qHZ4i...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/Better-Home-Products-Wo...\nNaN\n{}\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nsearch_product\n40\nB0CTZQ73SH\nFixtureDisplays® Bathtub and Shower Tension St...\nhttps://m.media-amazon.com/images/I/51F0eGaPI+...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/FixtureDisplays%C2%AE-B...\nNaN\n{}\n$22.60\n$\n22.60\nNaN\nNaN\n\n\n2\nsearch_product\n15\nB0CR633W5W\n4-Port USB 3.0 Hub Ultra-Thin Expand PC Connec...\nhttps://m.media-amazon.com/images/I/51sJxlimwP...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/4-Port-Ultra-Thin-Expan...\nNaN\n{}\n$3.99\n$\n3.99\nNaN\nNaN\n\n\n3\nsearch_product\n15\nB0CTQSPM8G\nMK000960GWSSD 960GB SATA 6G MU SFF SC DS SSD-1...\nhttps://m.media-amazon.com/images/I/11rdwQUxu0...\nFalse\nFalse\nFalse\nFalse\nNaN\nNaN\nhttps://www.amazon.com/Genuine-Original-MK0009...\nNaN\n{}\n$375.00\n$\n375.00\nNaN\nNaN\n\n\n4\nsearch_product\n44\nB0CJ9ZF7LQ\nSOL DE JANEIROCheirosa '62 Hair & Body Fragran...\nhttps://m.media-amazon.com/images/I/31mCVvwGqC...\nFalse\nFalse\nFalse\nFalse\n4.6\n14.0\nhttps://www.amazon.com/SOL-JANEIRO-Cheirosa-Fr...\nNaN\n{}\n$62.00\n$\n62.00\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The raw data gotten from Kaggle\n\n\n\n\n\n\nasin\ntitle\nimgUrl\nproductURL\nstars\nreviews\nprice\nlistPrice\ncategory_id\nisBestSeller\nboughtInLastMonth\n\n\n\n\n0\nB01CR9B6UY\nDC Cargo E-Track Ratcheting Straps Cargo Tie-D...\nhttps://m.media-amazon.com/images/I/81KFTHU7xx...\nhttps://www.amazon.com/dp/B01CR9B6UY\n4.7\n593\n36.99\n0.00\n24\nFalse\n100\n\n\n1\nB0BWFRDBBW\nLittle Girls Glitter Tulle Dress, Sparkle Polk...\nhttps://m.media-amazon.com/images/I/711QwZxcW8...\nhttps://www.amazon.com/dp/B0BWFRDBBW\n3.9\n7\n46.99\n0.00\n91\nFalse\n0\n\n\n2\nB00XV8CQGO\nClub Little Girl Summer Animal Collection Stic...\nhttps://m.media-amazon.com/images/I/41wQCnt7+E...\nhttps://www.amazon.com/dp/B00XV8CQGO\n0.0\n0\n9.99\n0.00\n95\nFalse\n50\n\n\n3\nB08MBJX3K1\nToddler Boys Girls Sneakers Size 5-12 Lightwei...\nhttps://m.media-amazon.com/images/I/81IscOXT6S...\nhttps://www.amazon.com/dp/B08MBJX3K1\n4.5\n0\n31.99\n35.99\n97\nFalse\n0\n\n\n4\nB08YKJDWLF\nCFS Cut to Fit Carbon Pad Pre Filter Roll for ...\nhttps://m.media-amazon.com/images/I/61j4Gpu4jH...\nhttps://www.amazon.com/dp/B08YKJDWLF\n4.6\n0\n12.59\n0.00\n171\nFalse\n0",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "Data Prep / EDA",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe datasets were cleaned seperately, then concatenated, then some final steps were taken to clean it.\nThe steps to clean the web-scaped data were:\n\nAdd date_scraped column\nRemove unecessary columns: type, position, has_prime, is_amazon_choice, is_limited_deal, availability_quantity, spec, price_string, price_symbol, section_name\nExpand and fix original_price\nRename columns to match standard snake case for merging both datasets\nDrop rows with no asin or name or price\nDrop rows with price of 0.0, since that doesn’t make sense\nFill NaN reviews column with 0\n\nThe steps to clean the Kaggle data were:\n\nAdd date_scraped column\nDrop rows with any NaNs\nFix list_price of $0 to be instead equal to price\nChange category_id to actual category by using category table\nDrop rows with price of $0, since that doesn’t make sense\nRename columns to match standard snake case for merging both datasets\n\nAnd then, after they were concatenated, the steps to clean were:\n\nRemove duplicates (by asin + date scraped)\nRename columns\n\nThe final cleaned (and concatenated) dataset can be seen in Table 2 (with the original raw data in Table 1):\n\n\n\n\nTable 2: The final unioned, cleaned, and processed data.\n\n\n\n\n\n\n\n\n\n\nAsin\nName\nImage Url\nIs Best Seller\nStars\nReviews\nUrl\nPrice\nDate Scraped\nList Price\nBought In Month\nCategory\n\n\n\n\n0\nB077H6599Q\nGenerac 7103 Cold Kit for 9kW - 22kW Air Coole...\nhttps://m.media-amazon.com/images/I/71+HddWhsK...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B077H6599Q\n94.99\n2023-11-01\n105.99\n0.0\nRV Parts & Accessories\n\n\n1\nB095HLD52Z\n3 Pack Apple MFi Certified iPhone Charger Cabl...\nhttps://m.media-amazon.com/images/I/61rqFEt6ku...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B095HLD52Z\n20.99\n2023-11-01\n20.99\n0.0\nAccessories & Supplies\n\n\n2\nB08XK4CBP8\n164pcs Blush White Balloons Garland Arch Kit P...\nhttps://m.media-amazon.com/images/I/71oD9RPq84...\nFalse\n4.5\n0.0\nhttps://www.amazon.com/dp/B08XK4CBP8\n14.99\n2023-11-01\n16.99\n50.0\nParty Decorations\n\n\n3\nB0C1Z9G9J9\nMudtun Bone Conduction Earbuds for Small Ear C...\nhttps://m.media-amazon.com/images/I/51js5mISHA...\nFalse\n3.5\n74.0\nhttps://www.amazon.com/dp/B0C1Z9G9J9\n19.99\n2023-11-01\n19.99\n0.0\nHeadphones & Earbuds\n\n\n4\nB0C5NB4SRN\nV-SHURA Music Gifts Table Lamp, Guitar Table L...\nhttps://m.media-amazon.com/images/I/710DKiJYTV...\nFalse\n4.8\n0.0\nhttps://www.amazon.com/dp/B0C5NB4SRN\n55.99\n2023-11-01\n55.99\n0.0\nHome Lighting & Ceiling Fans\n\n\n\n\n\n\n\n\n\n\nThe code for the data cleaning can be found here.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "data.html#data-preprocessing-visualization",
    "href": "data.html#data-preprocessing-visualization",
    "title": "Data Prep / EDA",
    "section": "Data Preprocessing / Visualization",
    "text": "Data Preprocessing / Visualization\nVarious types of EDA were performed in order to examine the data; as a note, most visuals are interactive (zoomable, pannable, etc). The code for all visualizations can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website) until all figures load.\n\n\nFirst, a histogram of all categories of all Amazon products is shown in Figure 1. Note scraped data did not have categories, but the Kaggle data did. It can be seen that from the figure, girl’s and boy’s clothing are the most populous categories, with toys & games the next most populous.\n\n\n\n\n                                                \n\n\nFigure 1: A histogram of all categories of all Amazon products. Note scraped data did not have categories, but the Kaggle data did.\n\n\n\n\nFigure 2 shows a histogram of the stars recieved for a product. It can be seen that the number of stars increases generally exponentially until 4.6 stars, with peaks at 0 and 5 stars as well. This data generally agrees with what can be seen on Amazon when browsing personally, and the peaks at 0 and 5 agree with having low number of reviews.\n\n\n\n\n                                                \n\n\nFigure 2: Histogram of the number of stars recieved for all products.\n\n\n\n\nFigure 3 shows violin plots of the numerical factors: volume of product bought in the month, the price of a product, and the number of reviews. A violin plot was chosen because a histogram would be visually uninformative, due to the amount of outliers. Some insights that can be drawn from these violin plots are:\n\nThese factors have a good number of outliers that are extremely large in value compared to the general distribution of values. This does not seem to be from incorrect values, but from sellers attempting to “game the system”.\n“Bought in Month” has outliers that are regularly spaced, indicating either a form of gaming the system by sellers that tend to follow specific formats, or Amazon only reports volume of bought products at low granularity for high-volume products.\nPrice tends to have outliers that continue regularly up the price scale, except for a couple at the $60-70k range. It is unclear whether to drop those rows containing those prices as they seem valid; it is entirely possible that sellers just list the product at a completely unreasonable price as a placeholder, and don’t expect anyone to purchase at that price.\nThe number of reviews for a product have been shown to be easily gameable with fake reviews, and is likely what is happening here.\n\n\n\n\n\n\n\n\nViolin plot of the number of a product bought in a month\n\n\n\n\n\nViolin plot of price of a product\n\n\n\n\n\n\n\nViolin plot of the number of reviews\n\n\n\n\n\nFigure 3: Violin plots (with overlayed box plot and outliers) of 3 of the main numerical columns.\n\n\n\nThe amount of stars, vs the number of reviews for a product can be seen in Figure 4, colored with the Is Best Seller flag. It can be seen that the distribution of Is Best Seller items tends to follow the overall distribution; additionally, the number of reviews tends to increase as the number of stars increases, with the products with the outlier number of reviews (over 200k) all above 4 stars.\n\n\n\n\n\n\n\n\nFigure 4: Stars vs number of reviews recieved by an amazon product, colored by whether the product was a best-seller.\n\n\n\n\n\nIn order to visualize the content of the text factors, namely the Category and Name of a product, a wordcloud format was used; wordclouds visualize the words that tend to appear in the text with the more frequent words appearing in larger text. In Figure 5, it can be seen that in the categories, “Care”, “Products”, “Boy’s Clothing”, “Consoles”, “Accessories”, and “Games” appear most. In the names of procucts, the words “Stainles Steel”, “Compatible”, “Heavy Duty”, “Boy”, “Girl”, and “Birthday” appear the most.\n\n\n\n\n\n\n\nWordcloud for categories of products\n\n\n\n\n\nWordcloud for the names of products\n\n\n\n\n\nFigure 5: Wordclouds (where more frequent appearing words are bigger) of the categories of products and the names of products.\n\n\n\nThe number of stars vs price, with price being on a log scale (to hangle outliers) is shown in Figure 6, along with being colored by if it’s a best seller. As was seen in Figure 4, the distribution of Is Best Seller items tends to follow the overall distribution; additionally, it can be seen that there are products with a price up to $10k across all number of stars.\n\n\n\n\n\n\n\n\nFigure 6: The number of stars a product recieved, vs the price in log scale, colored by whether the product was a best seller.\n\n\n\n\n\n\nInvestigation of products in both sets of data\nThe following plots focused on products that had the same Asin (identifying ID), that were in both the data personally scraped in 2024 and the Kaggle data from 2023. Investigating this subset of data could reveal a lot about how product prices, reviews, and other factors changed over time, as the only way to get time difference data was to use products that actually had more than one timepoint.\nFigure 7 shows the price vs list price of items, colored by what date they were scraped on, with a trendline plotted. It can be seen that on the whole, list price and actual price match and that prices seem to have increased since 2023 to 2024; however, below $50 price (when zooming in) the trendlines cross, implying that lower-priced products actually decreased in price.\n\n\n\n\n                                                \n\n\nFigure 7: Price vs list price of items with the same ASIN across dates scraped, with a trendline.\n\n\n\n\nGiven that outliers can be seen in price affecting the plot of the graph, it was decided for analysis to only consider those prices most populous, aka prices less than $800. Given that, the prices of each Asin were plotted, along with being colored by if the price increased from 2023 to 2024, or decreased. It can be seen that in Figure 8 that indeed for lower priced products (less than $50) the prices mostly did not increase, and instead decreased or stayed the same. The histogram shows the frequency of higher-priced procucts generally exponentially decreasing, and the likelihood of the price increasing going up.\n\n\n\n\n                                                \n\n\nFigure 8: Histogram of prices, colored by whether the change in price increased or decreased over time, for those items that were in both sets of data.\n\n\n\n\nFigure 9 shows a more granular form of the previous plot by plotting price vs the actual price difference amount from 2023 to 2024, with the ability to investigate which products are having such a dramatic increase or decrease in price over time. It can be seen that tech and suitcases were the outliers in price increase, and tech also being the outliers in price decrease.\n\n\n\n\n                                                \n\n\nFigure 9: Price vs the difference in price, over the two sets of data, colored by whether the price diff increased or decreased.\n\n\n\n\nFinally, it was investigated as to which product categories exactly had a price increase or decrease, and the ratio of number of increases to decreases. Only categories with more than 20 products were included, to reduce noise and to focus on meaningful categories. In Figure 10, it can be seen that “Sports and Outdoors”, “Makeup”, “Electrical Equipment”, and “Building Toys” all had more price increases than decreases; every other product category had more price decreases than increases, with “Smart Homes” and “Gift Cards” having the most price decreases.\n\n\n\n\n                                                \n\n\nFigure 10: For the included categories, a bar chat showing the ratio of products that had their price increase over time vs decrease.",
    "crumbs": [
      "Code",
      "Data Prep / EDA"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "An example of navigating Amazon\n\n\n\n\nIn today’s digital landscape dominated by e-commerce giants such as Amazon, the analysis of retail product data assumes paramount importance as a means to unravel the intricacies of consumer behavior, and unveil the complex relationships that exist between various products. The topic of this project poses a fundamental assertion: that mining the vast repository of Amazon’s retail product data holds the key to unlocking invaluable insights that can revolutionize marketing strategies, inform product development, and optimize overall business operations. This assertion underscores the critical role of data analytics in shaping the success trajectory of businesses operating in the contemporary marketplace. By delving into the granular details of consumer interactions with products on Amazon’s platform, stakeholders gain not only a deeper understanding of consumer preferences but also a nuanced perspective on the dynamics of the digital marketplace itself. The implications of this analysis extend far beyond mere academic curiosity; they are deeply intertwined with the strategic imperatives of businesses seeking to thrive amidst fierce competition and rapid technological advancements. The significance of analyzing Amazon retail product data transcends individual businesses, permeating into the realms of consumer behavior, marketing efficacy, policymaking, and academic research.\nAt its core, this endeavor resonates with a diverse array of stakeholders, each with a vested interest in deciphering the underlying patterns shaping modern consumption patterns and economic dynamics. Consumers, for instance, stand to benefit from the insights gleaned through data analysis, which can translate into more personalized shopping experiences, enhanced product recommendations, and greater transparency within the marketplace. Much work has been done, for example, to improve search engine results and product classification for recommendation purposes (Medini et al. 2019; Smith and Linden 2017). Marketers, on the other hand, find in this data a treasure trove of actionable intelligence, enabling them to fine-tune advertising strategies, optimize pricing models, and anticipate shifts in consumer preferences with a heightened degree of accuracy (Chen, Mislove, and Wilson 2016; Chevalier and Goolsbee 2003). Furthermore, researchers across various disciplines are drawn to the rich dataset offered by Amazon, utilizing sophisticated analytical techniques ranging from machine learning algorithms to traditional statistical methods to unravel the complexities of consumer behavior and product relationships (Shaikh, Rathi, and Janrao 2017; Bell and Bala 2015). Despite the considerable strides made in this field, there remains an abundance of untapped potential, beckoning further exploration and innovation to fully harness the transformative power of Amazon’s retail product data.\n\n\n\n\n\nML in retail\n\n\n\n\nIn this project, machine learning methods play a pivotal role in extracting meaningful insights from the vast sea of Amazon’s retail product data. Techniques such as Principal Component Analysis (PCA) enable dimensionality reduction, allowing for the identification of key features influencing consumer behavior. Clustering algorithms group similar products or consumers together, facilitating targeted marketing strategies and personalized recommendations. Neural networks, with their ability to learn intricate patterns, offer valuable predictive capabilities, aiding in demand forecasting and inventory management. Regression analysis uncovers relationships between variables, potentially helping businesses understand the impact of pricing, promotions, and other factors on sales. Support Vector Machines (SVM), Naive Bayes, and decision trees provide additional tools for classification and prediction tasks, enriching the analytical toolkit available to researchers and practitioners alike. Through the application of these advanced methodologies, one can navigate the complexities of the digital marketplace, leveraging data-driven insights to drive innovation and strategic decision-making.\nTo get specific, the way that machine learning will be used is in both supervised and unsupervised learning settings depends on the data present within this dataset. For unsupervised learning methods, much can be done with the text data (aka the title of the product and the categories); one can do embeddings, bag-of-words, or other methods that reduce text into a couple of meaningful tags/individual words. One can also do dimension reduction and clustering in unsupervised contexts, as there are plently of numerical features in the data (number of reviews, number of stars, price, etc). For supervised learning, the feature used for label could be the feature of “Is Best Seller”, which would give a binary label; or, for regression and numerical/continuous labeling, the feature “Price” could be used as it is an important feature to learn about and could potentially be predicted from the other features present. For both supervised and unsupervised learning, it is recognized that some features are essentially useless or not needed; those would be the unique strings for each product row, namely “ASIN”, “Image URL”, and “URL”. These are important info for getting context for the product (and potentially for aquiring new data in the form of image data), but not necessary for the techniques performed on this site as no computer vision methods will be used.\nIn addition to the techniques mentioned, reinforcement learning models can also be employed to optimize strategies in response to changing market dynamics. By simulating different scenarios and learning from interactions, these models can adapt and refine decision-making processes over time, enhancing business agility and resilience. Moreover, natural language processing (NLP) techniques can extract sentiment analysis from customer reviews, providing valuable insights into product perception and sentiment trends that influence purchasing decisions. The integration of these advanced methodologies into the analysis of Amazon’s retail product data signifies a holistic approach to understanding consumer behavior and market trends, empowering businesses to stay ahead in an ever-evolving digital landscape.\n\n\n\nIs there noticable trends in prices over time, over all products?\nAre there price trends over time within a specific product category?\nIs there any meaningful similarities in the images between products with similar descriptions?\nAre there price/rating similarities between the products with similar images?\nDo ratings have any correlation with the price of products?\nDo ratings have any correlation with the product category?\nCan the price of a product be predicted based on all other factors?\nCan the rating of a product be predicted based on all other factors?\nCan the category of a product be predicted based on the description?\nIs the description of a product similar to the generated description based on the product image (or the generated bag of words, tags, etc)?",
    "crumbs": [
      "Code",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#questions-to-answer",
    "href": "intro.html#questions-to-answer",
    "title": "Introduction",
    "section": "",
    "text": "Is there noticable trends in prices over time, over all products?\nAre there price trends over time within a specific product category?\nIs there any meaningful similarities in the images between products with similar descriptions?\nAre there price/rating similarities between the products with similar images?\nDo ratings have any correlation with the price of products?\nDo ratings have any correlation with the product category?\nCan the price of a product be predicted based on all other factors?\nCan the rating of a product be predicted based on all other factors?\nCan the category of a product be predicted based on the description?\nIs the description of a product similar to the generated description based on the product image (or the generated bag of words, tags, etc)?",
    "crumbs": [
      "Code",
      "Introduction"
    ]
  },
  {
    "objectID": "models/clustering.html",
    "href": "models/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a fundamental unsupervised learning technique in machine learning that involves grouping similar data points together based on certain features or characteristics. The goal of clustering is to partition a dataset into subsets or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. This process helps in discovering inherent structures within the data, identifying patterns, and gaining insights into the underlying relationships.\nPartitional clustering algorithms, such as k-means, partition the data into a predefined number of clusters. The k-means algorithm iteratively assigns data points to the nearest cluster centroid based on a distance metric, commonly the Euclidean distance. The centroids are then recalculated as the mean of the data points in each cluster, and the process continues until convergence. K-means is efficient for large datasets and works well when clusters are spherical and of similar size. However, it may struggle with non-linear or irregularly shaped clusters and requires specifying the number of clusters beforehand.\nHierarchical clustering, on the other hand, does not require specifying the number of clusters in advance. It creates a hierarchical tree-like structure, called a dendrogram, by iteratively merging or splitting clusters based on their similarity. There are two main approaches to hierarchical clustering: agglomerative (bottom-up) and divisive (top-down). Agglomerative clustering starts with each data point as a separate cluster and then merges the closest pairs of clusters until only one cluster remains. Divisive clustering begins with all data points in one cluster and recursively splits them into smaller clusters. The choice of distance metric, such as Euclidean, Manhattan, or Mahalanobis distance, influences the clustering results in hierarchical algorithms.\nThe distance metric used in clustering plays a crucial role in determining the similarity or dissimilarity between data points. Euclidean distance is commonly used and measures the straight-line distance between two points in Euclidean space. It works well when data features are continuous and have similar scales. Manhattan distance, also known as city block distance, calculates the distance along the axes, making it suitable for high-dimensional data or when features are not directly comparable in scale. Mahalanobis distance accounts for the covariance between variables and is useful when dealing with correlated features or data with different variances.\nIn this specific project, we will be using clustering in order to see if any trends can be found by hierarchical clustering that show relationships between the features that were previously unknown.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#overview",
    "href": "models/clustering.html#overview",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a fundamental unsupervised learning technique in machine learning that involves grouping similar data points together based on certain features or characteristics. The goal of clustering is to partition a dataset into subsets or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. This process helps in discovering inherent structures within the data, identifying patterns, and gaining insights into the underlying relationships.\nPartitional clustering algorithms, such as k-means, partition the data into a predefined number of clusters. The k-means algorithm iteratively assigns data points to the nearest cluster centroid based on a distance metric, commonly the Euclidean distance. The centroids are then recalculated as the mean of the data points in each cluster, and the process continues until convergence. K-means is efficient for large datasets and works well when clusters are spherical and of similar size. However, it may struggle with non-linear or irregularly shaped clusters and requires specifying the number of clusters beforehand.\nHierarchical clustering, on the other hand, does not require specifying the number of clusters in advance. It creates a hierarchical tree-like structure, called a dendrogram, by iteratively merging or splitting clusters based on their similarity. There are two main approaches to hierarchical clustering: agglomerative (bottom-up) and divisive (top-down). Agglomerative clustering starts with each data point as a separate cluster and then merges the closest pairs of clusters until only one cluster remains. Divisive clustering begins with all data points in one cluster and recursively splits them into smaller clusters. The choice of distance metric, such as Euclidean, Manhattan, or Mahalanobis distance, influences the clustering results in hierarchical algorithms.\nThe distance metric used in clustering plays a crucial role in determining the similarity or dissimilarity between data points. Euclidean distance is commonly used and measures the straight-line distance between two points in Euclidean space. It works well when data features are continuous and have similar scales. Manhattan distance, also known as city block distance, calculates the distance along the axes, making it suitable for high-dimensional data or when features are not directly comparable in scale. Mahalanobis distance accounts for the covariance between variables and is useful when dealing with correlated features or data with different variances.\nIn this specific project, we will be using clustering in order to see if any trends can be found by hierarchical clustering that show relationships between the features that were previously unknown.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#data-preperation",
    "href": "models/clustering.html#data-preperation",
    "title": "Clustering",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nClustering Methods\nClustering requires only unlabeled numeric data, so the following work was done in order to prepare the data:\n\nGet only the numerical/boolean columns\nFill NaNs with zeros\nScale the data to min/max [0, 1]\n\nA sample of the final form of the data used can be seen in Table 1. A link to the data used can be found here.\n\n\n\n\nTable 1: The data used for clustering.\n\n\n\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\n\n\n\n\n0\n0.78\n0.3\n0.0153\n0.0275\n0.333\n\n\n0\n0.88\n0\n0.0684\n0.0662\n0.0333\n\n\n0\n0.92\n0\n0.0258\n0.025\n0.0167\n\n\n0\n0.9\n0\n0.108\n0.104\n0\n\n\n0\n0.88\n0\n0.0153\n0.0149\n0.0333",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#code",
    "href": "models/clustering.html#code",
    "title": "Clustering",
    "section": "Code",
    "text": "Code\n\n\nThe jupyter notebook code for running the PCA can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#results",
    "href": "models/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nImportant\n\n\n\nIf the interactive figures don’t load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.\n\n\nThe following clustering algorithms were tested:\n\nK-means clustering via sklearn, with k = 2…5\nHierarchical clustering via sklearn, using Cosine Similarity as the distance measure\nKprototypes viea kprot, in order to cluster on the categorical data as well\n\nThe Kprototypes algorithm performed nearly exactly the same as hierarchical clustering, so from this point on we will only be discussing the first 2 clustering methods used.\nUnfortunately, the k-means clustering performed poorly, as can be seen in Figure 1. Additionally, the silhouette scores changed dramatically upon each re-run of the algorithm, showing poor convergence.\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) The elbow plot of WCSS vs rank.\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) The plot of silhouette scores vs rank.\n\n\n\n\n\n\n\nFigure 1: The plots showcasing the effectiveness of different ranks with Kmeans clustering.\n\n\n\nThe hierarchical clustering also struggled, likely as there were over 1M observations seen; ultimately, as seen in Figure 2, the clustering that produced 2 clusters seemed to be the “best”.\n\n\n\n\n\n\nFigure 2: A dengrogram of the hierarchical clustering performed.\n\n\n\nWe compare the output of the clusters found, at rank 2, across all algorithms used in Figure 3. As one can see, they were very similar, with Rand indexes very high (a Rand index of 1 means perfectly same clusters, and 0 meaning no relation at all).\n\n\n\n\n                                                \n\n\nFigure 3: A comparison of the clustering models ran, at rank 2, using the adjusted Rand Score Index.\n\n\n\n\nWe include a splom of the clusters found by hierarchical clutering in Figure 4. By visually inspecting the colored clusters, we can describe the clusters as the following:\n\n\nCluster 1\n\nStars above about 2.5\nMost review amounts\nNo discernible pattern amongst Price, Date Scraped\n\n\nCluster 2\n\nStars below about 2.5\nSmall amount of reviews\nNo discernible pattern amongst Price, Date Scraped\n\n\n\n\n\n\n\n\n\nFigure 4: A splom of selected features (and the subset of rows used in the clustering methods), colored by the cluster assigned to the data point.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/clustering.html#conclusions",
    "href": "models/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nFrom the above results, several interesting conclusions can be drawn:\n\nThe clusters found were in general very similar to each other across all algorithms used, when rank = 2.\nThe clusters found seemed to indicate a strong relation between products that have low amount of stars and reviews, vs everything else.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Clustering"
    ]
  },
  {
    "objectID": "models/nn.html",
    "href": "models/nn.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural networks are machine learning models inspired by the brain’s neural structure, comprising interconnected neurons organized in layers. Input data enters the input layer, propagates through hidden layers where computations with weights and biases occur, and outputs are generated from the output layer. Activation functions introduce non-linearity, aiding in learning complex patterns. Training involves adjusting weights and biases via algorithms like backpropagation to minimize prediction errors using labeled data. Once trained, neural networks can make predictions on new data, excelling in tasks like image recognition and natural language processing due to their ability to extract features, adapt to domains, and achieve high performance. In general their use and complexity (both mathematical formulation and construction) have exploded in recent years, and are used heavily in massive AI/ML applications (such as the popular ChatGPT).\nIn this project, two simple neural networks will be trained; one in order to predict Is Best Seller, and another to predict the value of Price Diff, the change in product price over time.\n\n\n\nThe data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models. The following transformations were done to the original (cleaned) dataset in order to make the 2 different input datasets:\n\n\nBest Seller NN\n\nOnly select rows that contain a non-NaN value for Category and Bought in Month\nSelect rows such that 1% of them are Is Best Seller == True. This is because the dataset contains more than &gt;1M rows, with only 0.4% of them Is Best Seller == True; this makes the prediction less biased and invariant for the model.\nMake datetime binary, as there are only 2 values used for it anyways.\nOne-hot encode categorical var Category\n\n\nPrice Difference NN\n\nMake DF of products that have a calculable price difference over time\nUse initial price and data, in order to predict later price change\nOnly use data with Category categories that have more than 5 products occuring, to improve prediction\nScale numerical data to [0, 1], make bool Is Best Seller an int\nOne-hot encode categorical var Category\n\n\n\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nA sample of the final form of the data used can be seen in Table 1. A link to the data used can be found here.\n\n\n\nTable 1: The form of the final dataset used for each of the 2 models, without the one-hot encoded Category columns included. The training and test sets made from these data look exactly the same, except with differing number of observations.\n\n\n\n\n\nSample of input data, where Is Best Seller is the binary classification label to predict.\n\n\n\n\n\n\n\n\n\n\nStars\nReviews\nPrice\nDate Scraped\nBought In Month\nIs Best Seller\n\n\n\n\n4.8\n508\n20.99\n0\n900\n0\n\n\n5\n2\n17.99\n0\n200\n0\n\n\n4.8\n0\n550\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n4.1\n0\n22.9\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n3.3\n0\n34.52\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nSample of input data, where Price Diff is the continuous numerical label to predict.\n\n\n\n\n\n\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\nPrice Diff\n\n\n\n\n0\n4.7\n5815\n12.97\n12.97\n3000\n1\n\n\n0\n4.4\n2457\n57.96\n57.96\n900\n-11.95\n\n\n0\n3.8\n185\n7.19\n7.19\n500\n1.69\n\n\n0\n4.6\n657\n89.99\n139.99\n300\n0.04\n\n\n0\n4.8\n1907\n8.97\n12.95\n1000\n-3.98\n\n\n0\n4.8\n72\n19.99\n19.99\n0\n0\n\n\n0\n4.8\n1783\n21.99\n21.99\n200\n0\n\n\n0\n4.5\n668\n244.95\n244.95\n50\n37.05\n\n\n0\n4\n1615\n211.94\n211.94\n0\n32.15\n\n\n0\n4.7\n151\n7.95\n7.95\n0\n-0.24\n\n\n\n\n\n\n\n\n\n\n\nThere are a lot of hyperparameters (options of a model that must be set before training/fitting it) associated with neural networks. For both networks, 3 hidden layers were used, with a sigmoid activation function after each layer. They also both used the ADAM algorithm optimizer for performing the gradient descent, with default values for rate and tolerance. They differ in the following hyperparameters:\n\n\nBest Seller NN\n\nInput dimensions: 253 cols (including the one-hot encoded Category columns)\nEpochs: 50, loss did not improve after that\nLoss function: Cross-entropy loss with extra weight on the True/1 class, in order to account for the imbalance and to properly get loss on the two binary classes.\nOutput: vector of dims [batch size, 2] where the 2 output values are the predicted probabilities for each class.\nBatch size: 100, as there are more than &gt; 1M input observations.\n\n\nPrice Difference NN\n\nInput dimensions: 118 cols (including the one-hot encoded Category columns)\nEpochs: 200, loss did not improve after that\nLoss function: mean squared error (MSE) loss\nOutput: vector of dims [batch size, 1] where the output value is the predicted price difference\nBatch size: 1, as there is only about a thousand training input observations.\n\n\n\nThe final structure of the networks can be seen below in Results.\n\n\n\n\n\nThe jupyter notebook code for running the neural networks, data prep, and everything else on this page can be found here, or click on the link card to the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe structures of each of the networks can be seen below in Figure 1. “Gemm” stands for General Matrix Multiplication, and represents the standard linear layer with weights B and bias C.\n\n\n\n\n\n\n\nThe structure of the “Is Best Seller” NN\n\n\n\n\n\nThe structure of the “Price Diff” NN\n\n\n\n\n\nFigure 1: The NN structures used\n\n\n\n\n\n\nThe average loss found at each epoch of the training can be seen in Figure 2. As can be seen, both NNs improved over time and had a typical bottoming-out of improvement, though at different number of epochs. We can also observe that the validation loss (or the loss when checked against the test data at that time) varies wildly, with spikes in both plots. The Price Diff NN even has the validation loss being consistently lower than the training loss, which is unusual; it is more common to see it being higher based on the amount of overfitting happening.\n\n\n\n\n\n\n\n“Is Best Seller” NN\n\n\n\n\n\n“Price Diff” NN\n\n\n\n\n\nFigure 2: The loss plots over all training epochs\n\n\n\nBest Seller NN\n\n\nThe performance metrics of the “Is Best Seller” NN can be seen in Table 2. As we can see, the model actually achieved lower accuracy (95%) than would have been gotten from predicting everything to be the dominant class (which comprised 99% of the data); this was due to the loss function being weighted to be biased towards the smaller class Is Best Seller=True.\nThe confusion matrix can be seen in Figure 3. As we can see, many observations were predicted to be a FP (flase positive), which aligns with the earlier observation that the network was biased towards the smaller class.\n\n\n\n\nTable 2: The performance metrics of the “Is Best Seller” NN.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\naccuracy\n\n\n\n\nNot Best Seller\n0.998181\n0.952533\n0.974823\n342237\n\n\n\nIs Best Seller\n0.062067\n0.644098\n0.113223\n1669\n\n\n\n\n\n\n\n\n0.951036\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The confusion matrix of the “Is Best Seller” NN.\n\n\n\n\n\nPrice Difference NN\n\n\n\n\n\n\n\n\nFigure 4: The plot of the actual true price difference, visualized against how much of a difference the model’s predicted value was against the true value.\n\n\n\n\nAs the model was fitting to a continuous label, there are no binary/classification report we can use; however, the overall MSE was \\(6.056\\).\nIn Figure 4, we observe the plot of the actual true price difference, visualized against how much of a difference the model’s predicted value was against the true value. As we can see, the model di fairly well at getting near the true value for price diffs of around -$4.30 to $300; we can also see that the model did much better for negative values than for positive values. Additionally, as the true price differences become large, the model struggles to adjust and predict the large values.\n\n\n\n\n\n\nFrom the above results, several interesting conclusions can be drawn:\nThe networks were able to learn and fit to the data, which was evident by the loss rate during training. They also performed better than other ML methods used in this project, on the same tasks. However, they did not perform amazingly; this seems like a lack in the amount of data (in the case of price difference prediction) or not predictive enough variables in the dataset (in the case of predicting best seller).",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/nn.html#overview",
    "href": "models/nn.html#overview",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural networks are machine learning models inspired by the brain’s neural structure, comprising interconnected neurons organized in layers. Input data enters the input layer, propagates through hidden layers where computations with weights and biases occur, and outputs are generated from the output layer. Activation functions introduce non-linearity, aiding in learning complex patterns. Training involves adjusting weights and biases via algorithms like backpropagation to minimize prediction errors using labeled data. Once trained, neural networks can make predictions on new data, excelling in tasks like image recognition and natural language processing due to their ability to extract features, adapt to domains, and achieve high performance. In general their use and complexity (both mathematical formulation and construction) have exploded in recent years, and are used heavily in massive AI/ML applications (such as the popular ChatGPT).\nIn this project, two simple neural networks will be trained; one in order to predict Is Best Seller, and another to predict the value of Price Diff, the change in product price over time.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/nn.html#data-prep",
    "href": "models/nn.html#data-prep",
    "title": "Neural Networks",
    "section": "",
    "text": "The data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models. The following transformations were done to the original (cleaned) dataset in order to make the 2 different input datasets:\n\n\nBest Seller NN\n\nOnly select rows that contain a non-NaN value for Category and Bought in Month\nSelect rows such that 1% of them are Is Best Seller == True. This is because the dataset contains more than &gt;1M rows, with only 0.4% of them Is Best Seller == True; this makes the prediction less biased and invariant for the model.\nMake datetime binary, as there are only 2 values used for it anyways.\nOne-hot encode categorical var Category\n\n\nPrice Difference NN\n\nMake DF of products that have a calculable price difference over time\nUse initial price and data, in order to predict later price change\nOnly use data with Category categories that have more than 5 products occuring, to improve prediction\nScale numerical data to [0, 1], make bool Is Best Seller an int\nOne-hot encode categorical var Category\n\n\n\nThe data was also split into 25% test data, and 75% training data. It’s essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model’s performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.\nA sample of the final form of the data used can be seen in Table 1. A link to the data used can be found here.\n\n\n\nTable 1: The form of the final dataset used for each of the 2 models, without the one-hot encoded Category columns included. The training and test sets made from these data look exactly the same, except with differing number of observations.\n\n\n\n\n\nSample of input data, where Is Best Seller is the binary classification label to predict.\n\n\n\n\n\n\n\n\n\n\nStars\nReviews\nPrice\nDate Scraped\nBought In Month\nIs Best Seller\n\n\n\n\n4.8\n508\n20.99\n0\n900\n0\n\n\n5\n2\n17.99\n0\n200\n0\n\n\n4.8\n0\n550\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n4.1\n0\n22.9\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n3.3\n0\n34.52\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nSample of input data, where Price Diff is the continuous numerical label to predict.\n\n\n\n\n\n\n\n\n\n\n\nIs Best Seller\nStars\nReviews\nPrice\nList Price\nBought In Month\nPrice Diff\n\n\n\n\n0\n4.7\n5815\n12.97\n12.97\n3000\n1\n\n\n0\n4.4\n2457\n57.96\n57.96\n900\n-11.95\n\n\n0\n3.8\n185\n7.19\n7.19\n500\n1.69\n\n\n0\n4.6\n657\n89.99\n139.99\n300\n0.04\n\n\n0\n4.8\n1907\n8.97\n12.95\n1000\n-3.98\n\n\n0\n4.8\n72\n19.99\n19.99\n0\n0\n\n\n0\n4.8\n1783\n21.99\n21.99\n200\n0\n\n\n0\n4.5\n668\n244.95\n244.95\n50\n37.05\n\n\n0\n4\n1615\n211.94\n211.94\n0\n32.15\n\n\n0\n4.7\n151\n7.95\n7.95\n0\n-0.24",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/nn.html#model-selectiondefinition",
    "href": "models/nn.html#model-selectiondefinition",
    "title": "Neural Networks",
    "section": "",
    "text": "There are a lot of hyperparameters (options of a model that must be set before training/fitting it) associated with neural networks. For both networks, 3 hidden layers were used, with a sigmoid activation function after each layer. They also both used the ADAM algorithm optimizer for performing the gradient descent, with default values for rate and tolerance. They differ in the following hyperparameters:\n\n\nBest Seller NN\n\nInput dimensions: 253 cols (including the one-hot encoded Category columns)\nEpochs: 50, loss did not improve after that\nLoss function: Cross-entropy loss with extra weight on the True/1 class, in order to account for the imbalance and to properly get loss on the two binary classes.\nOutput: vector of dims [batch size, 2] where the 2 output values are the predicted probabilities for each class.\nBatch size: 100, as there are more than &gt; 1M input observations.\n\n\nPrice Difference NN\n\nInput dimensions: 118 cols (including the one-hot encoded Category columns)\nEpochs: 200, loss did not improve after that\nLoss function: mean squared error (MSE) loss\nOutput: vector of dims [batch size, 1] where the output value is the predicted price difference\nBatch size: 1, as there is only about a thousand training input observations.\n\n\n\nThe final structure of the networks can be seen below in Results.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/nn.html#code",
    "href": "models/nn.html#code",
    "title": "Neural Networks",
    "section": "",
    "text": "The jupyter notebook code for running the neural networks, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/nn.html#results",
    "href": "models/nn.html#results",
    "title": "Neural Networks",
    "section": "",
    "text": "The structures of each of the networks can be seen below in Figure 1. “Gemm” stands for General Matrix Multiplication, and represents the standard linear layer with weights B and bias C.\n\n\n\n\n\n\n\nThe structure of the “Is Best Seller” NN\n\n\n\n\n\nThe structure of the “Price Diff” NN\n\n\n\n\n\nFigure 1: The NN structures used\n\n\n\n\n\n\nThe average loss found at each epoch of the training can be seen in Figure 2. As can be seen, both NNs improved over time and had a typical bottoming-out of improvement, though at different number of epochs. We can also observe that the validation loss (or the loss when checked against the test data at that time) varies wildly, with spikes in both plots. The Price Diff NN even has the validation loss being consistently lower than the training loss, which is unusual; it is more common to see it being higher based on the amount of overfitting happening.\n\n\n\n\n\n\n\n“Is Best Seller” NN\n\n\n\n\n\n“Price Diff” NN\n\n\n\n\n\nFigure 2: The loss plots over all training epochs\n\n\n\nBest Seller NN\n\n\nThe performance metrics of the “Is Best Seller” NN can be seen in Table 2. As we can see, the model actually achieved lower accuracy (95%) than would have been gotten from predicting everything to be the dominant class (which comprised 99% of the data); this was due to the loss function being weighted to be biased towards the smaller class Is Best Seller=True.\nThe confusion matrix can be seen in Figure 3. As we can see, many observations were predicted to be a FP (flase positive), which aligns with the earlier observation that the network was biased towards the smaller class.\n\n\n\n\nTable 2: The performance metrics of the “Is Best Seller” NN.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\naccuracy\n\n\n\n\nNot Best Seller\n0.998181\n0.952533\n0.974823\n342237\n\n\n\nIs Best Seller\n0.062067\n0.644098\n0.113223\n1669\n\n\n\n\n\n\n\n\n0.951036\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The confusion matrix of the “Is Best Seller” NN.\n\n\n\n\n\nPrice Difference NN\n\n\n\n\n\n\n\n\nFigure 4: The plot of the actual true price difference, visualized against how much of a difference the model’s predicted value was against the true value.\n\n\n\n\nAs the model was fitting to a continuous label, there are no binary/classification report we can use; however, the overall MSE was \\(6.056\\).\nIn Figure 4, we observe the plot of the actual true price difference, visualized against how much of a difference the model’s predicted value was against the true value. As we can see, the model di fairly well at getting near the true value for price diffs of around -$4.30 to $300; we can also see that the model did much better for negative values than for positive values. Additionally, as the true price differences become large, the model struggles to adjust and predict the large values.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/nn.html#conclusions",
    "href": "models/nn.html#conclusions",
    "title": "Neural Networks",
    "section": "",
    "text": "From the above results, several interesting conclusions can be drawn:\nThe networks were able to learn and fit to the data, which was evident by the loss rate during training. They also performed better than other ML methods used in this project, on the same tasks. However, they did not perform amazingly; this seems like a lack in the amount of data (in the case of price difference prediction) or not predictive enough variables in the dataset (in the case of predicting best seller).",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Neural Networks"
    ]
  },
  {
    "objectID": "models/regression.html",
    "href": "models/regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It’s a fundamental technique in machine learning and statistics, often used for predictive analysis and understanding the underlying patterns in data.\n\nAssumptions: Linear regression assumes a linear relationship between the independent variables (features) and the dependent variable (target). It also assumes that the errors or residuals (the differences between actual and predicted values) are normally distributed and have constant variance (homoscedasticity).\nModel Representation: The linear regression model is represented as: \\[\n  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon\n  \\] Where:\n\n\\(y\\) is the dependent variable.\n\\(x_1, x_2, ..., x_n\\) are the independent variables.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n\\) are the coefficients (parameters) that represent the relationship between the independent and dependent variables.\n\\(\\epsilon\\) is the error term.\n\n\nDuring the training process, the goal is to find the best values for the coefficients (weights) that minimize the difference between the actual and predicted values. This is often done using optimization algorithms like Ordinary Least Squares (OLS) or gradient descent. Once the model is trained, it can be used to make predictions on new data by plugging in the values of the independent variables into the model equation. Limitations of linear regression include:\n\nLinearity Assumption: Linear regression assumes a linear relationship between variables. If the true relationship is non-linear, linear regression may not capture it accurately.\nOutliers: Linear regression is sensitive to outliers, which are data points that deviate significantly from the rest of the data. Outliers can distort the model and lead to inaccurate predictions.\nMulticollinearity: When independent variables are highly correlated, it can cause multicollinearity issues in linear regression, leading to unstable coefficient estimates.\nOverfitting/Underfitting: Linear regression can suffer from overfitting (capturing noise in the training data) or underfitting (oversimplifying the relationship), especially when dealing with complex data patterns.\nAssumption Violations: If the assumptions of linear regression are violated (e.g., non-normality of residuals, heteroscedasticity), the model’s reliability and interpretability may be compromised.\n\nDespite these limitations, linear regression remains a valuable and widely used tool due to its simplicity, interpretability, and ability to provide insights into relationships between variables.\nIn this project, we specifically perform very simple linear regression by picking only one independent variable (Reviews) and the dependent variable (Stars).\n\n\n\nWe choose the independent variable to be Reviews and the dependent variable to be Stars, as they appear to have the most predictable linear relationship (see Figure 1); the variables List Price and Price are more linear but they also have too much multicollinearity and are not useful for prediction.\n\n\n\n\n\n\nFigure 1: Splom of all the numerical variables in the dataset.\n\n\n\n\n\n\nAs there is only one independent variable, no tranformations/scaling was needed for the dataset other than selecting the subset of the columns. Table 1 shows the final input data used for all models used, and the data can be found at this link.\n\n\n\nTable 1: A sample of the data used for the linear regression modeling. Reviews is the independent variable and Stars is the dependent variable/prediction.\n\n\n\n\n\nReviews\nStars\n\n\n\n\n515\n4.2\n\n\n0\n3.7\n\n\n502\n4.3\n\n\n0\n4.4\n\n\n3364\n4.5\n\n\n1106\n3.8\n\n\n0\n4\n\n\n0\n4.8\n\n\n0\n4.6\n\n\n164\n4.4\n\n\n\n\n\n\n\n\n\n\n\nThe jupyter notebook code for running linear regression, data prep, and everything else on this page can be found here, or click on the link card to the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe resulting fitted regression line formula is below, and can be seen in Figure 2.\n\\[\n\\textbf{y} = 0.0001696\\ \\textbf{x} + 3.9747\n\\]\n\ny: What it predicts, aka the average number of stars a product recieved.\nx: The input number of reviews a product recieved.\n\n\n\n\n\n\n\n\nFigure 2: The original data, with the fitted linear regression line plotted over.\n\n\n\n\n\n\n\n\n\n\nTable 2: The metrics of the fitted linear regression model.\n\n\n\n\n\nMetric\nValue\n\n\n\n\nmean_squared_error\n1.819\n\n\nmean_absolute_error\n0.849941\n\n\nexplained_variance_score\n0.00510836\n\n\nr2_score\n0.00510836\n\n\n\n\n\n\n\nThe metrics of the fitted model can be seen in Table 2. Some explanation of the metrics and how to interpret them, along with a judgement of the model is provided below:\n\n\\(R^2\\) Score: Represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. A score of 0 means perfect fit. As we can see, it is quite low, meaning the model ON AVERAGE predicts close to the value, even though as we can see from the fitted line, the spread of points is quite large.\nmean absolute error: A risk metric corresponding to the expected value of the absolute error loss or l1-norm loss. Aka, the average difference between the true and predicted value. Given that the total scale of Stars is only 0-5, an error of 0.85 is more than one SD away, giving large error.\nmean square error: A risk metric corresponding to the expected value of the squared (quadratic) error or loss. Used especially when needing a derivable loss function. In this case, it’s quite large given that the total scale of Stars is only 0-5.\nexplained variance regression score: The difference between the explained variance score and the R² score, the coefficient of determination is that when the explained variance score does not account for systematic offset in the prediction. For this reason, the R² score, the coefficient of determination should be preferred in general.\n\n\n\n\n\n\nIn general, the linear regression model performed quite poorly; however, this was to be expected given that the input variables did not appear to have an easy linear relationship to model in the first place. Overall, given the nature of the relationships between the variables in this dataset, no linear relationship-based model would have performed well and other methods need to be sought out in order to make statistically significant predictions.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  },
  {
    "objectID": "models/regression.html#overview",
    "href": "models/regression.html#overview",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It’s a fundamental technique in machine learning and statistics, often used for predictive analysis and understanding the underlying patterns in data.\n\nAssumptions: Linear regression assumes a linear relationship between the independent variables (features) and the dependent variable (target). It also assumes that the errors or residuals (the differences between actual and predicted values) are normally distributed and have constant variance (homoscedasticity).\nModel Representation: The linear regression model is represented as: \\[\n  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon\n  \\] Where:\n\n\\(y\\) is the dependent variable.\n\\(x_1, x_2, ..., x_n\\) are the independent variables.\n\\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n\\) are the coefficients (parameters) that represent the relationship between the independent and dependent variables.\n\\(\\epsilon\\) is the error term.\n\n\nDuring the training process, the goal is to find the best values for the coefficients (weights) that minimize the difference between the actual and predicted values. This is often done using optimization algorithms like Ordinary Least Squares (OLS) or gradient descent. Once the model is trained, it can be used to make predictions on new data by plugging in the values of the independent variables into the model equation. Limitations of linear regression include:\n\nLinearity Assumption: Linear regression assumes a linear relationship between variables. If the true relationship is non-linear, linear regression may not capture it accurately.\nOutliers: Linear regression is sensitive to outliers, which are data points that deviate significantly from the rest of the data. Outliers can distort the model and lead to inaccurate predictions.\nMulticollinearity: When independent variables are highly correlated, it can cause multicollinearity issues in linear regression, leading to unstable coefficient estimates.\nOverfitting/Underfitting: Linear regression can suffer from overfitting (capturing noise in the training data) or underfitting (oversimplifying the relationship), especially when dealing with complex data patterns.\nAssumption Violations: If the assumptions of linear regression are violated (e.g., non-normality of residuals, heteroscedasticity), the model’s reliability and interpretability may be compromised.\n\nDespite these limitations, linear regression remains a valuable and widely used tool due to its simplicity, interpretability, and ability to provide insights into relationships between variables.\nIn this project, we specifically perform very simple linear regression by picking only one independent variable (Reviews) and the dependent variable (Stars).",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  },
  {
    "objectID": "models/regression.html#model-selection",
    "href": "models/regression.html#model-selection",
    "title": "Linear Regression",
    "section": "",
    "text": "We choose the independent variable to be Reviews and the dependent variable to be Stars, as they appear to have the most predictable linear relationship (see Figure 1); the variables List Price and Price are more linear but they also have too much multicollinearity and are not useful for prediction.\n\n\n\n\n\n\nFigure 1: Splom of all the numerical variables in the dataset.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  },
  {
    "objectID": "models/regression.html#data-prep",
    "href": "models/regression.html#data-prep",
    "title": "Linear Regression",
    "section": "",
    "text": "As there is only one independent variable, no tranformations/scaling was needed for the dataset other than selecting the subset of the columns. Table 1 shows the final input data used for all models used, and the data can be found at this link.\n\n\n\nTable 1: A sample of the data used for the linear regression modeling. Reviews is the independent variable and Stars is the dependent variable/prediction.\n\n\n\n\n\nReviews\nStars\n\n\n\n\n515\n4.2\n\n\n0\n3.7\n\n\n502\n4.3\n\n\n0\n4.4\n\n\n3364\n4.5\n\n\n1106\n3.8\n\n\n0\n4\n\n\n0\n4.8\n\n\n0\n4.6\n\n\n164\n4.4",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  },
  {
    "objectID": "models/regression.html#code",
    "href": "models/regression.html#code",
    "title": "Linear Regression",
    "section": "",
    "text": "The jupyter notebook code for running linear regression, data prep, and everything else on this page can be found here, or click on the link card to the right.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  },
  {
    "objectID": "models/regression.html#results",
    "href": "models/regression.html#results",
    "title": "Linear Regression",
    "section": "",
    "text": "The resulting fitted regression line formula is below, and can be seen in Figure 2.\n\\[\n\\textbf{y} = 0.0001696\\ \\textbf{x} + 3.9747\n\\]\n\ny: What it predicts, aka the average number of stars a product recieved.\nx: The input number of reviews a product recieved.\n\n\n\n\n\n\n\n\nFigure 2: The original data, with the fitted linear regression line plotted over.\n\n\n\n\n\n\n\n\n\n\nTable 2: The metrics of the fitted linear regression model.\n\n\n\n\n\nMetric\nValue\n\n\n\n\nmean_squared_error\n1.819\n\n\nmean_absolute_error\n0.849941\n\n\nexplained_variance_score\n0.00510836\n\n\nr2_score\n0.00510836\n\n\n\n\n\n\n\nThe metrics of the fitted model can be seen in Table 2. Some explanation of the metrics and how to interpret them, along with a judgement of the model is provided below:\n\n\\(R^2\\) Score: Represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. A score of 0 means perfect fit. As we can see, it is quite low, meaning the model ON AVERAGE predicts close to the value, even though as we can see from the fitted line, the spread of points is quite large.\nmean absolute error: A risk metric corresponding to the expected value of the absolute error loss or l1-norm loss. Aka, the average difference between the true and predicted value. Given that the total scale of Stars is only 0-5, an error of 0.85 is more than one SD away, giving large error.\nmean square error: A risk metric corresponding to the expected value of the squared (quadratic) error or loss. Used especially when needing a derivable loss function. In this case, it’s quite large given that the total scale of Stars is only 0-5.\nexplained variance regression score: The difference between the explained variance score and the R² score, the coefficient of determination is that when the explained variance score does not account for systematic offset in the prediction. For this reason, the R² score, the coefficient of determination should be preferred in general.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  },
  {
    "objectID": "models/regression.html#conclusions",
    "href": "models/regression.html#conclusions",
    "title": "Linear Regression",
    "section": "",
    "text": "In general, the linear regression model performed quite poorly; however, this was to be expected given that the input variables did not appear to have an easy linear relationship to model in the first place. Overall, given the nature of the relationships between the variables in this dataset, no linear relationship-based model would have performed well and other methods need to be sought out in order to make statistically significant predictions.",
    "crumbs": [
      "Code",
      "Models and Methods",
      "Linear Regression"
    ]
  }
]