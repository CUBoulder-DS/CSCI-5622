# Data Prep / EDA

Where the data source, processing, and visualization (EDA) is presented.

```{python}
#| include: false
#| label: setup

%matplotlib inline

import pandas as pd
from datetime import datetime
import numpy as np

from IPython.display import display, Latex, Markdown

import plotly.express as px
import plotly.io as pio
# Settings for plot rendering, makes work with HTML output + jupyer lab + static output
# pio.renderers.default = "notebook+plotly_mimetype+png"
pio.renderers.default = "plotly_mimetype+notebook_connected+png"
```


## Data Collection

Amazon product information was scraped from the website using the API service [ScraperAPI](https://www.scraperapi.com); this is because, as Amazon is a hugely popular website, they have many anti-scraping measures in place such as rate-limiting, IP blocking, dymamic loading, and such. Using the external API service, these limitations were able to be avoided. The search queries chosen to search for items were based on top 100 Amazon searches, found on [this site](https://www.semrush.com/blog/most-searched-items-amazon/) and [this site](https://ahrefs.com/blog/top-amazon-searches/). An example of using the API, along with its core endpoint, is below.

```{python}
#| echo: true
#| eval: false

import requests

payload = {
   'api_key': 'API_KEY',
   'query': 'iphone 15 charger',
   's': 'price-asc-rank'
}

response = requests.get('https://api.scraperapi.com/structured/amazon/search',
                        params=payload).json()
```

The jupyter notebook code for the web scraping can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/amazon%20scraping.ipynb).

Additionally, more data was used to supplement the existing data. Since the scraped data was only about 26K rows, a [Kaggle dataset](
https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products/data) was used that contains more than one million rows, had around the same fields as the scraped data, and was also from the USA (many Amazon Kaggle datasets were from the non-US).

The raw data from both sources can be seen below in @tbl-rawdata; the scraped raw data CSV can also be viewed [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/data/scraped/scraped1_raw.csv).

```{python}
#| label: tbl-rawdata
#| tbl-cap: The raw data from both datasets.
#| tbl-subcap:
#|   - The raw data scraped from Amazon using ScraperAPI
#|   - The raw data gotten from Kaggle


df_scraped = pd.read_csv("../data/scraped/scraped1_raw.csv", index_col=0)
display(df_scraped.sample(frac=1).reset_index(drop=True).head())

df_kaggle = pd.read_csv("../data/kaggle_asaniczka/amazon_products.csv")
display(df_kaggle.sample(frac=1).reset_index(drop=True).head())
```


## Data Cleaning

The datasets were cleaned seperately, then concatenated, then some final steps were taken to clean it.

The steps to clean the web-scaped data were:

* Add `date_scraped` column
* Remove unecessary columns: `type`, `position`, `has_prime`, `is_amazon_choice`, `is_limited_deal`, `availability_quantity`, `spec`, `price_string`, `price_symbol`, `section_name`
* Expand and fix `original_price`
* Rename columns to match standard snake case for merging both datasets
* Drop rows with no asin or name or price
* Drop rows with price of 0.0, since that doesn't make sense
* Fill NaN `reviews` column with 0

The steps to clean the Kaggle data were:

* Add `date_scraped` column
* Drop rows with any NaNs
* Fix `list_price` of $0 to be instead equal to `price`
* Change `category_id` to actual category by using `category` table
* Drop rows with price of $0, since that doesn't make sense
* Rename columns to match standard snake case for merging both datasets

And then, after they were concatenated, the steps to clean were:

* Remove duplicates (by asin + date scraped)
* Rename columns

The final cleaned (and concatenated) dataset can be seen in @tbl-finaldata (with the original raw data in @tbl-rawdata):

```{python}
#| label: tbl-finaldata
#| tbl-cap: The final unioned, cleaned, and processed data.

df_az = pd.read_csv("../data/products_cleaned.csv")

df_az["Category"] = df_az["Category"].astype("category")
df_az["Date Scraped"] = df_az["Date Scraped"].astype("datetime64[ns]")

display(df_az.sample(frac=1).reset_index(drop=True).head())
```

The code for the data cleaning can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/data%20cleaning%20EDA.ipynb).


## Data Preprocessing / Visualization

Various types of EDA were performed in order to examine the data; as a note, most visuals are interactive (zoomable, pannable, etc). The code for all visualizations can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/data%20cleaning%20EDA.ipynb).

::: callout-tip
## Important

If the interactive figures don't load, dont worry: just turn off all ad-blockers/privacy browsing, make sure you are using Chrome/Firefox, and refresh the page until all figures load.
:::

@fig-cathist

```{python}
#| label: fig-cathist
#| fig-cap: A histogram of all categories of all Amazon products. Note scraped data did not have categories, but the Kaggle data did.

def layout(fig):
    fig.update_layout(paper_bgcolor="rgba(0, 0, 0, 0)", font_color="lightgreen")

fig = px.histogram(df_az, x="Category", title="Histogram of Product Categories")
layout(fig)
fig.show()
```

@fig-histstars

```{python}
#| label: fig-histstars
#| fig-cap: Histogram of the number of stars recieved for all products.

fig = px.histogram(df_az, x="Stars")
layout(fig)
fig.show()
```

@fig-violins

::: {#fig-violins layout-ncol=2}

![Violin plot of the number of a product bought in a month](images/violin_bim.png)

![Violin plot of price of a product](images/violin_price.png)

![Violin plot of the number of reviews](images/violin_reviews.png)

Violin plots (with overlayed box plot and outliers) of 3 of the main numerical columns.
:::


@fig-starreview

```{python}
#| label: fig-starreview
#| fig-cap: Stars vs number of reviews recieved by an amazon product, colored by whether the product was a best-seller.

fig = px.scatter(df_az, x="Reviews", y="Stars", color="Is Best Seller", opacity=0.3,
                 title="Stars vs Number of Reviews")
layout(fig)
fig.show(renderer="png")
```


@fig-wordclouds

::: {#fig-wordclouds layout-ncol=2}

![Wordcloud for categories of products](images/wordcloud_cats.png)

![Wordcloud for the names of products](images/wordcloud_names.png)

Wordclouds (where more frequent appearing words are bigger) of the categories of products and the names of products.
:::

@fig-prstars

```{python}
#| label: fig-prstars
#| fig-cap: The number of stars a product recieved, vs the price in log scale, colored by whether the product was a best seller.

fig = px.scatter(df_az, x="Stars", y="Price", color="Is Best Seller", opacity=0.3, log_y=True,
                    title="Number of Stars vs Price")
layout(fig)
fig.show(renderer="png")
```

### Investigation of products in both sets of data

The following plots focused on products that had the same Asin (identifying ID), that were in both the data personally scraped in 2024 and the Kaggle data from 2023. Investigating this subset of data could reveal a lot about how product prices, reviews, and other factors changed over time, as the only way to get time difference data was to use products that actually had more than one timepoint.


@fig-dupescatter


```{python}
#| label: fig-dupescatter
#| fig-cap: Price vs list price of items with the same ASIN across dates scraped, with a trendline.

df_dupes = df_az[df_az["Asin"].duplicated(keep=False)].sort_values(["Asin", "Date Scraped"]).reset_index()

fig = px.scatter(df_dupes,
                 x='Price',
                 y='List Price',
                 color="Date Scraped",
                 trendline="ols",
                 title="Price vs List Price of Items with Dupe Asin Across Dates Scraped",
                )
fig.update_layout(height=500)
layout(fig)
fig.show()
```

Given that we can see outliers in price affecting the plot of the graph, it was decided for analysis to only consider those prices most populous, aka prices less than $800.

@fig-dupehist

```{python}
#| label: fig-dupehist
#| fig-cap: Histogram of prices, colored by whether the change in price increased or decreased over time, for those items that were in both sets of data.

diffs = df_dupes[["Asin", "Date Scraped", "Price", "List Price"]].set_index("Asin").groupby(level=0).diff().reset_index()
df_dupes["Price Diff"] = diffs["Price"]
df_dupes["List Price Diff"] = diffs["List Price"]
df_dupes["Increased"] = df_dupes["Price Diff"] > 0
df_dupes["Category"] = df_dupes["Category"].ffill()

df_diffs = df_dupes[~df_dupes["Price Diff"].isna()][df_dupes["Price"] < 800]

fig = px.histogram(df_diffs, x="Price", color="Increased", marginal="violin",
                    title="Histogram of Prices")
layout(fig)
fig.show()
```

@fig-dupediff

```{python}
#| label: fig-dupediff
#| fig-cap: Price vs the difference in price, over the two sets of data, colored by whether the price diff increased or decreased.

fig = px.scatter(df_diffs, x="Price", y="Price Diff", color="Increased", hover_data=["Name", "Asin"], opacity=0.4,
                title="Price vs Price Difference")
layout(fig)
fig.show()
```


Only categories with more than 20 products were included, to reduce noise and to focus on meaningful categories. @fig-dupecats

```{python}
#| label: fig-dupecats
#| fig-cap: For the included categories, a bar chat showing the ratio of products that had their price increase over time vs decrease.

df_diffcats = df_diffs[df_diffs["Category"].isin(df_diffs["Category"].value_counts()[df_diffs["Category"].value_counts() > 20].index)]
df_catcounts = (df_diffcats.groupby(["Category", "Increased"])["index"].sum() / df_diffcats.groupby("Category")["index"].sum()).reset_index()
df_catcounts = df_catcounts.dropna()

fig = px.bar(df_catcounts, y="Category", x="index", color="Increased")
fig.update_layout(height=800, xaxis_title="Ratio")
layout(fig)
fig.show()
```
