# Data Prep / EDA

Where the data source, processing, and visualization (EDA) is presented.

```{python}
#| include: false
#| label: setup

%matplotlib inline

import pandas as pd
from datetime import datetime
import numpy as np
import os

from IPython.display import display, Latex, Markdown

import plotly
import plotly.express as px
import plotly.io as pio
import plotly.graph_objects as go

import sys
module_path = os.path.abspath("models")
if module_path not in sys.path:
    sys.path.append(module_path)

# Must be at end to override
from common_funcs import *
```


## Data Collection

Amazon product information was scraped from the website using the API service [ScraperAPI](https://www.scraperapi.com); this is because, as Amazon is a hugely popular website, they have many anti-scraping measures in place such as rate-limiting, IP blocking, dymamic loading, and such. Using the external API service, these limitations were able to be avoided. The search queries chosen to search for items were based on top 100 Amazon searches, found on [this site](https://www.semrush.com/blog/most-searched-items-amazon/) and [this site](https://ahrefs.com/blog/top-amazon-searches/). An example of using the API, along with its core endpoint, is below.

```{python}
#| echo: true
#| eval: false

import requests

payload = {
   'api_key': 'API_KEY',
   'query': 'iphone 15 charger',
   's': 'price-asc-rank'
}

response = requests.get('https://api.scraperapi.com/structured/amazon/search',
                        params=payload).json()
```

The jupyter notebook code for the web scraping can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/amazon%20scraping.ipynb).

Additionally, more data was used to supplement the existing data. Since the scraped data was only about 26K rows, a [Kaggle dataset](
https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products/data) was used that contains more than one million rows, had around the same fields as the scraped data, and was also from the USA (many Amazon Kaggle datasets were from the non-US).

The raw data from both sources can be seen below in @tbl-rawdata; the scraped raw data CSV can also be viewed [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/data/scraped/scraped1_raw.csv).

```{python}
#| label: tbl-rawdata
#| tbl-cap: The raw data from both datasets.
#| tbl-subcap:
#|   - The raw data scraped from Amazon using ScraperAPI
#|   - The raw data gotten from Kaggle


df_scraped = pd.read_csv(os.path.normpath("../data/scraped/scraped1_raw.csv"), index_col=0)
display(df_scraped.sample(frac=1).reset_index(drop=True).head())

df_kaggle = pd.read_csv(os.path.normpath("../data/kaggle_asaniczka/amazon_products.csv"))
display(df_kaggle.sample(frac=1).reset_index(drop=True).head())
```


## Data Cleaning

The datasets were cleaned seperately, then concatenated, then some final steps were taken to clean it.

The steps to clean the web-scaped data were:

* Add `date_scraped` column
* Remove unecessary columns: `type`, `position`, `has_prime`, `is_amazon_choice`, `is_limited_deal`, `availability_quantity`, `spec`, `price_string`, `price_symbol`, `section_name`
* Expand and fix `original_price`
* Rename columns to match standard snake case for merging both datasets
* Drop rows with no asin or name or price
* Drop rows with price of 0.0, since that doesn't make sense
* Fill NaN `reviews` column with 0

The steps to clean the Kaggle data were:

* Add `date_scraped` column
* Drop rows with any NaNs
* Fix `list_price` of $0 to be instead equal to `price`
* Change `category_id` to actual category by using `category` table
* Drop rows with price of $0, since that doesn't make sense
* Rename columns to match standard snake case for merging both datasets

And then, after they were concatenated, the steps to clean were:

* Remove duplicates (by asin + date scraped)
* Rename columns

The final cleaned (and concatenated) dataset can be seen in @tbl-finaldata (with the original raw data in @tbl-rawdata):

```{python}
#| label: tbl-finaldata
#| tbl-cap: The final unioned, cleaned, and processed data.

df_az = pd.read_csv(os.path.normpath("../data/products_cleaned.csv"))

df_az["Category"] = df_az["Category"].astype("category")
df_az["Date Scraped"] = df_az["Date Scraped"].astype("datetime64[ns]")

display(df_az.sample(frac=1).reset_index(drop=True).head())
```

The code for the data cleaning can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/data%20cleaning%20EDA.ipynb).


## Data Preprocessing / Visualization

Various types of EDA were performed in order to examine the data; as a note, most visuals are interactive (zoomable, pannable, etc). The code for all visualizations can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/data%20cleaning%20EDA.ipynb).

::: callout-tip
## Important

If the interactive figures don't load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website) until all figures load.
:::

First, a histogram of all categories of all Amazon products is shown in  @fig-cathist. Note scraped data did not have categories, but the Kaggle data did. It can be seen that from the figure, girl's and boy's clothing are the most populous categories, with toys & games the next most populous.

```{python}
#| label: fig-cathist
#| fig-cap: A histogram of all categories of all Amazon products. Note scraped data did not have categories, but the Kaggle data did.

def layout(fig):
    fig.update_layout(paper_bgcolor="rgba(0, 0, 0, 0)", font_color="lightgreen")

fig = px.histogram(df_az, x="Category", title="Histogram of Product Categories")
layout(fig)
fig.show()
```

@fig-histstars shows a histogram of the stars recieved for a product. It can be seen that the number of stars increases generally exponentially until 4.6 stars, with peaks at 0 and 5 stars as well. This data generally agrees with what can be seen on Amazon when browsing personally, and the peaks at 0 and 5 agree with having low number of reviews.

```{python}
#| label: fig-histstars
#| fig-cap: Histogram of the number of stars recieved for all products.

fig = px.histogram(df_az, x="Stars")
layout(fig)
fig.show()
```

@fig-violins shows violin plots of the numerical factors: volume of product bought in the month, the price of a product, and the number of reviews. A violin plot was chosen because a histogram would be visually uninformative, due to the amount of outliers. Some insights that can be drawn from these violin plots are:

* These factors have a good number of outliers that are extremely large in value compared to the general distribution of values. This does not seem to be from incorrect values, but from sellers attempting to "game the system".
* "Bought in Month" has outliers that are regularly spaced, indicating either a form of gaming the system by sellers that tend to follow specific formats, or Amazon only reports volume of bought products at low granularity for high-volume products.
* Price tends to have outliers that continue regularly up the price scale, except for a couple at the $60-70k range. It is unclear whether to drop those rows containing those prices as they seem valid; it is entirely possible that sellers just list the product at a completely unreasonable price as a placeholder, and don't expect anyone to purchase at that price.
* The number of reviews for a product have been shown to be easily gameable with fake reviews, and is likely what is happening here.

::: {#fig-violins layout-ncol=2}

![Violin plot of the number of a product bought in a month](images/violin_bim.png)

![Violin plot of price of a product](images/violin_price.png)

![Violin plot of the number of reviews](images/violin_reviews.png)

Violin plots (with overlayed box plot and outliers) of 3 of the main numerical columns.
:::


The amount of stars, vs the number of reviews for a product can be seen in @fig-starreview, colored with the `Is Best Seller` flag. It can be seen that the distribution of `Is Best Seller` items tends to follow the overall distribution; additionally, the number of reviews tends to increase as the number of stars increases, with the products with the outlier number of reviews (over 200k) all above 4 stars.

```{python}
#| label: fig-starreview
#| fig-cap: Stars vs number of reviews recieved by an amazon product, colored by whether the product was a best-seller.

fig = px.scatter(df_az, x="Reviews", y="Stars", color="Is Best Seller", opacity=0.3,
                 title="Stars vs Number of Reviews")
layout(fig)
fig.show(renderer="png")
```


In order to visualize the content of the text factors, namely the `Category` and `Name` of a product, a wordcloud format was used; wordclouds visualize the words that tend to appear in the text with the more frequent words appearing in larger text. In @fig-wordclouds, it can be seen that in the categories, "Care", "Products", "Boy's Clothing", "Consoles", "Accessories", and "Games" appear most. In the names of procucts, the words "Stainles Steel", "Compatible", "Heavy Duty", "Boy", "Girl", and "Birthday" appear the most.

::: {#fig-wordclouds layout-ncol=2}

![Wordcloud for categories of products](images/wordcloud_cats.png)

![Wordcloud for the names of products](images/wordcloud_names.png)

Wordclouds (where more frequent appearing words are bigger) of the categories of products and the names of products.
:::

The number of stars vs price, with price being on a log scale (to hangle outliers) is shown in @fig-prstars, along with being colored by if it's a best seller. As was seen in @fig-starreview, the distribution of `Is Best Seller` items tends to follow the overall distribution; additionally, it can be seen that there are products with a price up to $10k across all number of stars.

```{python}
#| label: fig-prstars
#| fig-cap: The number of stars a product recieved, vs the price in log scale, colored by whether the product was a best seller.

fig = px.scatter(df_az, x="Stars", y="Price", color="Is Best Seller", opacity=0.3, log_y=True,
                    title="Number of Stars vs Price")
layout(fig)
fig.show(renderer="png")
```

### Investigation of products in both sets of data

The following plots focused on products that had the same Asin (identifying ID), that were in both the data personally scraped in 2024 and the Kaggle data from 2023. Investigating this subset of data could reveal a lot about how product prices, reviews, and other factors changed over time, as the only way to get time difference data was to use products that actually had more than one timepoint.


@fig-dupescatter shows the price vs list price of items, colored by what date they were scraped on, with a trendline plotted. It can be seen that on the whole, list price and actual price match and that prices seem to have increased since 2023 to 2024; however, below $50 price (when zooming in) the trendlines cross, implying that lower-priced products actually decreased in price.


```{python}
#| label: fig-dupescatter
#| fig-cap: Price vs list price of items with the same ASIN across dates scraped, with a trendline.

df_dupes = df_az[df_az["Asin"].duplicated(keep=False)].sort_values(["Asin", "Date Scraped"]).reset_index()

fig = px.scatter(df_dupes,
                 x='Price',
                 y='List Price',
                 color="Date Scraped",
                 trendline="ols",
                 title="Price vs List Price of Items with Dupe Asin Across Dates Scraped",
                )
fig.update_layout(height=500)
layout(fig)
fig.show()
```

Given that outliers can be seen in price affecting the plot of the graph, it was decided for analysis to only consider those prices most populous, aka prices less than $800. Given that, the prices of each Asin were plotted, along with being colored by if the price increased from 2023 to 2024, or decreased. It can be seen that in @fig-dupehist that indeed for lower priced products (less than $50) the prices mostly did not increase, and instead decreased or stayed the same. The histogram shows the frequency of higher-priced procucts generally exponentially decreasing, and the likelihood of the price increasing going up.

```{python}
#| label: fig-dupehist
#| fig-cap: Histogram of prices, colored by whether the change in price increased or decreased over time, for those items that were in both sets of data.

diffs = df_dupes[["Asin", "Date Scraped", "Price", "List Price"]].set_index("Asin").groupby(level=0).diff().reset_index()
df_dupes["Price Diff"] = diffs["Price"]
df_dupes["List Price Diff"] = diffs["List Price"]
df_dupes["Increased"] = df_dupes["Price Diff"] > 0
df_dupes["Category"] = df_dupes["Category"].ffill()

df_diffs = df_dupes[~df_dupes["Price Diff"].isna()][df_dupes["Price"] < 800]

fig = px.histogram(df_diffs, x="Price", color="Increased", marginal="violin",
                    title="Histogram of Prices")
layout(fig)
fig.show()
```

@fig-dupediff shows a more granular form of the previous plot by plotting price vs the actual price difference amount from 2023 to 2024, with the ability to investigate which products are having such a dramatic increase or decrease in price over time. It can be seen that tech and suitcases were the outliers in price increase, and tech also being the outliers in price decrease.

```{python}
#| label: fig-dupediff
#| fig-cap: Price vs the difference in price, over the two sets of data, colored by whether the price diff increased or decreased.

fig = px.scatter(df_diffs, x="Price", y="Price Diff", color="Increased", hover_data=["Name", "Asin"], opacity=0.4,
                title="Price vs Price Difference")
layout(fig)
fig.show()
```


Finally, it was investigated as to which product categories exactly had a price increase or decrease, and the ratio of number of increases to decreases. Only categories with more than 20 products were included, to reduce noise and to focus on meaningful categories. In @fig-dupecats, it can be seen that "Sports and Outdoors", "Makeup", "Electrical Equipment", and "Building Toys" all had more price increases than decreases; every other product category had more price decreases than increases, with "Smart Homes" and "Gift Cards" having the most price decreases.

```{python}
#| label: fig-dupecats
#| fig-cap: For the included categories, a bar chat showing the ratio of products that had their price increase over time vs decrease.

df_diffcats = df_diffs[df_diffs["Category"].isin(df_diffs["Category"].value_counts()[df_diffs["Category"].value_counts() > 20].index)]
df_catcounts = (df_diffcats.groupby(["Category", "Increased"])["index"].sum() / df_diffcats.groupby("Category")["index"].sum()).reset_index()
df_catcounts = df_catcounts.dropna()

fig = px.bar(df_catcounts, y="Category", x="index", color="Increased")
fig.update_layout(height=800, xaxis_title="Ratio", xaxis_range=[0, 1])
layout(fig)
fig.show()
```
