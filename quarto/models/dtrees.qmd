---
lightbox: true
---

# Decision Trees

## Overview

::: {style="width:100%;"}
::: {style="float:right; width:34%; padding:2%"}
![Decision tree structure](../images/dt_1.jpg)
:::
:::

Decision trees are a popular supervised learning algorithm used for both classification and regression tasks in machine learning. They are tree-like structures where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (for classification) or a numerical value (for regression). In classification, they can classify instances by learning decision rules inferred from the features. With regression, they can predict continuous values by averaging the target values of instances falling into the same leaf. When it comes to determining the "goodness" of a split in the decision tree and updating it, several methods are employed. Gini Impurity measures the impurity of a node, where a node is pure (GINI = 0) if all instances below a node belong to the same class. Entropy measures the randomness or uncertainty of a node; it is maximum (1) when all classes are equally distributed and decreases as the node becomes more pure. Information Gain is used to determine the best split in a decision tree. It calculates the difference in entropy (or GINI impurity) before and after the split. The split with the highest information gain is chosen.

::: {style="width:100%;"}
::: {style="float:left; width:34%; padding:2%"}
![The decision tree structure for the given example](../images/dt_2.png)
:::
:::

For example, consider a dataset with weather attributes (including the feature "Outlook") and a target class 'Play' (Yes/No). Suppose we want to split based on the 'Outlook' attribute. We calculate the entropy for the current node (before split), then we calculate the entropy for each possible branch ('Sunny', 'Overcast', 'Rainy'). We then use Information Gain to determine the best split.

It's generally possible to create an infinite number of decision trees because:

- There can be different orderings of features for splitting.
- For continuous features, there can be infinitely many possible split points.
- Tree depth and complexity can vary widely, leading to numerous possible structures.

To manage this complexity and avoid overfitting, techniques like pruning, limiting tree depth, and using ensemble methods (like Random Forests) are employed. These strategies help create more generalizable and robust decision trees.

In this particular project, classification decision trees will be used to predict whether a product is a best seller or not; regression decision trees will be used to predict the price of a product. All numerical, boolean, and categorical features of the dataset will be used in the prediction (but not the unique string features like "Name" and "ASIN").

## Data Prep

The data features used were:

- `"Stars", "Reviews", "Price", "Date Scraped", "Bought In Month"` for predicting `"Is Best Seller"`
- `"Stars", "Reviews", "Date Scraped", "Bought In Month", "Is Best Seller"` for predicting `"Price"`.

The data is already clean with outliers removed and NaNs dealt with, but some more data engineering had to take place to make the features fit for running the models:

- Only select rows that contain a non-NaN value for `Category` and `Bought in Month`
- Select rows such that 1% of them are `Is Best Seller == True`. This is because the dataset contains more than >1M rows, with only 0.4% of them `Is Best Seller == True`; this makes the prediction less biased and invariant for the model.
- Make datetime binary, as there are only 2 values used for it anyways.

As DTs are invariant to the scale of the data, no normalization/min-max scaling was needed.

The data was also split into 25% test data, and 75% training data. It's essential for the training and test data to be disjoint, meaning they should not overlap, to ensure the validity and accuracy of the model's performance evaluation. Reasons for this include to avoid overfitting, assessing bias and variance, supporting reproducability and validation, and preventing data leakage.

@tbl-data shows the final input data used for all models used, and the data can be found [at this link](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/data/scraped/scraped1.csv). The test data is exactly the same format as the pictured samples of training data, except with 75% less rows.

::: {#tbl-data layout-ncol=2}
|   Stars |   Reviews |   Price |   Date Scraped |   Bought In Month |
|--------:|----------:|--------:|---------------:|------------------:|
|     4.3 |         0 |   45.99 |              0 |                 0 |
|     4.2 |         0 |   22.99 |              0 |               100 |
|     4.9 |         0 |  345    |              0 |                 0 |
|     5   |         0 |   28.49 |              1 |                 0 |
|     4.6 |       146 |   23.55 |              1 |                 0 |
|     4.7 |         0 |   71.13 |              0 |                 0 |

: The training features for the classification DT

|   Is Best Seller |
|-----------------:|
|                0 |
|                0 |
|                0 |
|                0 |
|                1 |
|                0 |

: The label for the classification DT

|   Stars |   Reviews |   Date Scraped |   Bought In Month | Is Best Seller   |
|--------:|----------:|---------------:|------------------:|:-----------------|
|     3.5 |         0 |              0 |                 0 | False            |
|     4.7 |      3874 |              0 |                50 | False            |
|     0   |         0 |              0 |                50 | False            |
|     4.6 |         0 |              0 |                50 | False            |
|     5   |         0 |              0 |               700 | False            |
|     4.4 |        37 |              0 |                 0 | False            |

: The training features for the regression DT

|   Price |
|--------:|
|   24.99 |
|   16.99 |
|   33.99 |
|   31.99 |
|    9.99 |
|   20.38 |

: The label for the regression DT

The input training and test data for the classifying and regression decision trees.
:::

## Models Used

As described above, a classification DT and a regression DT were run on their respective subsets of the dataset, using the standard defaults given by `sklearn`. In addition, a third DT was run: using random forests for classification (using the same dataset as for the standard classification DT). Random forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. Random decision forests correct for decision trees' habit of overfitting to their training set.

## Code

::: {.columns}
::: {.column}
The jupyter notebook code for running decision trees, data prep, and everything else on this page can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/Bayes%20DT.ipynb), or click on the link card to the right.
:::

::: {.column width=10%}
<!-- Spacing column -->
:::

::: {.column width=30%}
[![](https://github-link-card.s3.ap-northeast-1.amazonaws.com/CUBoulder-DS/CSCI-5622.png)](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/Bayes%20DT.ipynb)
:::
:::

## Results

The models were run, and the standard metrics for evaluating classification (including accuracy) and regression (including R^2 score) are shown in @tbl-metrics. As we can see, the random forest classfier (described from here on as "best classifier DT") was the better performing model slightly; we can also see that the regression DT did not perform well.

::: {#tbl-metrics layout-ncol=2}

| Metric                    |       Value |
|:--------------------------|------------:|
| Mean Squared Error (MSE)  | 3996.33     |
| Mean Absolute Error (MAE) |  32.5754  |
| R^2                       |  -0.030492    |

: The performance of the regression DT.

| Model                |      |   precision |      recall |    f1-score |   support |          |
|:-------------------------|:-------------|------------:|------------:|------------:|----------:|-----------:|
| Standard Classifier DT   | False        |   0.990381  |   0.995355  |   0.992862  |    166850 |         |
|    | True         |   0.0673887 |   0.033553  |   0.0448    |      1669 |         |
|    | accuracy     |          |          |          |        |   0.985829 |
|    | macro avg    |   0.528885  |   0.514454  |   0.518831  |    168519 |         |
|    | weighted avg |   0.98124   |   0.985829  |   0.983472  |    168519 |         |
| Random Forest Classifier | False        |   0.990312  |   0.997393  |   0.99384   |    166850 |         |
|  | True         |   0.0861345 |   0.0245656 |   0.0382284 |      1669 |         |
|  | accuracy     |          |          |          |        |   0.987758 |
|  | macro avg    |   0.538223  |   0.510979  |   0.516034  |    168519 |        |
|  | weighted avg |   0.981357  |   0.987758  |   0.984376  |    168519 |        |

: The performance of the 2 classification DT models.


Standard metrics showing the performance of the 3 models.
:::

We also show the structure of the decision trees at the first 3 levels (all trees had more than 50 final levels which was unable to be visualized). In @fig-dts, we can see that `Bought in Month` feature was very important to all 3 trees; we can also see that `Stars` and `Price` has a very low Gini index for both classifier DTs with `Stars` also appearing early in the tree, indicating the features' importances to the predictions.

::: {#fig-dts layout-ncol=3}

![Random Forest Classifier](../images/dt_tree_rf.png)

![Standard Classifier](../images/dt_tree_class.png)

![Regression DT](../images/dt_tree_reg.png)

A visualization of the first 3 levels of the 3 decision tree models run.
:::

### Regression Plots

We show the basic plot of true vs predicted label (aka `Price`) for the regression decision tree in @fig-reg1; we can see that unlike the straight line of scatter points we would see in a perfect classification, we have an approximate log-negative trend. The model tended to overprice items with actual price < $100, and underprice the products with high prices.

We also show the predicted price vs the error residuals in @fig-reg2; this plot gives similar insight to the previous, where we can see highest error on low predicted prices and less, but still increasing, error on higher-predicted prices.

::: {.columns}
::: {.column}
![True vs predicted label for the regression DT.](../images/dt_reg1.png){#fig-reg1}
:::
::: {.column}
![Predicted price vs the residuals for the regression DT.](../images/dt_reg2.png){#fig-reg2}
:::
:::

### Classifier Plot

::: {.columns}
::: {.column}
For the best performing classifier DT, we show the confusion matrix of the labels in @fig-cm. As we can see, The model did great on correctly classifying not best-sellers; however, that was because there were very few true best-sellers (this agrees with the F1 scores per-category in the above table). The model struggled with correctly classifying true best sellers and had many more FP and FN than TP.
:::

::: {.column}
![The confusion matrix of true vs predicted labels of the random forest classifier.](../images/dt_cm.png){#fig-cm}
:::
:::





## Conclusion

For the classification task, some conclusions and observations can be drawn from the results:

- The accuracy is deceptively high for both classifiers, as the second `True` label only was present in 1% of the data.
- The classifiers were biased towards predicting that a product was NOT a best-seller, as that gave the highest accuracy ultimately.
- Although the model had more FP than TP, overall it had many more FN than FP, indicating once again that the model tended to classify products as not best-sellers.

For the regression task, similarly we can draw observations from the results:

- Though the MAE was low, the low R^2 score indicates that the model overall tended to not predict the price well.
- Based on the residuals plots, the model did best (most residuals near zero) when the predicted price was around $150.
- The model tended to underprice and overprice when further from the $150 price value.

Overall, it seemed the models struggled to perform well but did adequately, especially the regression model which was mildly suprising. It can be concluded that the best-seller status of a product, along with price, is somewhat hard to predict from just the numerical values present in the dataset.
