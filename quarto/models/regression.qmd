---
lightbox: true
---

# Linear Regression

## Overview

::: {style="width:100%;"}
::: {style="float:right; width:34%; padding:2%"}
![](../images/lr.png)
:::
:::

Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It's a fundamental technique in machine learning and statistics, often used for predictive analysis and understanding the underlying patterns in data.

- **Assumptions**: Linear regression assumes a linear relationship between the independent variables (features) and the dependent variable (target). It also assumes that the errors or residuals (the differences between actual and predicted values) are normally distributed and have constant variance (homoscedasticity).

- **Model Representation**: The linear regression model is represented as:
    $$
    y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon
    $$
    Where:
  - $y$ is the dependent variable.
  - $x_1, x_2, ..., x_n$ are the independent variables.
  - $\beta_0, \beta_1, \beta_2, ..., \beta_n$ are the coefficients (parameters) that represent the relationship between the independent and dependent variables.
  - $\epsilon$ is the error term.

During the training process, the goal is to find the best values for the coefficients (weights) that minimize the difference between the actual and predicted values. This is often done using optimization algorithms like Ordinary Least Squares (OLS) or gradient descent. Once the model is trained, it can be used to make predictions on new data by plugging in the values of the independent variables into the model equation. Limitations of linear regression include:

1. **Linearity Assumption**: Linear regression assumes a linear relationship between variables. If the true relationship is non-linear, linear regression may not capture it accurately.

2. **Outliers**: Linear regression is sensitive to outliers, which are data points that deviate significantly from the rest of the data. Outliers can distort the model and lead to inaccurate predictions.

3. **Multicollinearity**: When independent variables are highly correlated, it can cause multicollinearity issues in linear regression, leading to unstable coefficient estimates.

4. **Overfitting/Underfitting**: Linear regression can suffer from overfitting (capturing noise in the training data) or underfitting (oversimplifying the relationship), especially when dealing with complex data patterns.

5. **Assumption Violations**: If the assumptions of linear regression are violated (e.g., non-normality of residuals, heteroscedasticity), the model's reliability and interpretability may be compromised.

Despite these limitations, linear regression remains a valuable and widely used tool due to its simplicity, interpretability, and ability to provide insights into relationships between variables.

In this project, we specifically perform very simple linear regression by picking only one independent variable (`Reviews`) and the dependent variable (`Stars`).


## Model Selection

We choose the independent variable to be `Reviews` and the dependent variable to be `Stars`, as they appear to have the most predictable linear relationship (see @fig-splom); the variables `List Price` and `Price` are more linear but they also have too much multicollinearity and are not useful for prediction.

![Splom of all the numerical variables in the dataset.](../images/lr_splom.png){#fig-splom}


## Data Prep

As there is only one independent variable, no tranformations/scaling was needed for the dataset other than selecting the subset of the columns. @tbl-data shows the final input data used for all models used, and the data can be found [at this link](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/data/scraped/scraped1.csv).


|   Reviews |   Stars |
|----------:|--------:|
|       515 |     4.2 |
|         0 |     3.7 |
|       502 |     4.3 |
|         0 |     4.4 |
|      3364 |     4.5 |
|      1106 |     3.8 |
|         0 |     4   |
|         0 |     4.8 |
|         0 |     4.6 |
|       164 |     4.4 |

: A sample of the data used for the linear regression modeling. `Reviews` is the independent variable and `Stars` is the dependent variable/prediction. {#tbl-data}


## Code

::: {.columns}
::: {.column}
The jupyter notebook code for running linear regression, data prep, and everything else on this page can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/NN%20LM.ipynb), or click on the link card to the right.
:::

::: {.column width=10%}
<!-- Spacing column -->
:::

::: {.column width=30%}
[![](https://github-link-card.s3.ap-northeast-1.amazonaws.com/CUBoulder-DS/CSCI-5622.png)](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/NN%20LM.ipynb)
:::
:::

## Results

::: {.columns}
::: {.column}
The resulting fitted regression line formula is below, and can be seen in @fig-lrplot.

$$
\textbf{y} = 0.0001696\ \textbf{x} + 3.9747
$$

- y: What it predicts, aka the average number of stars a product recieved.
- x: The input number of reviews a product recieved.

:::
::: {.column}

![The original data, with the fitted linear regression line plotted over.](../images/lr_plot.png){#fig-lrplot}
:::
:::


::: {.columns}
::: {.column}

| Metric                   |      Value |
|:-------------------------|-----------:|
| mean_squared_error       | 1.819      |
| mean_absolute_error      | 0.849941   |
| explained_variance_score | 0.00510836 |
| r2_score                 | 0.00510836 |

: The metrics of the fitted linear regression model. {#tbl-metrics}

:::
::: {.column}
The metrics of the fitted model can be seen in @tbl-metrics. Some explanation of the metrics and how to interpret them, along with a judgement of the model is provided below:

- $R^2$ Score: Represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. A score of 0 means perfect fit. As we can see, it is quite low, meaning the model ON AVERAGE predicts close to the value, even though as we can see from the fitted line, the spread of points is quite large.
- mean absolute error: A risk metric corresponding to the expected value of the absolute error loss or l1-norm loss. Aka, the average difference between the true and predicted value. Given that the total scale of `Stars` is only 0-5, an error of 0.85 is more than one SD away, giving large error.
- mean square error: A risk metric corresponding to the expected value of the squared (quadratic) error or loss. Used especially when needing a derivable loss function. In this case, it's quite large given that the total scale of `Stars` is only 0-5.
-  explained variance regression score: The difference between the explained variance score and the R² score, the coefficient of determination is that when the explained variance score does not account for systematic offset in the prediction. For this reason, the R² score, the coefficient of determination should be preferred in general.
:::
:::


## Conclusions

In general, the linear regression model performed quite poorly; however, this was to be expected given that the input variables did not appear to have an easy linear relationship to model in the first place. Overall, given the nature of the relationships between the variables in this dataset, no linear relationship-based model would have performed well and other methods need to be sought out in order to make statistically significant predictions.
