# PCA and SVD

```{python}
#| include: false
#| label: setup

# In order to force reload any changes done to the models package files
%load_ext autoreload
%autoreload 2

import pandas as pd
import numpy as np
import os

from IPython.display import display

import plotly
import plotly.express as px
import plotly.io as pio
import plotly.graph_objects as go

# Has to be after all other imports
from common_funcs import *
from common_funcs import df_show_rand_sample
```

## Overview

::: {style="width:100%;"}
::: {style="float:right; width:65%; padding:2%" layout-ncol=2}
![An example 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components generated from it, respectively.](../images/pca_ex1.jpeg)

![Scatterplot after PCA reduced from 3-dimensions to 2-dimensions.](../images/pca_ex2.jpeg)
:::
:::

Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and ML. At its core, PCA aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. This is achieved by identifying the directions of maximum variance in the original data, known as principal components. These principal components are the eigenvectors of the covariance matrix of the data, with their corresponding eigenvalues equaling the amount of variance explained by each principal component (PC). By retaining the PCs that capture the majority of the variance, PCA enables dimensionality reduction without significant loss of information.

Reducing the dimensionality of data is crucial for simplifying analysis and visualization, as it alleviates the curse of dimensionality and reduces computational complexity. High-dimensional data presents challenges such as increased computational cost, sparsity, and overfitting, which can get in the way of analysis and interpretation. Dimensionality reduction techniques like PCA help address these challenges by capturing the essential structure of the data in a lower-dimensional space, making it easier to visualize, interpret, and analyze complex datasets. Also, reducing dimensionality can improve the performance of machine learning algorithms by reducing noise, mitigating overfitting, enhancing model generalization, and potentially increasing the speed of training/evaluation of the models with the reduced amount of data.

In this specific project, PCA is used to reduce the amount of numerical/boolean factors to just 2 factors, which then enables the examination of those 2 components visually. The relationship between the reduced-dimension factors and the original factors in the dataset is explored, showing the effect and importance of the various factors. And especially since there are more than 1 million rows in the dataset, every amount of data reduction is helpful for subsequent machine learning model training.



## Data Preperation

```{python}
#| include: false

data_path = os.path.normpath("../../data/products_cleaned.csv")

df_az = pd.read_csv(data_path, dtype={"Category": "category"}, parse_dates=["Date Scraped"])
```

The data, since already cleaned, did not need much transformation to work with PCA. The following steps were taken to prepare the data:

- Selected only numerical and boolean columns
- Outliers (outside the 99% quantile) were removed
- The data was scaled to min/max of 0, 1

A sample of the final form of the data used can be seen in @tbl-data. A link to the data used can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/data/output/products_sample.csv).


@tbl-data

```{python}
#| label: tbl-data
#| layout-ncol: 2
#| tbl-cap: A random sample of the data used with PCA.
#| tbl-subcap:
#|   - Unscaled data
#|   - Scaled (by min/max) data

df_pca = df_az.select_dtypes(include=[bool, np.number])
df_pca["Is Best Seller"] = df_pca["Is Best Seller"].astype(int)
df_pca = df_pca.fillna(0)

df_show_rand_sample(df_pca)

outlier_cols = ["Reviews", "Price", "List Price", "Bought In Month"]
q99 = df_pca[outlier_cols].quantile(0.99)

# Remove outliers and scale
df_pca_scaled = df_pca[~(df_pca[outlier_cols] >= q99).any(axis=1)]
df_pca_scaled = (df_pca_scaled-df_pca_scaled.min())/(df_pca_scaled.max()-df_pca_scaled.min())

df_show_rand_sample(df_pca_scaled)
```


## Code

::: {.columns}
::: {.column}
The jupyter notebook code for running the PCA can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/PCA%20SVD.ipynb), or click on the link card to the right.
:::

::: {.column width=10%}
<!-- Spacing column -->
:::

::: {.column width=30%}
[![](https://github-link-card.s3.ap-northeast-1.amazonaws.com/CUBoulder-DS/CSCI-5622.png)](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/PCA%20SVD.ipynb)
:::
:::


## Results

::: callout-tip
## Important

If the interactive figures don't load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.
:::

In  @fig-evbar, we show the results of running PCA on both the unscaled data, and the data that was scaled (and outliers removed) as a scree plot of the explained variance for each PC found. As one can see, The unscaled data found that 2 PCs were enough, while the scaled data benefits from keeping 3/4 of the PCs. Based on teh number of input columns (6), 3 was chosen.

```{python}
#| label: fig-evbar
#| fig-cap: A bar chart of the explained variance ratios of each principal component, and the scree plot of cummulative percentage of the ratios, for both the normal and scaled data.
#| fig-subcap:
#|   - Unscaled data
#|   - Scaled (and outliers removed) data
#| layout-ncol: 2

ev_ratios = [9.26318388e-01, 5.54266166e-02, 1.82461238e-02, 8.72660644e-06]

ev_ratios_scaled = [0.52386047, 0.24187108, 0.1572563,  0.04710463, 0.02847362]

pc_names = ["PC1", "PC2", "PC3", "PC4", "PC5"]

fig = px.bar(x=pc_names[:-1], y=ev_ratios)
fig.add_scatter(x=pc_names[:-1],
                y=np.cumsum(ev_ratios),
                mode="lines+markers+text",
                texttemplate="%{y:.1%}",
                textposition="top center",
                textfont=dict(color="darkgreen", size=13),
                line_color="green")

fig.update_layout(showlegend=False,
    width=300,
    xaxis_title="Principal Component",
    yaxis_title="Explained Variance Ratio")
fig.show()

fig = px.bar(x=pc_names, y=ev_ratios_scaled)
fig.add_scatter(x=pc_names,
                y=np.cumsum(ev_ratios_scaled),
                mode="lines+markers+text",
                texttemplate="%{y:.1%}",
                textposition="top center",
                textfont=dict(color="darkgreen", size=13),
                line_color="green")

fig.update_layout(showlegend=False,
    width=300,
    xaxis_title="Principal Component",
    yaxis_title="Explained Variance Ratio")
fig.show()
```



In @fig-pcs, one can see the visualization of the PCs, colored by specific features chosen that show clear stratification in the PC points. The 3-PC data (from using scaled data) is shown on the left, and the 2-PC data (unscaled data) on the right.

::: {#fig-pcs layout-ncol=2 fig-cap-location=top}

![](../images/pca_f7.png){.lightbox}

![](../images/pca_f3.png){.lightbox}

![](../images/pca_f4.png){.lightbox}

![](../images/pca_f1.png){.lightbox}

![](../images/pca_f5.png){.lightbox}

![](../images/pca_f2.png){.lightbox}

![](../images/pca_f6.png){.lightbox}

The found principal components, colored by a feature in the dataset where the feature has visible stratification in the PCs.
:::

### Relationship of Variables and PCs

::: {.columns}
::: {.column width=30%}
![An example of the projection of the original data points onto the reduced-dimension line of an eigenvector. The angle/direction of the line is computationally chosen to maximize the sum of all the variance, aka the dotted lines.](../images/pca_ex3.jpeg)
:::

::: {.column width=5%}
<!-- Spacing column -->
:::

::: {.column}
Principal Component Analysis (PCA) seeks to capture the maximum variance in a dataset by transforming the original variables into a new set of uncorrelated variables called principal components (PCs). The relationship between the variables and the principal components lies in the weights or loadings assigned to each variable in the construction of these components. These loadings indicate the contribution of each variable to each principal component. Variables with higher loadings on a particular principal component are more strongly correlated with that component and thus play a more significant role in defining its characteristics. Conversely, variables with low or near-zero loadings contribute less to that component's representation. Therefore, PCA helps to uncover underlying patterns in data by highlighting the relationships and importance of variables in creating the principal components that capture the data's variance efficiently.
:::
:::



## Conclusions

From the above results, several interesting conclusions can be drawn:

- Bought in month was an "important" enough feature that PCA stratified on it in both the 3 PC and 2 PC cases.
- No other feature was as cleanly stratified-on in both cases.
- The features that were stratified on in teh 2 PC case (where data was not scaled) were the features that didn't need scaling anyways (`Reviews`, `Date Scraped`); this implies the scaling made the features more able to be seperable on variance, which aligns to the reasoning for scaling (and dropping outliers) int the first place.
- The high explained variance indicates a good ability to capture the variance in the data.

Overall, PCA did a reasonable job with dimensionality reduction, though futher work is needed as only 6 features were used in the first place.
