# PCA and SVD

Where Principal Component Analysis and Singular Value Decomposition are used, for dimensionality reduction and EDA.

```{python}
#| include: false
#| label: setup

# In order to force reload any changes done to the models package files
%load_ext autoreload
%autoreload 2

import pandas as pd
import numpy as np
import os

from IPython.display import display

import plotly
import plotly.express as px
import plotly.io as pio
import plotly.graph_objects as go

# Has to be after all other imports
from common_funcs import *
from common_funcs import df_show_rand_sample
```

## Overview

::: {style="width:100%;"}
::: {style="float:right; width:65%; padding:2%" layout-ncol=2}
![An example 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components generated from it, respectively.](../images/pca_ex1.jpeg)

![Scatterplot after PCA reduced from 3-dimensions to 2-dimensions.](../images/pca_ex2.jpeg)
:::
:::

Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and ML. At its core, PCA aims to transform high-dimensional data into a lower-dimensional space while preserving the most important information. This is achieved by identifying the directions of maximum variance in the original data, known as principal components. These principal components are the eigenvectors of the covariance matrix of the data, with their corresponding eigenvalues equaling the amount of variance explained by each principal component (PC). By retaining the PCs that capture the majority of the variance, PCA enables dimensionality reduction without significant loss of information.

Reducing the dimensionality of data is crucial for simplifying analysis and visualization, as it alleviates the curse of dimensionality and reduces computational complexity. High-dimensional data presents challenges such as increased computational cost, sparsity, and overfitting, which can get in the way of analysis and interpretation. Dimensionality reduction techniques like PCA help address these challenges by capturing the essential structure of the data in a lower-dimensional space, making it easier to visualize, interpret, and analyze complex datasets. Also, reducing dimensionality can improve the performance of machine learning algorithms by reducing noise, mitigating overfitting, enhancing model generalization, and potentially increasing the speed of training/evaluation of the models with the reduced amount of data.

In this specific project, PCA is used to reduce the amount of numerical/boolean factors to just 2 factors, which then enables the examination of those 2 components visually. The relationship between the reduced-dimension factors and the original factors in the dataset is explored, showing the effect and importance of the various factors. And especially since there are more than 1 million rows in the dataset, every amount of data reduction is helpful for subsequent machine learning model training.




## Data Preperation

```{python}
#| include: false

data_path = os.path.normpath("../../data/products_cleaned.csv")

df_az = pd.read_csv(data_path, dtype={"Category": "category"}, parse_dates=["Date Scraped"])
```

@tbl-pcadata

```{python}
#| label: tbl-pcadata
#| layout-ncol: 2
#| tbl-cap: A random sample of the data used with PCA.
#| tbl-subcap:
#|   - Unscaled data
#|   - Scaled (by min/max) data

df_pca = df_az.select_dtypes(include=[bool, np.number])
df_pca["Is Best Seller"] = df_pca["Is Best Seller"].astype(int)
df_pca = df_pca.fillna(0)

df_show_rand_sample(df_pca)

outlier_cols = ["Reviews", "Price", "List Price", "Bought In Month"]
q99 = df_pca[outlier_cols].quantile(0.99)

# Remove outliers and scale
df_pca_scaled = df_pca[~(df_pca[outlier_cols] >= q99).any(axis=1)]
df_pca_scaled = (df_pca_scaled-df_pca_scaled.min())/(df_pca_scaled.max()-df_pca_scaled.min())

df_show_rand_sample(df_pca_scaled)
```


## Code

::: {.columns}
::: {.column}
The jupyter notebook code for running the PCA can be found [here](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/PCA%20SVD.ipynb), or click on the link card to the right.
:::

::: {.column width=10%}
<!-- Spacing column -->
:::

::: {.column width=30%}
[![](https://github-link-card.s3.ap-northeast-1.amazonaws.com/CUBoulder-DS/CSCI-5622.png)](https://github.com/CUBoulder-DS/CSCI-5622/blob/main/src/PCA%20SVD.ipynb)
:::
:::


## Results

::: callout-tip
## Important

If the interactive figures don't load (is blank screen space with caption showing), dont worry: just turn off all ad-blockers/privacy browsing, make sure that Chrome/Firefox is being used, and refresh the page (or close and re-open the website tab) until all figures load.
:::


@fig-evbar

```{python}
#| label: fig-evbar
#| fig-cap: A bar chart of the explained variance ratios of each principal component, and the scree plot of cummulative percentage of the ratios, for both the normal and scaled data.
#| fig-subcap:
#|   - Unscaled data
#|   - Scaled (and outliers removed) data
#| layout-ncol: 2

ev_ratios = [9.26318388e-01, 5.54266166e-02, 1.82461238e-02, 8.72660644e-06]

ev_ratios_scaled = [0.52386047, 0.24187108, 0.1572563,  0.04710463, 0.02847362]

pc_names = ["PC1", "PC2", "PC3", "PC4", "PC5"]

fig = px.bar(x=pc_names[:-1], y=ev_ratios)
fig.add_scatter(x=pc_names[:-1],
                y=np.cumsum(ev_ratios),
                mode="lines+markers+text",
                texttemplate="%{y:.1%}",
                textposition="top center",
                textfont=dict(color="darkgreen", size=13),
                line_color="green")

fig.update_layout(showlegend=False,
    width=300,
    xaxis_title="Principal Component",
    yaxis_title="Explained Variance Ratio")
fig.show()

fig = px.bar(x=pc_names, y=ev_ratios_scaled)
fig.add_scatter(x=pc_names,
                y=np.cumsum(ev_ratios_scaled),
                mode="lines+markers+text",
                texttemplate="%{y:.1%}",
                textposition="top center",
                textfont=dict(color="darkgreen", size=13),
                line_color="green")

fig.update_layout(showlegend=False,
    width=300,
    xaxis_title="Principal Component",
    yaxis_title="Explained Variance Ratio")
fig.show()
```



@fig-pcs

::: {#fig-pcs layout-ncol=2 fig-cap-location=top}

![TODO](../images/pca_f7.png){.lightbox}

![TODO](../images/pca_f3.png){.lightbox}

![TODO](../images/pca_f4.png){.lightbox}

![TODO](../images/pca_f1.png){.lightbox}

![TODO](../images/pca_f5.png){.lightbox}

![TODO](../images/pca_f2.png){.lightbox}

![TODO](../images/pca_f6.png){.lightbox}

The found principal components, colored by a feature in the dataset where the feature has visible stratification in the PCs.
:::

### Relationship of Variables and PCs

::: {.columns}
::: {.column width=30%}
![An example of the projection of the original data points onto the reduced-dimension line of an eigenvector. The angle/direction of the line is computationally chosen to maximize the sum of all the variance, aka the dotted lines.](../images/pca_ex3.jpeg)
:::

::: {.column width=5%}
<!-- Spacing column -->
:::

::: {.column}
:::
:::



## Conclusions

